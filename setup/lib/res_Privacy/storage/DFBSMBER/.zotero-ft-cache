1270

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

A Study of Online Social Network Privacy Via the TAPE Framework
Yongbo Zeng, Student Member, IEEE, Yan (Lindsay) Sun, Senior Member, IEEE, Liudong Xing, Senior Member, IEEE, and Vinod Vokkarane, Senior Member, IEEE

Abstract—While personal information privacy is threatened by online social networks, researchers are seeking for privacy protection tools and methods to assist online social network users. In this paper, we propose a Trust-Aware Privacy Evaluation framework, called TAPE, aiming to address this problem. Under the TAPE framework we investigate how to quantitatively evaluate the privacy risk, as a function of people's awarenesses of privacy risks as well as whether people can be trusted by their friends to protect others' personal information. Simulations are performed to illustrate the key concepts and calculations, as well as the advantages of TAPE. Based on the TAPE framework, we also propose an unfriending strategy in terms of privacy protection, which outperforms other existing unfriending strategies.
Index Terms—Online social networks, privacy, trust-awareness.
I. INTRODUCTION
W ITH the emergence of Online Social Networks (OSN), people are facing critical privacy risks. In OSN, personal information can be abused, which will put users into risks. Researchers identiﬁed OSN privacy issues as two categories, inadvertent disclosure of personal information, and stalking or backtracking [1], [2]. Krishnamurthy et al. studied the problem of personal identity information leakage and how it can be misused by third parties [3]. This kind of information is able to distinguish an individual's identity either alone or when being combined with other information that is linked to a speciﬁc individual, and its leakage will lead to identity theft. Livingstone [4] demonstrated the risks when young people make friends and share personal information to express themselves online. Real life stories of sensitive information leakage in OSNs happen frequently. For example, most employers began to collect potential employees' information using social networks. According to a survey released on the EU Data Protection Day [5], privacy leakage had put people's careers on risk.
Manuscript received October 01, 2014; revised February 23, 2015; accepted April 23, 2015. Date of publication April 29, 2015; date of current version September 14, 2015. The guest editor coordinating the review of this manuscript and approving it for publication was Prof. Wade Trappe.
Y. Zeng and Y. Sun are with the Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI 02881 USA (e-mail: yongbozeng@ele.uri.edu; yansun@ele.uri.edu).
L. Xing is with the Department of Electrical and Computer Engineering, University of Massachusetts Dartmouth, Dartmouth, MA 02747 USA (e-mail: ldxing@ieee.org).
V. Vokkarane is with the Department of Electrical and Computer Engineering, University of Massachusetts Lowell, Lowell, MA 01854 USA (e-mail: vinod_vokkarane@uml.edu).
Color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/JSTSP.2015.2427774

In the current commercial OSN design, privacy risk is unavoidable, due to the publicity of OSN [6], [7]. In order to beneﬁt from the convenience of OSNs, people share personal information with friends, which makes privacy leakage possible. When privacy risk is unavoidable, we assume a risk and attempt to reduce the likelihood of harmful events. Under the assumption of unavoidable risk, risk analysis becomes extremely important. According to the National Institute of Standards and Technology (NIST), risk analysis is deﬁned as “the process of identifying risk, assessing risk, and taking steps to reduce risk to an acceptable level” [8]. In [9], In et al. discussed the advantages of risk analysis, including designing secure privacy management, monitoring and protecting critical data, making privacy policies, etc.
Risk analysis can be performed either quantitatively or qualitatively. Quantitative risk analysis plays a critical role. The advantages of quantitative risk analysis are discussed in [10], such as using metrics to evaluate risk parameters, analyzing risk events, making sophisticated decisions.
Solutions for addressing privacy issue include educational aspect and technical aspect. For example, in [11], Gundecha et al. studied privacy issues and protection recommendations, for the purpose of educating OSN users and raising their privacy awareness. The technical aspect includes managing privacy setting [12] and adopting new architectures to build OSNs [13], [14]. There are also some other categorizations in the literature. For example, in [15], Jeckmans et al. categorized privacy research into 5 categories, including raising user awareness, law and regulations, personal information anonymization, perturbing user information, and data encryption. We attempt to solve privacy issue from another perspective—providing a quantitative privacy risk analysis framework for OSN users and researchers. In the OSN privacy research literatures, quantitatively analyzing privacy risk is still not mature, as we discuss in Section II. Therefore, we propose a framework in this paper to quantitatively evaluate the privacy level of OSN users. We believe that the proposed framework can help people to understand their privacy situations, raise their privacy awareness and thus reduce privacy risks.
Quantitatively evaluating privacy level in OSN is a challenging task. First, quantitative user privacy level is not a well deﬁned concept in OSNs. Second, human users play an important role in the personal information leakage. It is complicated to predict an individual user's behavior. Third, personal information can be propagated through both online and ofﬂine media by many ways, such as chatting, emails, instant messages, Facebook postings, picture postings, tweets on Twitter,

1932-4553 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1271

etc. Fourth, it is extremely difﬁcult, if not impossible, to obtain the ground truth about a user's privacy level, with which the quantitative evaluation results can be compared.
In this paper, we address the ﬁrst challenge by proposing quantitative deﬁnition of privacy risk, based on privacy hazard and its probabilities. This quantitative measurement will lead to the privacy level calculation tools, which were originally proposed in the reliability analysis ﬁeld. To address the second and third challenge, we have to consider the availability of social data. Since nobody can monitor the users' all communication behaviors (online and ofﬂine), researchers have to work on limited data, which can be obtained with reasonable costs. In this work, Facebook privacy setting is used as the primary data source. We also focus on the ‘word-of-mouth’, which is the primary drive of OSN information diffusion [16]. Although other privacy leakage scenarios, which we discuss in Section VI-G-1, are not considered in this work, the proposed concepts, including privacy awareness and privacy trust, can be extended to those scenarios. For the fourth challenge, due to the lack of ground truth of users' privacy level, we compare the proposed scheme with some existing approaches, such as privacy concern model in [17] and vulnerability analysis in [11]. In addition, Monte Carlo simulation is employed to verify the results of privacy risk evaluation.
In this paper, we propose a TAPE (Trust-Aware Privacy Evaluation) framework for quantitatively evaluating users' privacy level in OSNs. The TAPE framework contains several novel aspects.
• It ﬁnds the similarity between the reliability analysis in wireless sensor networks (WSN) and the privacy risk estimation in OSNs. It sets up the stage for utilizing reliability analysis tools for privacy analysis.
• It considers the privacy leakage through nodes (i.e. users) and through links (i.e. friend connections) separately. Here, the privacy leakage through nodes mainly depends on the users' behavior, and we deﬁne two metrics in TAPE to estimate it. The ﬁrst one reﬂects whether one respects others' privacy, and it is named as Privacy Awareness. The other one reﬂects how much one's friends trust her/him in terms of not gossiping their information to others, and it is named as Privacy Trust. The privacy leakage through a link mainly depends on the relationship between the two users, in terms of whether one paying attention to the other' personal information.
• It proposes the desirable properties of privacy awareness and privacy trust metrics, as well as speciﬁc ways to calculate them under the guidance of trust management theory. It is the ﬁrst time that the privacy trust concept has been used in evaluating privacy level in OSN.
Besides privacy risk estimation, the TAPE framework has the ability to conduct sensitivity analysis for friend links, which is similar to the concept of vulnerability in [11]. Through the sensitivity analysis, an OSN user can understand how much his/her privacy level is affected by a particular friend connection. The sensitivity analysis yields a practical way to improve OSN users' privacy level.
As a summary, the contributions of this work include: (a) the TAPE framework, which considers privacy leakage through

nodes and links separately and utilizes traditional reliability analysis tools, as well as the deﬁnition of privacy risk, in a quantitative way; (b) the privacy awareness and privacy trust metrics; (d) a privacy awareness algorithm, which shows a clear advantage over the know algorithm called IRT [17] in the current literature; (d) the sensitivity analysis metric, from which we propose an efﬁcient unfriending strategy.
This paper is organized as follows. Related work is discussed in Section II. TAPE framework is described in Section III, followed by discussion of information spreading probability algorithms and the proposed algorithms in Section IV. Privacy assessment and sensitivity analysis metric are presented in Section V. Experiment results and conclusion are presented in Section VI and Section VII respectively.
II. RELATED WORK
Privacy in OSN have attracted many attentions. OSN service providers allow users to manage who can access which information (e.g. in Facebook and Google+), and to hide sensitive information to non-connected users (e.g. in Linkedin). Researchers studied privacy protection from two directions. Along the ﬁrst direction, fundamental changes to the current design of OSN were suggested to enhance users' privacy. Felt et al. [14] studied and discussed the privacy concerns of social network APIs for third parties. Guha et al. [18] proposed an approach to hide user data by mapping real data to fake data. Within the ﬁrst direction, “Privacy by Design” (PbD) is an important approach. In [19], Wolf et al. operationalized the concept of PbD through the process of design and development of OSN, and several social requirements of OSN were identiﬁed to optimize the privacy from a user perspective. Encryptions are usually used when adopting PbD. For example, in [13], Baden et al. proposed a new type of OSNs by using attribute-based encryption to hide user data, in which symmetric keys are used to encrypt messages and only the designated friend groups can decrypt the messages. In [20], Erkin et al. proposed to use homomorphic encryption and multi-party computation techniques to hide privacy-sensitive data from the service provider in a recommender system, without losing the signiﬁcant usability of data. The second direction is developing privacy protection tools based on existing OSNs. For example, Fang et al. [12] developed privacy wizards to give user recommendation for privacy setting, and Gundecha et al. [11] proposed an approach to identify a user's vulnerable friends. In this paper, we propose to assist users' privacy protection by providing quantitative evaluation of privacy risk and conducting sensitivity analysis for friend links. Our work belongs to the second direction.
There have been several quantiﬁcation models for privacy evaluation in OSN. Alim et al. [21] proposed a vulnerability quantiﬁcation model which consists of three components: individual, relative and absolute vulnerabilities. They examined the visibility of OSN users' proﬁles and computed the clustering coefﬁcient to compose individual vulnerability. Based on individual vulnerability, relative vulnerability and absolute vulnerability were calculated. Besides privacy risk evaluation, friend vulnerability analysis, also referred to as sensitivity analysis in this paper, is considered to be a good way to improve

1272

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

personal privacy. Abdulrahman et al. proposed a node vulnerability metric [22] and a multi-agent vulnerability analysis [23] based on the friendship graph of MySpace. Vulnerability index (V-Index) was proposed to measure how vulnerable an OSN user is based on her/his friends' privacy setting [11]. Privacy setting and its implications were considered as a primary factor in the existing models. In this paper, we consider privacy setting as one of the primary factors. The implications of privacy setting are represented as two metrics—privacy awareness and privacy trust. Besides privacy setting, the TAPE framework is able to adopt social tie analysis approaches when implementing the module of link information spreading probability. The network topology and information diffusion patterns are also considered.
The proposed work is also related to information diffusion in OSN. Gruhl et al. [24] studied the dynamics of information spreading in weblogs. Adar et al. [25] demonstrated a technique for inferring information propagation through a blog network by applying epidemic models of information spreading. Cha et al. [16] studied social cascades over Flickr social network. Researchers also attempt to build mathematical model to solve problems of information diffusion in online social network, such as [26]–[28]. In addition, there are literatures discussing the relationship between tie strength and information propagation [29], which is related to the information spreading probability that is discussed in this paper. Different from the previous information diffusion work, the proposed TAPE framework considers information diffusion in the context of privacy protection, which requires different set of features and considerations.

III. TRUST-AWARE PRIVACY EVALUATION FRAMEWORK
In this section, the TAPE framework is discussed in details. We ﬁrst deﬁne privacy risk from the perspective of information diffusion. The binary decision diagram (BDD) which was commonly used for system reliability analysis is employed to calculate privacy risks. The concepts of node information spreading probability and link information spreading probability are proposed.

A. Acronyms

OSN PIO UD UG PA IPA PT ISP NISP LISP WSN BDD BM IRT

Online Social Network Personal Information Owner Undesirable Destination Undesirable Group Privacy Awareness Individual Information Privacy Awareness Privacy Trust Information Spreading Probability Node Information Spreading Probability Link Information Spreading Probability Wireless Sensor Network Binary Decision Diagram Birnbaum's Measure Item Response Theory

Fig. 1. Online social network of Example 1.
B. Notations
Alice's type personal information Alice's undesirable group related to Alice's th undesirable destination in Privacy risk for Privacy leakage hazard for Privacy leakage probability for Calculation of ISP The set of Alice's privacy setting Alice's privacy setting for User 's PA Calculation of PA Proportion of users whose privacy setting for is looser than user Proportion of users whose privacy setting for is tighter than user Friend link between and User 's PT PT evaluation based on friend 's recommendation Calculation of PT Evaluation of how much trusts Positive recommendations for user
C. Online Social Network Privacy
Some OSNs (e.g. Facebook and Linkedin) encourage people to use real names and upload personal information onto a page known as ‘Proﬁle’. Such personal information is often accessible by friends directly, and can even ﬂow to thousands of other people through retweet (e.g. on Twitter), sharing (e.g. on Facebook) and online communication (e.g. chatting). The privacy concern in OSNs is well known, but how can we deﬁne the privacy risk in a quantitative way?
Before discussing quantiﬁcation of privacy risk, we ﬁrst look at two examples.
Example 1: Alice is a student, and she posted a piece of comment complaining her teacher Cris on her social network site. Alice does not want Cris and other teachers in the same department to know the comment. Fig. 1 shows the example social network.

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1273

Example 2: Alice posted a photo, and she does not want

anyone, except her friends, to see this photo.

Generally, in some scenarios, we want some personal infor-

mation to be known only by friends, and in some other scenarios

we don't want certain personal information to be viewed by spe-

ciﬁc people [2]. In Example 1, the personal information con-

cerned by Alice is her comment on Cris, and in Example 2, the

personal information is her photo. It is clear that an user has dif-

ferent types of personal information, and that the privacy con-

cerns depend on the particular type of personal information. We

introduce the notation to denote user 's type personal in-

formation. Without loss of generality, we present the framework

in the context of protecting Alice's privacy, i.e.

".

Alice is also referred to as the personal information owner

(PIO). In the rest of the paper, for simplicity, we use to rep-

resent

.

It is noted that privacy concerns are related to the “undesir-

able viewers”. We deﬁne the concept of Undesirable Group

(UG) of , denoted by , as follows. If Alice does not want

her information to be seen by user , then is put into ,

where is also called Undesirable Destination (UD) of . In

Example 1, Alice's UG is {Cris}. In Example 2, Alice's UG con-

tains all users except her friends.

In other words, if ﬂows to any UD, Alice considers her

privacy of being violated and privacy leakage happens. In

the rest of the paper, for simplicity, we use to represent

.

Information leaking to different persons has different poten-

tial risks to the PIO. Without deﬁning UD, this difference cannot

be captured. The privacy deﬁnition based on UD is a more gen-

eralized deﬁnition. In this deﬁnition, users are classiﬁed into 4

types:

1) personal information owner (e.g. Alice),

2) users who are allowed to access personal information ac-

cording to the privacy setting (e.g. Alice's friends),

3) users to whom the exposure of personal information causes

damage (i.e. the undesirable group)

4) users not belonging to the above three types.

In the existing work, people usually assume that there are no

type 4 users, such as in [11]. The deﬁnition of privacy leakage

in this paper becomes the traditional deﬁnition as long as the

UD is deﬁned as the complement set of the type 1 and type 2

users. Our deﬁnition can also handle the cases that Alice only

concerns that the privacy leaks to a speciﬁc set of users, as seen

in Example 1. In other words, our deﬁnition can capture the fact

that privacy leaking to different persons has different damage to

the PIO. Such difference is usually not captured by the privacy

setting alone.

D. Privacy Risk and Related Concepts
With the proposed TAPE framework, we aim to answer two questions: 1) Can we measure the probability of personal information leakage as a measurement of privacy risk in OSN? 2) How is the personal information leakage related to privacy risk? In this subsection, we ﬁrst introduce the key concepts of the TAPE framework.
In [15], privacy is considered as keeping a piece of information in its intended scope. In TAPE, the leakage of personal information occurs when any users in the undesirable group

view . Here the undesirable group is the same as the unintended scope in [15]. We assume that can only be obtained through online information diffusion, which only occurs through friend connections. This assumption is a result of the limitation of data, as discussed in Section I. In the future, if more data are available, such as cell phone contact data, this assumption can be revised. Due to this assumption, the UG in Example 2 can be simpliﬁed as {all of Alice's 2-hop neighbors}. We deﬁne privacy leakage probability of , denoted by , as the probability that at least one UD views through information diffusion in the OSN.
(1)
In statistics, the notion of risk is often modeled as the expected value of an undesired outcome [30]. That is

(2)

In the context of OSN, we argue that privacy risk of information , denoted by can be computed as

(3)

where is privacy leakage probability as deﬁned in (1) and describes the expected loss/damage in case of privacy leakage. In this paper, we also use another term “privacy level” to describe an individual's privacy, and obviously, the lower privacy risk is, the higher the privacy level is. In TAPE, is referred to as privacy leakage hazard and is normalized within interval [0, 1]. We argue that should be determined by the PIO (e.g. Alice) when the damage of the privacy leakage is subjective. For instance, in Example 1, Alice may be the best person who determines the damage if Cris saw her complain on Facebook? In many cases, PIO is often the best person to estimate the damage/loss of privacy leakage. In some other cases, PIO may not have the knowledge to determine the damage. For example, in Example 2, Alice may not understand the consequence of revealing her photo to strangers. In such case, Alice needs assistance to determine the damage, such as the privacy leakage problem study in [3]. In this paper, we simply assume that can be provided by PIO. In the rest of this paper, when we compare privacy risks, is considered as constant 1. Based on this assumption, the privacy leakage probability is equivalent to privacy risk . The core task in TAPE is to estimate the privacy leakage probability .

E. Toward Privacy Leakage Probability Estimation

In TAPE, a social network is represented by an undirected di-

agram. OSN users are the nodes, denoted as , and their friend

connections are the links, denoted as

. As discussed in

the previous section, personal information can be diffused to un-

intended recipients through friend links. It is important to point

out that the existence of a link does not necessarily mean the per-

sonal information will be transmitted through it. For instance, in

Example 1, Alice posted a piece of comment, but some of her

friends may not read it. Here are three typical situations.

• Alice's friend Bob does not pay attention to Alice's posting

at all. Alice's comment does not disseminate to Bob

through the link between Alice and Bob.

1274

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

Fig. 2. Core structure of the TAPE framework.

• Alice's friend Bob pay attention to Alice's posting and read

it. Alice's comment disseminates to Bob through the link

between Alice and Bob. Then, Bob respects Alice's privacy

and does not tell others about Alice's comment. In this case,

Alice's comment does not disseminate to others through

Bob.

• Bob reads Alice's comment, and retweets it. Such

retweeting can be seen by all Bob's friends. In this case,

Alice's comment disseminates to Bob, and it is possible to

disseminate to others through Bob.

We argue that the privacy leakage probability estimation

problem can be decomposed into two tasks.

1) Task 1: The ﬁrst task is to estimate the probability whether

a user's personal information will be disseminated through

a particular component (a link or a node). In this

work, such probabilities are referred to as information

spreading probabilities (ISP), denoted by

. The

ISP of the link between Alice and Bob depends on fac-

tors such as whether Alice and Bob are good friends,

whether Bob actively communicates with Alice in OSN,

and whether the information is interesting enough to

catch Bob's attention. The ISP of a node is determined

by complicated factors, ranging from knowledge to per-

sonality, which is extremely difﬁcult to quantify or even

understand.

2) Task 2: The second task is to compute the probability of

privacy leakage (i.e. ), given the network topology, the

information spreading probabilities of links and nodes, the

PIO (i.e. Alice), and the UG.

In the rest of this section, we ﬁrst discuss the solution to the

second task (Section III-F), and then present the solution to the

ﬁrst task (Section IV). Fig. 2 shows the core structure of the

TAPE framework.

F. Privacy Analysis and Reliability Analysis
When investigating information diffusion in OSN, we found reliability graph, which has been used as one of the reliability analysis tools (e.g. WSN reliability), can be adapted to solve the problem. An example is shown in Fig. 3.
In a reliability analysis problem, the system is represented by a reliability graph, whose links and nodes are assigned failure probabilities. The system has a source node and a sink node

Fig. 3. Similarity between WSN and OSN. In 3a, Sensor A detects ﬁre, and the detection will be sent to the server through other sensor nodes. In 3b, Alice (PIO) feels her photos are improper to be viewed by Eve (UD). (a) Wireless sensor network example; (b) online social network example.
that is usually a station. If there is no path from the source to the sink can be established, the system fails. For example, in a WSN, the nodes are sensors, and the links are the communication channels. A sensor's failure probability depends on its battery, environment temperature, work load, etc. A communication channel's failure probability depends on distance, environment noise, etc. In the context of reliability analysis, one often needs to estimate the probability that there is at least one path can be established from the source sensor to the destination sensor [31].
In the TAPE framework, we have deﬁned the information spreading probability for nodes and links in the previous section. This concept is kind of “opposite” to the failure probability. For example, if node A fails to forward data to its neighboring nodes with probability , node A's failure probability is in the context of WSN reliability analysis, whereas this node's information spreading probability is in the context of privacy anal-

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1275

TABLE I CONCEPTS MAPPING

In this work, BDD is employed to compute the probability of

information diffusion after modiﬁcation. Due to the large size

of the social network and the high computation cost of BDD,

we adopt a reduced BDD algorithm. In particular, we set the

maximum traversing depth as times the number of hops be-

tween PIO and UD. For example, when

and the UD is 3

hops away from the PIO, the branches longer than 6 (3 2) are

discarded from the BDD graph. In Section VI, we set

.

Fig. 4. An OSN information diffusion example.

G. Summary
By studying the similarity between the reliability analysis in WSN and the privacy risk estimation, we modify the BDD method to evaluate information leakage probability. The concept of node ISP and link ISP are developed. The core structure of the TAPE is shown in Fig. 2. As a summary, TAPE is presented as a framework to solve task 2 described in Section III-E. In Section IV, we discuss details of ISP calculation. Particularly, the metrics of privacy awareness and privacy trust are proposed for node ISP calculation.

Fig. 5. BDD graph of the example in Fig. 4.

ysis. The goal of WSN is to transmit data successfully, whereas

the goal of privacy protection is to prevent personal informa-

tion from propagation. Therefore, in the TAPE framework, we

can also deﬁne failure probability of nodes/links as

.

We propose to use the binary decision diagram (BDD) method,

which is commonly used in reliability analysis [31]–[33], to

solve Task 2 described in Section III-E. Table I shows the im-

portant concepts in TAPE, as well as the concepts mapping.

A BDD is a directed acyclic graph created based on Shannon's

decomposition. It is an efﬁcient tool to manipulate boolean ex-

pressions. For example, in Fig. 4, Alice is PIO and Bob is UD.

All nodes and links are assigned ISPs. In order to calculate the

information leakage probability , we ﬁrst use a boolean ex-

pression to represent .

(4)
Then, a BDD graph is constructed based on the reliability expression. The BDD graph is a binary tree (Fig. 5), each subtree is considered to be a sub-expression. The left sub-tree of a BDD node represents the expression when the node successfully spreads information. The right sub-tree represents the expression when the node fails to spread information. When traversing from the root to a leaf node, if the leaf node is a left child, then it gives a information leakage case. Based on the BDD diagram, we can evaluate using a recursive method. The details of the BDD approach can be found in [31].

IV. INFORMATION SPREADING PROBABILITY ALGORITHMS
While most social network information diffusion models consider the impact of nodes and links together [25], we argue that information propagation through nodes and through links should be considered separately. This is why we deﬁne information spreading probability of node (NISP), also referred to as node ISP, and information spreading probability of link (LISP), also referred to as link ISP, which can better describe the information diffusion process. NISP is the probability that a node will spread others' information, and LISP is the probability that a link will be in the path of information diffusion. NISP and LISP imitate the nature human communication process in the real world (i.e. ofﬂine social network).
• NISP describes the probability of speaking, i.e. talking about others.
• LISP describes the probability of listening, i.e. hearing what is said.
In this section, we focus on the algorithm of NISP, followed by a brief introduction of the LISP algorithms proposed in literatures.
A. Node Information Spreading Probability (NISP)
Evaluating NISP of a person is very challenging, because it is related to one's knowledge and personality. In the ofﬂine social network, we probably can estimate the NISP of a person based on experiences if we know this person well. Obviously, such estimation can be biased and limited, and most importantly cannot be applied in OSNs due to data limitation. Instead of resolving a challenging problem in social science, we propose to examine NISP based on the quantitative data available in OSNs.
In particular, we propose two metrics that should be used to estimate NISP—privacy awareness and privacy trust.
1) Privacy Awareness: The ﬁrst metric is privacy awareness (PA), which depends on a user's privacy setting. We argue that privacy setting reﬂects a user's privacy protection awareness, describing whether a user is paying attention to his/her own privacy. There are many different ways to compute a user's PA. In TAPE, PA evaluation is a module. The input is a set of the

1276

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015
TABLE II DESIRABLE PROPERTIES OF PA ALGORITHMS

TABLE III AN EXAMPLE: PRIVACY SETTING STATISTICS FOR BIRTHDAY

user's privacy setting, which is represented as

where is the user and is the privacy setting for

information . Privacy setting has options

, in

which is a looser setting than for

, and correspond-

ingly, can have a value in

. For example,

in Facebook,

',

',

',

', and

'. Let

be the photo in Example 2. Alice's privacy setting for is

‘friends’, i.e.

. Without loss of generality, in the rest

of this paper, we use and to represent Alice's privacy set-

ting set and privacy setting respectively.

Let

represent PA of user and

represent the

adopted PA calculation, then

(5)

The TAPE framework can accommodate many PA algorithms.

However, what are the design criteria for PA algorithms? Based

on the possible distributions of privacy setting and Alice's pos-

sible adoptions, we identiﬁed seven special cases and the desir-

able PA values in these special cases in Table II, which serves

as a guidance for the PA algorithm design. To better understand

Table II, we deﬁne

as the proportion of users whose pri-

vacy setting for is looser than , and

as the proportion

of users whose privacy setting for is tighter than . As long

as we know the statistics of users' privacy setting for and the

adoption of , we can compute

and

.

Example 3: Table III shows the statistics of birthday

privacy setting as an example. Alice allows only her friends to

see her birthday.

In Example 3,

,

,

and

.

In the rest of this section, we look at the insights of spacial

cases in Table II. We assume people can apply privacy setting

for one type of information in Table II. However, it is easy to

extend it to multiple types of information.

Case 1 and Case 2: In fact, these two cases rarely happen in

real life. We use them to demonstrate the extreme cases in PA

calculations. In Case 1, Alice chooses to open her information

in OSN, while others choose to hide it, which may indicate that

the information is sensitive and releasing this type of informa-

tion does not beneﬁt the PIOs. In this case, Alice should get a

minimum PA value due to the disclosure of sensitive informa-

tion. On the other hand, if Alice chooses to hide the information,

while others open it. It may means Alice is prudent when de-

ciding to open information. Thus, Alice should get a maximum

PA value in case 2.

Case 3: Without further evidence, it is difﬁcult to interpret

one's PA. Therefore, Alice has a neutral PA. In addition, if an

action made by the majority, without further evidence, the action

should get neither a signiﬁcant negative nor positive assessment,

e.g. neutral PA in TAPE.

Case 4 and Case 5: To see the insight of case 4 and case 5,

we look at an example. Assume many people release birthday

information to friends because they want to remind friends

about their birthdays, even if they know the privacy risk. In

this case, if Alice releases her birthday, her PA should not be

largely reduced. On the other hand, if Alice hides her birthday,

her PA should be relatively high, since in order to gain better

privacy protection, she gives up the opportunity of receiving

more birthday gifts and greetings.

Case 6 and Case 7: Many people using tight privacy setting

may imply that the information is sensitive. If Alice adopts a

loose privacy setting for this type of information, her PA is low.

On the other hand, if Alice adopts a tight setting, she should get

a larger PA but relatively smaller than that in case 5.

We have to point out that Table II may not include all possible

cases. For example, if Alice adopts a tight setting for birthday

and a loose setting for phone number, and Bob adopts a loose

setting for birthday and a tight setting for phone number, it is dif-

ﬁcult to compare Alice's PA with Bob's. In such case, we need

more data to make the PA evaluation more accurate. At current

stage, we argue that those desirable properties in Table II pro-

vide a satisfying guidance for the PA algorithm design.

2) PA Algorithm: In our proposed PA algorithm, individual

information privacy awareness (IPA) is calculated ﬁrst. IPA

is the PA value calculated from privacy setting of one type of

information. Let

denote the IPA of for .

(6)

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1277

us that Alice may be trusted not to propagate others' personal information (Fig. 6(b)). In real life, a person working on privacy research (e.g. myself) usually has good attention to the protection of privacy, thus this person has high PA. If I choose to tell someone my personal information, it means that I trust this person not to release my personal information to others. Although such implicit recommendations have noises, it may be the best resource to compute PT in OSNs.
PT calculation in TAPE is a module whose inputs are PAs of the user's friends and trust evaluations that how much the user is trusted by friends. Let represent PT of and represent PT calculation, then

(8)

Fig. 6. Privacy Trust model. (a) General recommendation trust model; (b) recommendation trust model in TAPE; (c) multiple recommendations in TAPE trust model.

where

and

are deﬁned in Section IV-A-1. It

is easy to verify that (6) satisﬁes the desirable properties in

Table II. As an example,

in Example 3.

Obviously,

. In fact, people can develop more

sophisticated calculation to replace (6), according on the imple-

mentation environment and data availability. After calculating

the IPAs for all types of information, the PA of is calculated

by

(7)

In the literature, there are some approaches proposed to evaluate similar metrics. For example, in [17], the Item Response Theory (IRT) was used for modeling “privacy concern”. In Section VI-A, we compare the proposed PA algorithm with IRT privacy concern model.
3) Privacy Trust: We propose another metric to evaluate how much a person should be trusted in terms of protecting privacy. Because this metric reﬂects how much one's friends trust her/him in terms of not gossiping their information to others, it is named as privacy trust (PT). In fact, this type of trust is very difﬁcult to evaluate based on direct evidence. First, direct evidence is rarely available, because we cannot wait someone to commit bad behaviors (e.g. gossip others) before estimating PT. Second, the clues that people use to determine whether a person is trustworthy in ofﬂine social networks are usually not available in OSNs. Alternatively, indirect evidence is used to predict OSN users' PT. Such indirect evidence can be established based on recommendations [34]. For example, Fig. 6(a) shows a typical recommendation based trust model. If A trusts B, and B gives a recommendation saying that she/he trusts C, then A is able to develop certain level of trust to C.
In TAPE, we propose to evaluate an individual's PT based on implicit recommendations from her/his friends. For example, an implicit recommendation for Alice from her friend Bob is established when Bob allows her to access his personal information. Moreover, if Bob has a high PA value, it implicitly tells

where

indicates how much is trusted by friends.

Similar to PA calculation, we argue that the PT calculation

should follow 3 rules.

Rule 1. The level of PT depends on the number of positive

recommendations. The recommendation from a high PA friend

is considered to be a positive recommendation. The more posi-

tive recommendations a user gets, the higher her/his PT should

be.

Rule 2. Negative recommendations should be carefully used.

On the one hand, if a user with low PA trusts Alice, this should

not affect Alice's PT either positively or negatively. On the

other hand, if a user with high PA does not allow Alice to view

personal information, it is not sufﬁcient to indicate Alice is

trustless.

Rule 3. Although each additional positive recommendation

can increase the PT, such incremental diminishes when the

number of positive recommendations is getting very large. For

example, when positive recommendation number increases

from 3 to 6, PT can increase a lot. However, when the number

increases from 300 to 303, PT should not increase as much as

the earlier case.

4) PT Algorithm: In the literature, there are many trust

models. We adopt a trust model using Beta function to ad-

dress concatenation propagation and multi-path propagation of

trust [34].

In the context of PT, the recommendation accuracy (arrow

from A to B in Fig. 6(a)) is replaced by PA of node B, and

the trust value (arrow from B to C in Fig. 6(a)) is the implicit

trust of B towards C, represented by

in TAPE. For sim-

plicity, in the paper, we set

as a constant value, by as-

suming that when two users are connected in OSN, they have

certain chance to see each other's personal information, but it

does not necessarily mean they have already read or will read

that information. In the future, when more OSN data is avail-

able, such as the nuanced privacy setting,

can be calcu-

lated more accurately.

We use to denote the set of positive recommendations for

, i.e. 's friends whose PA values are higher than a threshold

. The PT calculation we adopt is described as follows.

First, we estimate PT through one recommendation path

(9)

1278

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

Fig. 7. Node ISP calculation diagram.

where

is th high PA friend of . Then the variance of

the estimation of

is calculated

(10)

The PT is calculated using Beta trust model (11)

where

,

and

(12)

(13)

5) Calculation of NISP: PA and PT are surely two important factors that determine NISP. However, PA and PT metrics are not probability values. No theories can be used to compute NISP from PA and PT. In this work, we adopt a heuristic approach, which estimates the NISP as a weighted average of PA and PT,

(14)

where is a weight factor between 0 and 1. Here weighted av-

erage is one of the simplest ways to combine PA and PT. In fact,

People can develop more complicated calculation depending on

the implementation environment and data availability. In the ex-

periments in Section VI, we choose

. Fig. 7 shows the

diagram of the NISP calculation. In the future, real human users

must be involved (e.g. questionnaire) to understand the relation-

ship among NISP, PA and PT.

B. Link Information Spreading Probability (LISP)
As discussed earlier, LISP of the link between Alice and Bob depends on whether Bob heard what Alice said. Furthermore, it depends on whether Alice has a strong tie with Bob and whether the information is interesting enough to catch Bob's attention. In the current literature, many works have investigated social ties [29], [35]. Note that the TAPE framework can accommodate any algorithms for LISP calculation, as long as the outcome of LISP calculation is a value between 0 and 1 indicating the probability of information spreading. In this paper, we do not propose a speciﬁc algorithm for calculating LISP. In the experiments, we adopt a constant value for LISP and focus on demonstrating the impacts of PA and PT.

V. PRIVACY ASSESSMENT AND PRIVACY IMPROVEMENT THROUGH TAPE
By evaluating NISP and LISP, and utilizing the reliability analysis method, TAPE has the ability to assess one's OSN privacy level. More importantly, based on the privacy assessment process, TAPE is able to tell people the strategies of improving privacy level.

A. Privacy Assessment

As discussed in Section III, by utilizing the BDD method and

adopting proper NISP and LISP algorithms, TAPE is able to

evaluate privacy leakage probability from the PIO to the UD.

In real life, people usually want to avoid certain personal infor-

mation being viewed by multiple people, which is the reason

why we deﬁne undesirable group. Without further modiﬁca-

tion, TAPE can solve multiple UD case. Given an undesirable

group

, the informa-

tion leakage probability to UG is

(15)

where is the privacy leakage probability to . Here, privacy leakage happens if any one UD gets the information.
B. Privacy Improvement Strategies
The goal of privacy protection in TAPE is to reduce privacy risk. From a user's perspective, the most practical strategy is to block a friend to access certain personal information, also referred to as unfriending. In TAPE, we develop a method that can identify the friend link which contributes to the privacy leakage the most. We adopt Birnbaum's measure (BM) [36] to ﬁnd such a friend link.
Originally, Birnbaum's measure was used to examine the sensitivity or importance of a component in reliability graph. In TAPE, we use it to evaluate the sensitivity of a friend link. Birnbaum's measure evaluates the partial derivative of the leakage probability with respect to LISP of link .
(16)
For the single UD case, the detailed calculation of BM, which uses the BDD graph, can be found in [36].
We derive Birnbaum's measure for multiple UD case. By rewriting (15), we get
(17)

For the right-hand side,

(18) For the left-hand side:
(19)

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1279

TABLE IV CASE STUDY: RANK PA VS. IRT PA

Plugin together, the Birnbaum's measure of for UG is

(20)

where

is the contribution weight of

the th UD, and

is the Birnbaum's measure for when

computing the privacy leakage probability to . Finally, the

unfriending strategy proposed in TAPE is to ﬁnd a friend link,

which is

(21)

TAPE suggests to block to improve privacy level.

VI. EXPERIMENT RESULTS AND DISCUSSION

TAPE framework is implemented in Matlab, and the experi-

ments are conducted. The reduction factor of reduced BDD is 2,

i.e.

. In PT calculation, PA threshold is 0.5 and

is 0.7. NISP values are calculated according to Fig. 7. At the be-

ginning of this section, we ﬁrst do a case study to demonstrate

the calculation of PA, and then we apply TAPE to two real OSN

datasets. The privacy risks and friends sensitivities are calcu-

lated. Several unfriending strategies are compared.

A. Case Study
We compare two PA algorithms. The proposed PA algorithm, referred to as Rank PA, is described in Section IV-A-2. The comparison algorithm is described in [17], referred to as IRT. Brieﬂy speaking, this scheme calculates a metric called “privacy concern” based on privacy settings, by utilizing Item Response Theory. The goal is to estimate OSN users privacy concerns toward information sharing.
Since there is no ground truth on what should be the most “correct” value of PA, in order to demonstrate their major features, we compare these two schemes in special situations. Assume Alice has 3 types of information , and , and the related privacy settings are , and . The privacy setting is binary, either open (represented by 0) or hidden (represented by 1). The column index in Table IV is the possible privacy setting. We randomly generate privacy setting data as follows. For each

special situation, we ﬁrst specify the proportion of each privacy conﬁguration (e.g. ‘000’, ‘001’, etc.), and then generate the privacy conﬁguration realities according to the distribution. 10,000 privacy conﬁguration realities are generated for each special situation. We conduct the case studies, and the special situations we investigate are follows.
Special Situation 1—Most users (88.91% for , 88.63% for and 89.35% for ) choose to open all types of information. Special Situation 2—Most users (89.32%) hide type 1 information. For type 2 and type 3 information, most user (88.75% for and 88.92% for ) open them to public. Special Situation 3—Most users (89.02% for and 88.82% for ) hide type 1 and type 2 information, and for type 3 information 89.20% users disclose it. Special Situation 4—Most users (88.51% for , 88.48% for and 88.85% for ) hide all types of information. The PA calculation are shown in Table IV, in which the proposed PA algorithm is referred to as “Rand PA” and the comparison scheme is referred to as “IRT ”. We ﬁrst investigate the range of each scheme. IRT has narrow ranges for the studied situations, although the theoretical range of IRT can be
. In order to adopt IRT in TAPE as a PA algorithm, non-trivial normalization is needed. On the other hand, the proposed Rank PA has a range from 0 to 1 as expected. In addition, the neutral value for Rank PA is 0.5, and neutral value for IRT is 0. Then, we investigate both schemes according to desirable properties of PA in Table II and get follow observations.
1) The majority always get PA values close to neutral for Rank PA, i.e. “000” of special situation 1, “011” of special situation 2, “110” of special situation 3 and “111” of special situation 4. However, when IRT is used, such majority behavior cannot be captured.
2) Special situation 1 corresponds to case 4 and case 5 in Table II. It is seen that both Rank PA and IRT satisfy the desirable properties of case 4 and case 5.
3) Special situation 4 corresponds to PA case 6 and PA case 7 in Table II. Rank PA satisﬁes the desirable properties in PA case 6 and PA case 7. When look at “000” in special situation 4, IRT gives a higher value than the same privacy setting in special situation 1, which violates the desirable properties.
4) When investigating column “000” in Table IV, it is expected that the PA values from top to bottom should change

1280

TABLE V DATASETS SUMMARY

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

from neutral to small, because the more people adopting tight privacy setting may indicate that the information is more sensitive and opening it can yield lower PA values. Rank PA has such trend, while IRT does not. In addition, IRT is designed for binary privacy setting. However, real privacy setting usually has more than two options, such as in Facebook. Additionally, compare to the proposed Rank PA, IRT also suffers from higher computational cost.
B. Datasets
We use two datasets to conduct experiments. Dataset I contains a small number of users with detailed privacy setting. In particular, the privacy setting of 514 Facebook users in the community of University of Rhode Island (URI) are collected through a survey, and their public friends relationships are obtained by a crawler. In dataset I, there are 16 types of personal information, such as ‘email address’, ‘mobile phone number’, ‘education’ and ‘status and links’. Each type has 5 privacy setting options, including ‘everyone’, ‘networks’, ‘friends of friends’, ‘friends’ and ‘self’. Dataset II contains a large number of users with limited privacy setting information, constructed by the authors in [37]. It contains about 957,000 Facebook users. There are 4 types of personal information, including ‘add as friend’, ‘photo’, ‘view friends’ and ‘send message’. Each type of information has 2 privacy setting options, ‘open’ and ‘hidden’. Information of the two datasets is listed in Table V.
C. Privacy Risk
It is well known that the reliability of data transmission can drop signiﬁcantly as the distance (i.e. the number of hops) increases. In the context of privacy protection, does the privacy risk heavily depend on this distance? We study the relationship between the privacy risk and the distance from the PIO to UD.
We ﬁrst randomly pick 100 nodes from dataset I and put them in the PIO set. In each round of simulation, we pick one node (without replacement) from the PIO set as the PIO, and pick another node from the network as the corresponding UD, which is no more than 6-hop away from the PIO. If the picked PIO is an isolated node (i.e. degree is 0), we skip it. For each pair of PIO and UD, we measure the distance, compute the privacy risk using TAPE, and plot the privacy risk in Fig. 8. Each point represents one pair of PIO and UD. The x-axis indicates the distance between PIO and UD, and y-axis is the privacy risk.

Fig. 8. Privacy risk vs. PIO UD distance.

In this experiment, LISP is chosen from 0.5, 0.8, 0.9, and 0.95. We have the following observations
• As expected, when the distance increases, privacy risk has a decreasing trend.
• The privacy risk to 1-hop UDs (i.e. friends) can be greater than the LISP. This is because Alice's friends not only get Alice's information from Alice directly, but also through other paths. For example, Alice's friend Bob may not hear what Alice said, but he could get the message from Charlie who is another friend of Alice.
• When the distance is small, the privacy risk varies in a large range. The distance is not a dominating factor. The PA, PT and network topology jointly determine users' privacy risk. A user who is 3 hops away may be more likely to obtain Alice's person information than a user who is 2 hops away.
• As the LISP decreases, the privacy risk decreases. In the future work, incorporating the estimation of LISP will yield even a larger variation in the privacy risk values.

D. The Impact of PA and PT

Since the lack of “ground truth” about the real privacy risk

of users, it is hard to compare TAPE with other privacy evalu-

ation methods that consider different features of the users. In-

stead of comparing TAPE with a speciﬁc method, we argue that

a prevalent type of privacy study in OSN only focuses on net-

work topology. We construct a comparison method, referred to

as topology-based method, which uses the BDD to compute the

privacy risk with ﬁxed LISP and NISP. By comparing TAPE

with the topology based method, we will see whether consid-

ering PA and PT metrics reveals more information that is not

captured by considering the topology alone. In the experiment,

we set the LISP to be 0.5, and set the NISP of the topology based

method to be the average of the NISP values when considering

PA and PT.

The experiment setup is similar to that in Section VI-C. We

construct PIO sets for both dataset I and dataset II, and each

set has 100 nodes. In each round of simulation, one node is

picked up (without replacement) from the PIO sets as the PIO,

and another node that is 3 hops away from the PIO is picked

as UD. We calculate the privacy risk using TAPE and using the

topology based method. We deﬁne proportional difference as

, where

is

the privacy risk calculated based on topology, and

is

the privacy risk calculated by TAPE. The histograms of for

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1281

TABLE VI IMPACT OF LIST VALUE WHEN COMPARING TAPE
AND THE TOPOLOGY-BASED METHOD

Fig. 10. TAPE vs Monte-Carlo simulation.

dataset I earlier in this section, and we repeat it by selecting one LISP value from {0.2, 0.35, 0.5, 0.65, 0.8} at one time. The proportional difference is calculated. We list the statistics of in Table VI. The proportional difference does not change when we select different LISP. However, we have to point out that smaller LISP values will give smaller privacy risk estimations, and we already observed it in Fig. 8.

Fig. 9. Hstogram of proportional difference. (a) Dataset I; (b) Dataset II.
both datasets are shown in Fig. 9. It is seen that the proportional difference range is from 25% to 5%. Hence PA and PT do provide additional and useful information beyond the topology. In addition, it is seen that dataset II shows more concentrated distribution around 0, and dataset I has a wider range. It is known that, dataset II has 4 types of personal information and each type has 2 privacy setting options, while dataset I has 16 types of personal information and each type has 5 privacy setting options. We argue that the comprehensiveness of privacy setting can impact the performance of TAPE.
In the topology-based method, we set the LISP to be 0.5 and the NISP to be the average NISP in TAPE. When choosing the NISP value, we argue that the NISP setting favors the topology based method. Particularly, when PA and PT are not available, it is very difﬁcult to choose a proper NISP value for the topology based method. By choosing the average NISP value from TAPE, we believe that it will provide a reasonable NISP estimation for the topology based method. In the rest of this section, we conduct experiment to study how much the LISP value can impact the results when comparing TAPE and the topology-based method. The experiment setup is the same as the one using

E. Veriﬁcation of TAPE Calculation

In the previous experiments, the privacy risks are calculated

from LISP and NISP using BDD as described in Section III-F.

In order to verify this calculation, Monte-Carlo simulations are

used and the results are compared with the outputs of TAPE.

The simulation is conducted as follows. At the initial stage,

a node is selected as PIO, and the PIO owns a token, which

represents one type of personal information. During the simula-

tion stage, we divide the time into steps. At each step, every

node with a token can pass duplicates of the token to its neigh-

bors. The probability that node successfully passes the token

duplicate to its neighbor is

, where

is the NISP of and

is the LISP of link

. After steps, the simulation is terminated. If there is

any UD that obtains a token duplicate, this simulation is marked

as ‘information leakage observed’. By repeating the simulation

times, we will get ‘information leakage observed’ simu-

lations, and the simulated privacy risk is

.

In the experiment, we randomly select 1,000 PIOs from

dataset II, and those whose degrees are less than 2 are skipped.

The 3-hop privacy risks are computed using TAPE. Corre-

sponding Monte-Carlo simulations are conducted, in which

and

. Fig. 10 shows the results of our experi-

ments. It is seen that the simulation justiﬁes the calculation of

TAPE, because a strong linear relationship between simulated

1282

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

based on TAPE, performs the best. On the other hand, we show that the other unfriending strategies, which consider less information, are not as promising as the proposed TAPE framework.

Fig. 11. Privacy improvement of unfriending.
TABLE VII STATISTIC SUMMARY OF UNFRIENDING STRATEGIES
results and TAPE results is observed. It is noticed that the slop of the curve depends on the number of simulation steps, i.e. .
F. Sensitivity Analysis and Unfriending Strategy Unfriending is suggested in [22], [38]. We propose an un-
friending strategy based on Birnbaum's Measure, referred to as TAPE unfriending, which evaluates the partial derivative of the leakage probability with respect to the LISP of a given friend connection. In this section, we conduct experiments to compare TAPE unfriending with 3 unfriending approaches.
1) TAPE: In this approach, the friend link that has the largest Birnbaum's measure is blocked and the privacy improvement is calculated.
2) Friend Degree: Usually, those who are popular in OSNs are considered to be the critical points in information diffusion. Therefore, we examine the privacy improvement by blocking the friend with largest degree.
3) V-Index: Vulnerability index was proposed by Gundecha et al. [11], which is based on privacy setting of friends. We use this approach for unfriending, by blocking the friend with the largest V-Index.
4) Random: We also calculate the privacy improvement by randomly removing a friend link. This approach helps us to understand the average case when no friend sensitivity indicators are available.
The experiment setup is the same as that in Section VI-E. For each PIO-UD pair, we use above approaches to remove one friend link and calculate the privacy risk reductions. The experiment results are shown in Fig. 11, in which the x-axis is the index of PIO-UD pair and y-axis is the privacy risk reduction. The statistic summary is shown in Table VII. We can see that TAPE gives the best performance. It is important to point out that the privacy risk reductions are calculated using the TAPE framework. It is not surprising that the Birnbaum's measure, which is

G. Discussion
We deﬁne a probability based deﬁnition for privacy risk (level). Starting from the quantitative deﬁnition, the reliability evaluation method is utilized to calculate one's privacy risk in terms of information diffusion. In TAPE, PA and PT are used to capture OSN users' privacy protection behaviors. TAPE can also compute the sensitivity of one's friend links, which can assist the user to adopt unfriending strategies. TAPE can be a starting point of enhance OSN users' privacy level. Since it highly depends on data sufﬁciency, the OSN service providers who control the most user data could be the best candidates to implement TAPE, and their users can really beneﬁt from it. In addition, it is expected that social links (LISP) can also impact the calculation of privacy level. Real applications should adopt an LISP algorithm while being implemented.
1) Privacy Leakage Beyond One OSN: In TAPE, we assume that information can only be obtained through information diffusion within OSN. In practice, information diffusion is a much more complex process. There are several scenarios of information diffusion in social networks.
1) Cross-OSN diffusion: People can be active in multiple OSN platforms. For example, Alice is a friend of Bob on Twitter. She sees news about Bob on Twitter, and then she posts some words about the news on Facebook.
2) Ofﬂine diffusion: This is the traditional way we spread information through face-to-face conversation, phone calls etc.
3) Online-Ofﬂine diffusion: Information is propagated through both online and ofﬂine channels. This is the most common way we spread information in the information era.
Whereas scenario 2 is well studied in social science, scenarios 1 and 3 are challenging. In all the three scenarios, the concepts of privacy awareness and privacy trust are still valid. They have a great potential to be adopted in these scenarios and contribute to a broader study on personal privacy leakage in a hybrid onlineofﬂine world.
VII. CONCLUSION AND FUTURE WORK
In this paper, we present a TAPE framework for the quantitative evaluation of users' privacy risk in OSNs. Mathematical tools (e.g. statistics, modeling techniques) are used to process online social network data, and signal processing tools are utilized in this work. The concepts of privacy awareness and privacy trust are introduced. Simulations are performed to illustrate the computation of privacy leakage probability, as well as to demonstrate that TAPE can capture useful information which was not captured previously. Several unfriending strategies are compared with the Birnbaum's method of TAPE, and TAPE gives the best performance. More importantly, TAPE sets up the stage for utilizing reliability analysis, which is a well-developed ﬁeld, to solve privacy risk analysis problems. Besides BDD, other tools such as sensitivity analysis can surely beneﬁt privacy research.

ZENG et al.: A STUDY OF ONLINE SOCIAL NETWORK PRIVACY VIA THE TAPE FRAMEWORK

1283

Future work includes developing better PA and PT algorithms, implementation of TAPE as a Facebook application and performing real user testing.
REFERENCES
[1] D. M. Boyd and N. B. Ellison, “Social network sites: Deﬁnition, history, and scholarship,” J. Computer-Mediated Communication, vol. 13, no. 1, pp. 210–230, 2007.
[2] R. Gross and A. Acquisti, “Information revelation and privacy in online social networks,” in Proc. ACM Workshop Privacy Electron. Soc., Alexandria, VA, USA, 2005.
[3] B. Krishnamurthy and C. E. Wills, “On the leakage of personally identiﬁable information via online social networks,” in Proc. 2nd ACM Workshop Online Social Netw., Barcelona, Spain, 2009.
[4] S. Livingstone, “Taking risky opportunities in youthful content creation: Teenagers' use of social networking sites for intimacy, privacy and selfexpression,” New Media Soc., vol. 10, no. 3, pp. 393–411, 2008.
[5] EurActiv, 2010, “Social Networks Put Careers at Risk, Survey Finds,” [Online]. Available: http://www.euractiv.com/Social-networks-careers-risk
[6] S. Strauß and M. Nentwich, “Social network sites, privacy and the blurring boundary between public and private spaces,” Sci. Public Policy, vol. 40, no. 6, pp. 724–732, 2013.
[7] S. E. Schechter, “computer security strength and risk: A quantitative approach,” Ph.D. dissertation, Harvard Univ., Cambridge, MA, USA, 2004.
[8] G. Stoneburner, A. Goguen, and A. Feringa, “Risk management guide for information technology systems,” NIST Special Publicat., 2002, vol. 800, no. 30.
[9] H. In, Y.-G. Kim, T. Lee, C.-J. Moon, Y. Jung, and I. Kim, “A security risk analysis model for information systems,” in Systems Modeling and Simulation: Theory and Applications. Berlin/Heidelberg, Germany: Springer , 2005, vol. 3398, pp. 505–513.
[10] M. Hamdi and N. Boudriga, “Computer and network security risk management: Theory, challenges, and countermeasures,” Int. J. Commun. Syst., vol. 18, no. 8, pp. 763–793, 2005.
[11] P. Gundecha, G. Barbier, and H. Liu, “Exploiting vulnerability to secure user privacy on a social networking site,” in Proc. 17th ACM SIGKDD Int. Conf. Knowl. Disc. Data Mining, San Diego, CA, USA, 2011.
[12] L. Fang and K. LeFevre, “Privacy wizards for social networking sites,” in Proc. 19th Int. Conf. World Wide Web, Raleigh, NC, USA, 2010.
[13] R. Baden, A. Bender, N. Spring, B. Bhattacharjee, and D. Starin, “Persona: An online social network with user-deﬁned privacy,” in Proc. ACM SIGCOMM 2009 Conf. Data Commun., Barcelona, Spain, 2009.
[14] A. Felt and D. Evans, “Privacy protection for social networking APIs,” in Proc. Web 2.0 Security Privacy, Oakland, CA, USA, 2008.
[15] A. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. Lagendijk, and Q. Tang, “Privacy in recommender systems,” in Social Media Retrieval. Computer Communications and Networks. London, U.K.: Springer-Verlag, 2013, pp. 263–281.
[16] M. Cha, A. Mislove, and K. P. Gummadi, “A measurement-driven analysis of information propagation in the Flickr social network,” in Proc. 18th Int. Conf. World Wide Web, Madrid, Spain, 2009.
[17] S. Guo and K. Chen, “Mining privacy settings to ﬁnd optimal privacy-utility tradeoffs for social network services,” in Proc. Int. Conf. Privacy, Security, Risk, Trust and Int. Conf. Social Comput., 2012.
[18] S. Guha, K. Tang, and P. Francis, “NOYB: Privacy in online social networks,” in Proc. 1st Workshop Online Social Netw., Seattle, WA, USA, 2008.
[19] R. De Wolf, R. Heyman, and J. Pierson, “Privacy by design through a social requirements analysis of social network sites form a user perspective,” in European Data Protection: Coming of Age. Dordrecht, The Netherlands: Springer Netherlands, 2013, pp. 241–265.
[20] Z. Erkin, T. Veugen, and R. Lagendijk, “Generating private recommendations in a social trust network,” in Proc. Int. Conf. Comput. Aspects Soc. Netw. (CASoN '11), Salamanca, Spain, 2011.
[21] S. Alim, D. Neagu, and M. Ridley, “A vulnerability evaluation framework for online social network proﬁles: Axioms and propositions,” Int. J. Internet Technol. Secured Trans., vol. 4, no. 2, pp. 178–206, 2012.
[22] R. Abdulrahman, S. Alim, D. Neagu, and M. Ridley, “Algorithms for data retrieval from online social network graphs,” in Proc. IEEE 10th Int. Conf. Comput. Inf. Technol., Bradford, U.K., 2010, pp. 1660–1666.
[23] R. Abdulrahman, S. Alim, D. Neagu, D. R. W. Holton, and M. Ridley, “Multi agent system approach for vulnerability analysis of online social network proﬁles over time,” Int. J. Knowl. Web Intell., vol. 3, no. 3, pp. 256–286, 2012.
[24] D. Gruhl, R. Guha, D. Liben-Nowell, and A. Tomkins, “Information diffusion through blogspace,” in Proc. 13th Int. Conf. World Wide Web, New York, NY, USA, 2004.

[25] E. Adar and L. Adamic, “Tracking information epidemics in blogspace,” in IEEE/WIC/ACM Int. Conf. Web Intelligence, Compiègne, France, 2005.
[26] F. Wang, H. Wang, and K. Xu, “Diffusive logistic model towards predicting information diffusion in online social networks,” in Proc. 32nd Int. Conf. Distrib. Comput. Syst. Workshops, Macau, China, 2012.
[27] M. Z. Shaﬁq and A. X. Liu, “Modeling morphology of social network cascades,” Comput. Res. Repository, vol. abs/1302.2376, 2013.
[28] E. Sadikov, M. Medina, J. Leskovec, and H. Garcia-Molina, “Correcting for missing data in information cascades,” in Proc. 4th ACM Int. Conf. Web Search and Data Mining, Hong Kong, China, 2011.
[29] J. Zhao, J. Wu, X. Feng, H. Xiong, and K. Xu, “Information propagation in online social networks: A tie-strength perspective,” Knowl. Inf. Syst., vol. 32, no. 3, pp. 589–608, 2012.
[30] Y.-G. Kim and J. Lim, “Quantitative risk analysis and evaluation in information systems: A case study,” in Comput. Sci.—ICCS’07, 2007, vol. 4489, pp. 1040–1047, Springer Berlin Heidelberg.
[31] X. Zang, H. Sun, and K. S. Trivedi, “A BDD-based algorithm for reliability graph analysis,” Dept. of Elect. Eng., Duke Univ., 2000, Tech. Rep.
[32] C. Wang, L. Xing, V. Vokkarane, and Y. Sun, “Reliability analysis of wireless sensor networks using different network topology characteristics,” in Proc. Int. Conf. Quality, Reliability, Risk, Maint., Safety Eng., Chengdu, China, 2012.
[33] C. Wang, L. Xing, V. M. Vokkarane, and Y. Sun, “Many cast and any cast-based infrastructure communication reliability for wireless sensor networks,” in Proc. 18th ISSAT Int. Conf. Reliab. Qual. Design, Boston, MA, USA, 2012.
[34] Y. Sun, Z. Han, W. Yu, and K. Liu, “A trust evaluation framework in distributed networks: Vulnerability analysis and defense against attacks,” in Proc. 25th IEEE Int. Conf. Comput. Commun. Proc., Barcelona, Spain, 2006.
[35] R. Xiang, J. Neville, and M. Rogati, “Modeling relationship strength in online social networks,” in Proc. 19th Int. Conf. World Wide Web, Raleigh, NC, USA, 2010.
[36] J. Andrews and S. Beeson, “Birnbaum's measure of component importance for noncoherent systems,” IEEE Trans. Reliab., vol. 52, no. 2, pp. 213–219, Jun. 2003.
[37] M. Kurant, M. Gjoka, C. T. Butts, and A. Markopoulou, “Walking on a graph with a magnifying glass: Stratiﬁed sampling via weighted random walks,” in Proc. ACM SIGMETRICS, San Jose, CA, USA, 2011.
[38] C. Sibona and S. Walczak, “Unfriending on facebook: Friend request and online/ofﬂine behavior analysis,” in Proc. 44th Hawaii Int. Conf. Syst. Sci., Kauai, HI, USA, 2011.
Yongbo Zeng received his B.S. degree in information security from the University of Science and Technology of China (USTC) in 2009 and the M.S. degree in electrical engineering from the University of Rhode Island in 2014. He is currently a Ph.D. student of electrical engineering at the University of Rhode Island. His research interests include online social network privacy, online reputation systems, and trust evaluation.
Yan (Lindsay) Sun received her B.S. degree with the highest honors from Peking University in 1998 and the Ph.D. degree in electrical and computer engineering from the University of Maryland in 2004. She joined the University of Rhode Island in 2004, where she is currently a Professor in the department of Electrical, Computer and Biomedical Engineering.
Dr. Sun is an elected member of the Information Forensics and Security Technical Committee (IFSTC), in the IEEE Signal Processing Society. She is the Chief Editor of Sigport. She also serves on the editorial board of IEEE Security and Privacy Magazine. She has been an associate editor of SIGNAL PROCESSING LETTERS since 2013 and an associate editor of Inside Signal Processing eNewsletter (2010–2014). Dr. Sun's research interests include power grid security, trustworthy social computing, wireless network security, and reliable biomedical systems. She applied signal processing techniques in modeling, detection, and estimation of abnormal behaviors in various computing and communication systems. Dr. Sun received the best paper awards at the IEEE International Conference on Communications (ICC'14) and the IEEE International Conference on Social Computing (SocialCom’10). She was the recipient of NSF CAREER Award.

1284

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 9, NO. 7, OCTOBER 2015

Liudong Xing received the B.E. degree in computer science from Zhengzhou University, China, in 1996, and the M.S. and Ph.D. degrees in electrical engineering from the University of Virginia in 2000 and 2002, respectively. She is Professor with the Department of Electrical and Computer Engineering, University of Massachusetts (UMass) Dartmouth, USA. Her research focuses on reliability modeling and analysis of complex systems and networks. Prof. Xing is an Associate Editor for International Journal of Systems Science and International Journal of Systems Science: Operations & Logistics. She is also an Assistant Editor-in-Chief for the International Journal of Performability Engineering and an editorial board member for Reliability Engineering and System Safety. She was the recipient of UMass Dartmouth Leo M. Sullivan Teacher of the Year Award (2014), Scholar of the Year Award (2010), and Outstanding Women Award (2011). She is also the recipient of the IEEE Region 1 Technological Innovation (Academic) Award in 2007 and the co-recipient of the Best Paper Award at the IEEE International Conference on Networking, Architecture, and Storage in 2009. She is a senior member of IEEE and a member of IEEE-Eta Kappa Nu.

Vinod Vokkarane (S’02–M’04–SM’09) received the B.E. degree with Honors in Computer Science and Engineering from the University of Mysore, India, in 1999, the M.S. and the Ph.D. degree in Computer Science from the University of Texas at Dallas in 2001 and 2004, respectively. He is an Associate Professor in the department of Electrical and Computer Engineering at the University of Massachusetts Lowell. Prior to this, he was an Associate Professor of Computer and Information Science at the University of Massachusetts Dartmouth from 2004. He was also a Visiting Scientist at the Claude E. Shannon Communication and Network Group, Research Laboratory of Electronics (RLE) at the Massachusetts Institute of Technology (MIT) from 2011 to 2014. His primary areas of research include design and analysis of architectures and protocols for ultra-high speed networks, grid and cloud networks, and green networking. He has published more than 120 peer-reviewed journal and conference papers. He is the recipient of the UMass Dartmouth Scholar of the Year Award 2011, the UMass Dartmouth Chancellor's Innovation in Teaching Award 2010–11, the University of Texas at Dallas Computer Science Dissertation of the Year Award 2003–04, and the Texas Telecommunication Engineering Consortium Fellowship 2002–03. Dr. Vokkarane is the co-author of a book, Optical Burst Switched Networks (Springer, 2005). He is currently on the Editorial Board of the IEEE/OSA Journal of Optical Communications and Networking (JOCN) and Springer Photonic Network Communications Journal and has also served as an Editor of the IEEE COMMUNICATIONS LETTERS and Elsevier Journal of Optical Switching and Networking. He has co-authored several Best Paper Awards, including the IEEE GLOBECOM 2005 and IEEE ANTS 2010. He has served as the Technical Program Committee Chair for the Optical Networks and Systems (ONS) symposia at ICCCN 2007 and 2010, GLOBECOM 2011, ICC 2012, INFOCOM High-Speed Networks (HSN) workshop 2011, and IEEE ANTS 2013 and IEEE 2014. He is currently serving as the General Vice-Chair for IEEE ANTS 2015. He is a Senior Member of IEEE.

