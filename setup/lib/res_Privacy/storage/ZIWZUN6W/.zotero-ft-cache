SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

Privacy Measurement for Social Network Actor Model

Yong Wang College of Business and Information Systems
Dakota State University Madison, SD, 57103 yong.wang@dsu.edu

Raj Kumar Nepali
College of Business and Information Systems Dakota State University Madison, SD, 57103 rknepali@ pluto.dsu.edu

Abstract—Privacy measurement is critical to evaluate privacy risks for business, public policy and legislation. However, there is a lack of effective and practical way to quantify, measure, and evaluate privacy. In this paper, we propose three privacy indexes for privacy measurement (ranking) for social network actor model, i.e., weighted privacy index (w-PIDX), maximum privacy index (m-PIDX), and composite privacy index (cPIDX). We also introduce a novel virtual attribute for social network actor model to describe the combined attributes’ behavior. We further evaluate and demonstrate the effectiveness of these PIDXes for various user groups in different testing scenarios. Our tests and analysis show that composite privacy index, c-PIDX, is the best to measure privacy for social network actor model. A practical approach to evaluate w-PIDX, m-PIDX, and c-PIDX is also presented in the paper.
Keywords: privacy measurement, privacy index, w-PIDX, mPIDX, c-PIDX
I. INTRODUCTION
Many concerns have been raised regarding the privacy issues in social media. The risks, as well as the security and privacy issues of social media in business, public policy, and legislation need to be evaluated and studied. However, there is lack of effective and practical way to quantify, measure, and evaluate privacy. This paper focuses on privacy measurement issues in social media. We consider all public accessible information as social media, such as online social networks, public records of law court proceedings, records of births, marriages, etc.
Recent incidents indicate that risks, security, and privacy issues of social media to business, public policy, and legislation need to be further evaluated and studied. For example, between April 17 and April 19, 2011, user account information for the Sony PlayStation Network and its Qriocity service was compromised [1]. Sony suffered a massive breach in its video game online network that led to the theft of names, addresses and possibly credit card data belonging to 77 million user accounts. It is one of the largest-ever Internet security breaches. Sony has estimated a total cost of $172 million for the PlayStation Network breach.
In such incidents, it is very important for entrepreneurs, administrators, public policy makers, and legislature to evaluate the impact of these incidents to general public, such as, how many people were affected, and how much of their privacy would be disclosed. However, how to quantify, measure, and evaluate privacy is very challenging. First, the

definition of privacy is subjective. People may have different opinions about privacy. Second, privacy can be identified by certain disclosed attributes, such as, name, credit card number, SSN, biometric details, etc. One attribute may have different impact to individuals. Third, hidden information exists and could be further inferred from known attributes. This makes privacy measurement more challenging. Fourth, certain attributes could be combined and used to reveal more personal information. The impact of combined attributes’ behavior needs to be further studied.
We presented a social network model, SONET, for social media privacy monitoring and ranking in [2]. SONET includes three models, actor model, community model, and social network model. We proposed to use privacy index (PIDX) for privacy measurement. The work in [2] focuses on the social network modeling and does not include privacy measurement functions to evaluate PIDX. In this paper, we propose three privacy measurement functions, i.e., weighted privacy measurement function, maximum privacy measurement function, and composite privacy measurement function for privacy measurement. Three PIDXes, weighted privacy index (w-PIDX), maximum privacy index (m-PIDX), and composite privacy index (cPIDX) can be further built on these proposed privacy measurement functions. We also introduce a novel virtual attribute for actor model to describe combined attributes’ behavior.
Our main contributions in the paper include: • We propose three privacy measurement functions
and three privacy indexes for privacy measurement. • We introduce a novel virtual attribute concept for
social network actor model to describe combined attributes’ behavior. • We further test and demonstrate the effectiveness of these three PIDXes. Our tests and analysis show that the proposed composite PIDX (c-PIDX) is the best to measure privacy for social network actor model. Few works have been conducted in the literature and have limitations when used to measure privacy. There is significant difference between our proposed approach and the existing work. Our proposed PIDXes provides an effective and practical way to measure privacy and thus is very useful to evaluate privacy risks for business, public policy, and legislation. The paper is organized as follows: Section II discusses the related work. Section III introduces the actor model and our proposed privacy measurement functions and privacy

978-0-7695-5137-1/13 $26.00 © 2013 IEEE

659

DOI 10.1109/SocialCom.2013.99

indexes, followed by testing and results in Section IV, comparison and analysis in Section V. Section VI summarizes the paper and future works.
II. RELATED WORK
Social media privacy has raised many concerns and may affect individuals, enterprises, legislatures, and government agencies. Different types of attacks have been investigated in [3][4][5][6][7][8]. Previous works on social media privacy focus on privacy preserving [9] [10] [11] [12][13] and privacy policy conflicts [14][15]. Few works [16][17][18] [19][20][21] have been conducted on privacy measurement due to the challenges to quantify the privacy risk associated with online social network users.
Recent works attempting to quantify the privacy risks associated with the usage of online social networks can be found in [16][17][18] [19][20]. In [16], the authors propose to use the amount of information revealed in online social networks to quantify the privacy risks. However, there is no measurement functions developed in [16]. In [17], the authors present an approach in which privacy score is calculated by computing sensitivity and visibility of attributes. Naïve approach for evaluating sensitivity and visibility of attributes is demonstrated in [17]. The authors extend their works to another approach in [18]. Item Response Theory (IRT) is used to evaluate sensitivity and visibility of attributes when evaluating privacy scores [18]. The authors in [19] develop a tool, Privometer, to measure information leakage. The leakage is measured by a numerical value derived from combined probability of inference. The tool can suggest self-sanitization actions based on the numerical value. The work in [20] proposes to measure the privacy risk based on social networks’ privacy policy and practices when handling user [20]. Privacy scores are calculated in a debatable manner. In [21], the authors use risk labeling approach to tag users based on the community members’ feedback. Active learning method is used to correctly label strangers.
III. PRIVACY INDEXES FOR ACTOR MODEL
Actor model is first introduced in [2]. In this section, we briefly introduce actor model. Then, we introduce virtual attributes, privacy measurement functions, and privacy indexes.
1. Actor Model
Definition An actor is a social entity (e.g. people, organization, etc.) in a social network.
Definition Actor has certain characteristics that describe its features known as attributes.
Definition Each attribute has a different impact on privacy. This impact is referred as Attribute Privacy Impact Factor (APIF). Privacy impact factor is a numerical value.
An actor has attributes like name, address, social security number (SSN), phone number, education, marital status, etc.

In general, SSN has higher impact on privacy than phone number. Let ‫ܣ‬௜ be an actor and ‫ܮ‬௜ ൌ ሼܽ௜ଵǡ ܽ௜ଶǡ ǥ ǡ ܽ௜௡ሽ represent its attributes. Then, ‫ܣ‬௜ሺܽ௜ଵǡ ܽ௜ଶǡ ǥ ǡ ܽ௜௡ሻ is a representation of an actor with attributes.
We consider privacy impact factor for full privacy disclosure is 1. An attribute’s privacy impact factor is a ratio of its privacy impact to full privacy disclosure. Thus, an attribute’s privacy impact factor has a value between 0 and 1. We use ܵ௜ ൌ ሼ‫ݏ‬௜ଵǡ ‫ݏ‬௜ଶǡ ǥ ǡ ‫ݏ‬௜௡ሽ to represent attributes’ privacy impact factors.
There are certain attributes which could be inferred from other attributes. For example, if an actor’s occupation is known, the actor’s salary information can be inferred. Such information is known as hidden information.
Definition Hidden information is indirect information which is not available firsthand but could be inferred from existing data.
We use ൫ܽ௜௝ǡ ܽ௜௞ǡ ‫݌‬௝௞൯ to further indicate hidden relationship ܽ௜௝ ௣ሱሮೕೖ ܽ௜௞ between two attributes. ൫ܽ௜௝ǡ ܽ௜௞ǡ ‫݌‬௝௞൯ indicates that ܽ௜௞ can be inferred from ܽ௜௝ in probability ‫݌‬௝௞.
2. Virtual Attributes
Each attribute has an impact on privacy. Certain attributes could be combined and used to further disclose privacy. For example, the work in [22] shows that 87% of Americans can be uniquely identified by five digit zip code, gender, and date of birth. However, none of them alone can significantly affect privacy [22][23]. We propose to use virtual attributes to describe attribute group behavior.
Definition A virtual attribute describes a group of attributes behavior and their impact on privacy. A virtual attribute may have significant impact on the privacy of an actor. However, none of the attributes alone can significantly affect privacy.
Let ‫ݒ‬௜௝ be a virtual attribute and ‫ݒ‬௜௝ is decided by a group of attributes ܴ௜௝ ሺܴ௜௝ ‫ܮ ؿ‬ሻ. We use ൫ܴ௜௝ǡ ‫ݒ‬௜௝ǡ ‫݌‬௜௝൯ to indicate virtual attribute relationship ܴ௜௝ ௣ሱሮ೔ೕ ‫ݒ‬௜௝ . ܴ௜௝ ௣ሱሮ೔ೕ ‫ݒ‬௜௝ indicates that ‫ݒ‬௜௝can be inferred from ܴ௜௝ in probability ‫݌‬௜௝. Assume we have ݇ virtual attributes. Then, we have
ܴ௜ଵ ௣ሱሮ೔భ ‫ݒ‬௜ଵ ቐǥ
ܴ௜௞ ௣ሱሮ೔ೖ ‫ݒ‬௜௞ where ܴ௜௝ ‫ ܮ ؿ‬ሺͳ ൑ ݆ ൑ ݇ሻ.
3. Privacy Index
Among all the attributes ‫ ܮ‬ൌ ሼܽଵǡ ܽଶǡ ǥ ǡ ܽ௡ሽ in an actor model, there is a subset of attributes which might be known. If certain attributes are known, privacy might be disclosed. Let ‫ܮ‬௞ ൌ ሼܽଵᇱ ǡ ܽଶᇱ ǡ ǥ ǡ ܽ௠ᇱ ሽ be a known attribute list (‫ܮ‬௞ ‫)ܮ ك‬ and ܵ௞ ൌ ሼ‫ݏ‬ଵᇱ ǡ ‫ݏ‬ଶᇱ ǡ ǥ ǡ ‫ݏ‬௠ᇱ ሽ be the corresponding privacy impact factors of these attributes. Let ݂ be a privacy

660

measurement function which returns a numeric value on ‫ܮ‬௞. Let

‫ݓ‬௅ೖ ൌ ݂ሺ‫ܮ‬௞ǡ ܵ௞ሻ be the total privacy impact of ‫ܮ‬௞ . Privacy Index can be
used to measure privacy exposure.

Definition Privacy Index (PIDX) is used to describe an

entity’s privacy exposure factor based on the known

attributes. Higher PIDX indicates higher exposure of

privacy. Privacy Index ܲ‫ ܺܦܫ‬is between 0 and 100.

ܲ‫ܺܦܫ‬

ൌ

‫ݓ‬௅ೖ ‫ݓ‬௅

ൈ

ͳͲͲ

Let ܶ represent a threshold which is critical to an actor’s

privacy. We define privacy invasion as

ܲ‫ ܺܦܫ‬൒ ܶ

If ܲ‫ ܺܦܫ‬is lower than T, privacy is considered to be

preserved.

a. Senstivity

Each attribute is given a privacy impact factor. The privacy impact factor reflects the sensitivity of the attribute. Large number indicates more sensitive information. Attributes’ sensitivity is represented by ܵ௜ ൌ ሼ‫ݏ‬௜ଵǡ ‫ݏ‬௜ଶǡ ǥ ǡ ‫ݏ‬௜௡ሽ.
b. Visibility

The actor model also uses probability to describe hidden information and virtual attributes. The probability is a reflection of information visibility. The visibility can be represented using ܸ௜ ൌ ሼ‫݌‬௜ଵǡ ‫݌‬௜ଶǡ ǥ ǡ ‫݌‬௜௡ሽ ሺͲ ൑ ‫݌‬௜௝ ൑ ͳሻǤ 0 indicates an unknown attribute and 1 indicates a known attributes. A value between 0 and 1 indicates partial disclosure of an attribute.
Privacy index provides a practical way to measure and evaluate privacy. We consider both sensitivity and visibility of attributes in the model and privacy index can be calculated accordingly.

4. Privacy Measurement Function

Let ሺ‫ܮ‬ǡ ܵሻ represent an actor model’s complete attribute

list, and the attributes’ privacy impact factors.

൜‫ܵܮ‬

ൌ ൌ

ሼܽଵǡ ሼ‫ݏ‬ଵǡ

ܽଶǡ ‫ݏ‬ଶǡ

ǥ ǥ

ǡ ǡ

ܽ௡ ሽ ‫ݏ‬௡ ሽ

Let ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ represent the known attribute list, and their

sensitivities, and visibilities.

‫ܮ‬௞ ൌ ሼܽଵᇱ ǡ ܽଶᇱ ǡ ǥ ǡ ܽ௠ᇱ ሽ ቐ ܵ௞ ൌ ሼ‫ݏ‬ଵᇱ ǡ ‫ݏ‬ଶᇱ ǡ ǥ ǡ ‫ݏ‬௠ᇱ ሽ
ܸ௞ ൌ ሼ‫݌‬ଵᇱ ǡ ‫݌‬ଶᇱ ǡ ǥ ǡ ‫݌‬௠ᇱ ሽ Without losing generality, we use ܸ ൌ ሼ‫݌‬ଵǡ ‫݌‬ଶǡ ǥ ǡ ‫݌‬௡ሽ ‫݌‬௜ ൌ ͳ ሺͳ ൑ ݅ ൑ ݊ሻ to represent ‫ܮ‬Ԣ‫ ݏ‬visibilities.
Privacy can be evaluated based on three measurement

metrics, i.e., known attribute list, attribute sensitivities, and

attribute visibilities. Thus, a privacy measurement function

can be evaluated based on these three inputs. We use

݂ሺ‫ܮ‬ǡ ܵǡ ܸሻ to represent a privacy measurement function.

In this paper, we propose three privacy measurement

functions, i.e., weighted privacy measurement function ݂௪,

maximum privacy measurement function ݂௠, and composite privacy measurement function ݂௖ . Using these three functions, we present three privacy indexes
correspondingly, i.e., weighted privacy index (w-PIDX), maximum privacy index (m-PIDX), composite privacy index (c-PIDX). These privacy measurement functions and
privacy indexes are discussed below.

a. Weighted Privacy Measurement Function and Weighted Privacy Index (w-PIDX)

Weighted privacy measurement function is defined as
௠

݂௪ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൌ ‫݌‬ଵᇱ ‫ݏ‬ଵᇱ ൅ ‫݌‬ଶᇱ ‫ݏ‬ଶᇱ ൅ ‫ ڮ‬൅ ‫݌‬௠ᇱ ‫ݏ‬௠ᇱ ൌ ෍ ‫݌‬௝ᇱ‫ݏ‬௝ᇱ

௝ୀଵ

w-PIDX is an index which measures an entity’s privacy

based on known attribute list weight. w-PIDX is defined as

‫ݓ‬

െ

ܲ‫ܺܦܫ‬

ൌ

‫ݓ‬௅ೖ ‫ݓ‬௅

ൈ

ͳͲͲ

ൌ

݂௪ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ݂௪ሺ‫ܮ‬ǡ ܵǡ ܸሻ

ൈ

ͳͲͲ

ൌ

σ௝௠ୀଵ ‫݌‬௝ᇱ ‫ݏ‬௝ᇱ σ௡௝ୀଵ ‫ݏ‬௝

ൈ

ͳͲͲ

b. Maximum Privacy Measuement Function and Maximum Privacy Index (m-PIDX)

Maximum privacy measurement function is defined as ݂௠ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൌ ݉ܽ‫ݔ‬ሺ‫݌‬ଵᇱ ‫ݏ‬ଵᇱ ǡ ‫݌‬ଶᇱ ‫ݏ‬ଶᇱ ǡ ǥ ǡ ‫݌‬௠ᇱ ‫ݏ‬௠ᇱ ሻ
m-PIDX is an index which measures an entity’s privacy
based on the maximum attribute impact factor of all the
known attributes. ݉ െ ܲ‫ ܺܦܫ‬ൌ ݂௠ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൈ ͳͲͲ

ൌ ݉ܽ‫ ݔ‬ሺ‫݌‬ଵᇱ ‫ݏ‬ଵᇱ ǡ ‫݌‬ଶᇱ ‫ݏ‬ଶᇱ ǡ ǥ ǡ ‫݌‬௠ᇱ ‫ݏ‬௠ᇱ ሻ ൈ ͳͲͲ where max is a function returning the maximum value in the
list.

c. Composite Privacy Measurement Function and
Composite Privacy Index (c-PIDX)
Composite privacy measurement function is defined as ݂௖ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൌ ݂௠ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൅ ሺͳ െ ݂௠ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻሻ ൈ
௙ೢሺ௅ೖǡௌೖǡ௏ೖሻ)
௙ೢሺ௅ǡௌǡ௏ሻ
c-PIDX is an index which measures an entity’s privacy based on composite privacy measurement factors. c-PIDX is defined as
ܿ െ ܲ‫ ܺܦܫ‬ൌ ݂௖ሺ‫ܮ‬௞ǡ ܵ௞ǡ ܸ௞ሻ ൈ ͳͲͲ ܿ െ ܲ‫ ܺܦܫ‬can be represented using m-PIDX and w-PIDX.
‫ ݓ‬െ ܲ‫ܺܦܫ‬ ܿ െ ܲ‫ ܺܦܫ‬ൌ ݉ െ ܲ‫ ܺܦܫ‬൅ ሺͳͲͲ െ ݉ െ ܲ‫ܺܦܫ‬ሻ ‫ͲͲͳ כ‬ The equation above can be verified easily and the detailed proof is skipped in the paper.

5. Actor Model Privacy Measurement
We suggest a three-step procedure to calculate privacy indexes for actor model as shown in Figure 1.

661

Figure 1.

Actor Model Privacy Measurement

Let ሺ‫ܮ‬ǡ ܵǡ ܸሻ represent an actor model’s attribute list, sensitivities, and visibilities. First, we expand ሺ‫ܮ‬ǡ ܵǡ ܸሻ to ሺ‫ܮ‬ாǡ ܵாǡ ܸாሻ to include virtual attributes. Assume we have k virtual attributes, we have

‫ܮ‬ா ൌ ‫ ׫ ܮ‬ሼ‫ݒ‬ଵǡ ǥ ǡ ‫ݒ‬௞ሽ

Without losing generality, we assume in ‫ܮ‬ா , ܽଵ to ܽ௡ are attributes, and ܽ௡ାଵto ܽ௡ା௞ are virtual attributes. The same expansion will be applied to ܵ and ܸ too. A virtual attribute
ܽ௡ା௧ has privacy impact factor ‫ݏ‬௡ା௧ and default visibility ‫݌‬௡ା௧ ൌ Ͳ. Its visibility is further decided by ܴ௧ ՜௣೟ ‫ݒ‬௧ . We assign ‫݌‬௡ା௧ ൌ ‫݌‬௧ if all the attributes in ܴ௧ is known. For each attribute ܽ௜ ( ͳ ൑ ݅ ൑ ݊ ), we assume ‫݌‬௜ ൌ ͳ if ܽ௜ is known and ‫݌‬௜ ൌ Ͳ if ܽ௜ is unknown. For hidden information ܽ௜ ሱ௣ሮ೔ೕ ܽ௝, we will assign ‫݌‬௝ ൌ ‫݌‬௜௝ only if ܽ௜is known and ‫݌‬௜௝
is greater than ܽ௝’s current visibility. After expansion with
virtual attributes and hidden information, we have Table 1.

Attributes

Virtual Attributes

‫ܮ‬ா

ܽଵ

ܽଶ …

ܽ௡

ܽ௡ାଵ

…

ܽ௡ା௞

ܵா

‫ݏ‬ଵ

‫ݏ‬ଶ …

‫ݏ‬௡

‫ݏ‬௡ାଵ

…

‫ݏ‬௡ା௞

ܸா

‫݌‬ଵ

‫݌‬ଶ …

‫݌‬௡

‫݌‬௡ାଵ

…

‫݌‬௡ା௞

Table 1. Actor Model Expansion

w-PIDX, m-PIDX, and c-PIDX can be calculated as

‫ݓ‬

െ

ܲ‫ܺܦܫ‬

ൌ

σ௡௝ୀାଵ௞ ‫݌‬௝‫ݏ‬௝ σ௡௝ୀାଵ௞ ‫ݏ‬௝

ൈ

ͳͲͲ

݉ െ ܲ‫ ܺܦܫ‬ൌ ሺ‫݌‬ଵ‫ݏ‬ଵ ǡ ‫݌‬ଶ‫ݏ‬ଶǡ ǥ ǡ ‫݌‬௡ା௞ܽ௡ା௞ሻ ൈ ͳͲͲ ‫ ݓ‬െ ܲ‫ܺܦܫ‬
ܿ െ ܲ‫ ܺܦܫ‬ൌ ݉ െ ܲ‫ ܺܦܫ‬൅ ሺͳͲͲ െ ݉ െ ܲ‫ܺܦܫ‬ሻ ‫ͲͲͳ כ‬

IV. PIDX TESTING AND RESULTS In this section, we describe our testing and results.

1. Attribute Selection

Our testing starts with identifying privacy attributes. We extracted preliminary attributes from social networking sites’ personal profile and privacy settings. We then developed a survey to collect user’s rating of the privacy impact of each attribute. After the survey, we selected 20 attributes for our testing. These 20 attributes are shown in Table 2. Biometric Details include information such as height, weight etc.

2. Attribute Privacy Impact Factor Assignation

According to the survey, we assign a privacy impact factor for each attribute. The attribute and its privacy impact

factor are listed in Table 2. Each attribute is also identified

by an ID number.

1 Full Name

0.05 11 Friend List

0.60

2 Education

0.15 12 Email

0.65

3 Marital Status

0.15 13 Hometown

0.65

4 Family Members 0.25 14 Places Visited

0.65

5 Gender

0.25 15 Date of Birth

0.65

6 City

0.45 16 Personal Phone Number 0.70

7 State

0.45 17 Current Location

0.80

8 Father's Name 0.45 18 Mother's Maiden Name 0.80

9 Mother's Name 0.45 19 Biometric Details

0.90

10 Photos

0.55 20 SSN

0.90

Table 2. Privacy Impact Factor

Attributes selection and privacy impact factor assignation are important for privacy measurement. However, these two issues are out of the scope of this paper. In this paper, we use the static model to demonstrate the privacy measurement functions and privacy indexes.

3. Privacy Measurement for Attribute Incremental Changes

We first evaluate the three PIDXes when known
attributes change incrementally. We use ܶ௜ to represent a testing case where ݅ is an integer between 1 and 20. ܶ௜ represents all the attributes between 1 and ݅ are known. For example, ܶଷ indicates that attributes 1 to 3, full name, education, and marital status, are known. We evaluate the
three privacy indexes for 20 testing cases from ܶଵ to ܶଶ଴. In these testing cases, no hidden information and virtual
attributes are considered. Figure 2 shows the testing results.

Figure 2.

w-PIDX vs. m-PIDX vs. c-PIDX

Figure 2 shows that w-PIDX is good at reflecting
attribute incremental changes. However, w-PIDX does not
reflect the actual privacy exposure. m-PIDX can be used for
privacy ranking. However, m-PIDX does not reflect the attribute incremental changes. For example, from ܶଵଶ to ܶଵହ, m-PIDX is the same. c-PIDX has both the advantage of w-PIDX and m-PIDX.

4. User Groups

We further tested the three privacy indexes for different user groups. User groups are selected according to the work in [24]. The authors in [24] divided users into three categories according to their privacy preferences:
• Privacy Fundamentalist (PF): extremely concerned
users unwilling to share data

662

• Pragmatic Majority (PM): concerned but willing to share information with privacy control
• Marginally Concerned (MC): willing to provide any data

We further divide Privacy Fundamentalist (PF) group into two sub groups and Pragmatic Majority (PM) group into four sub groups. In each group or sub group, users have different privacy preferences and certain attributes are known. User group settings are shown in Table 3.

Attributes

PIF PF I PF II PM I PM II PMIII PM IV MC

1 Full Name

0.05 x x X x x X x

2 Education

0.15 x x X x x X x

3 Marital Status

0.15

xx X x

4 Family Members

0.25

xx X x

5 Gender

0.25 x x X x x X x

6 City

0.45

x

xXx

7 State

0.45

x

xxx

8 Father's Name

0.45

xx

9 Mother's Name

0.45

xx

10 Photos

0.55

Xxx x x

11 Friend List

0.60

Xxx x x

12 Email

0.65

xx x x

13 Hometown

0.65

xxx

16 Places Visited

0.65

x

18 Date of Birth

0.65

x

14 Personal Phone Number 0.70

x

15 Current Location

0.80

x

17 Mother's Maiden Name 0.80

19 Biometric Details

0.90

20 SSN

0.90

Table 3. User Groups

Three privacy indexes are evaluated for all these user groups and the results are shown in Figure 3.

Hidden information is desirable to measure the real privacy risks for a user.

Figure 4.

PIDXes and Hidden Information

6. Virtual Attributes

Virtual attribute is another factor which may affect

PIDXes. Let ‫ݒ‬ଵ be a virtual attribute and it can be decided

by

gender, city, state, and ܴଵሺ
ǡ ǡ

date of ǡ

birth in  

probability ሻ ՜௣భ ‫ݒ‬ଵ

‫݌‬ଵ

.

‫ݒ‬ଵ is assigned a privacy impact factor 0.9 since it might be

used to identify a person. In scenario R, we assume gender,

city, state, date of birth is known, we evaluate PIDXes

without considering virtual attribute. We further evaluate R

with virtual attribute ‫ݒ‬ଵ in two scenarios, ‫݌‬ଵ ൌ ͲǤͺ͹ and ‫݌‬ଵ ൌ ͳ. The results are shown in Figure 5.

Figure 3.

PIDXes for Different User Groups

Figure 3 shows that c-PIDX is good for privacy ranking for different user groups. m-PIDX does not differentiate privacy risks among PM II, PM III, and PM IV.

5. Hidden Information and PIDXes

Hidden information may affect PIDXes too. For example, from a person’s education, it is likely to guess the person’s hometown. Let
௣ୀ଴Ǥଽ
ܽଶǣ ‫ ݊݋݅ݐܽܿݑ݀ܧ‬ሱۛۛሮ ܽଵଷǣ ‫݊ݓ݋ݐ݁݉݋ܪ‬ In consideration of hidden information, we evaluate its impact to PIDXes for Privacy Fundamentalist Group I and Group II. The results are shown in Figure 4. In Figure 4, c-PIDX of PF I changes from 28 to 63, a 125% change on the c-PIDX. It shows that privacy might be disclosed even for users who are unwilling to share data.

Figure 5.

PIDXes and Virtual Attributes

Figure 5 shows that, in consideration of virtual attributes,
c-PIDX changes from 71 to 83 (‫݌‬ଵ ൌ ͲǤͺ͹), a 16% increase, and to 92 (‫݌‬ଵ ൌ ͳ), a 30% increase. Thus, virtual attribute is critical to measure privacy risks.

V. COMPARISON AND ANALYSIS
The proposed model differs significantly from the privacy scores in [17][18]. The proposed approach in [18] is based on IRT. However, IRT is not designed for a complex behavioral network like social networks. IRT has three basic assumptions: items are independent, users are independent, and items and users are independent. However, these assumptions do not apply to social networks. Further, the work in [18] assumes attributes are independent and does not consider relationships between attributes. However, as found in [22][23], revelation of a combination of a few attributes can jeopardize the privacy of a user since it can lead to easy access of other attributes.
Our model does not have these limitations and it considers all the relationships between actors and attributes.

663

The sensitivity and visibility of attributes are further characterized by the possibilities in attribute to attribute relationships. In this paper, we propose three index for privacy measurement.
w-PIDX is a simple way to calculate privacy index. It is good to measure attribute incremental changes. However, wPIDX is not good for privacy ranking. For example, let ‫ݒ‬ଵǡ ‫ݒ‬ଶ ሺ‫ݒ‬ଵ ൐ ‫ݒ‬ଶሻ be two w-PIDXes, ‫ݒ‬ଵ does not indicate it has more privacy disclosure than ‫ݒ‬ଶ although ‫ݒ‬ଵ ൐ ‫ݒ‬ଶ.
m-PIDX is another way to calculate privacy index. mPIDX is good to measure privacy relative value. For example, let ‫ݒ‬ଵǡ ‫ݒ‬ଶ ሺ‫ݒ‬ଵ ൐ ‫ݒ‬ଶሻ be two m-PIDXes, it is safe to assume that ‫ݒ‬ଵ indicates more privacy disclosure than ‫ݒ‬ଶ . However, m-PIDX is not good at measuring privacy increment change.
c-PIDX is a composite measurement for privacy. It combines the advantages of both weighted privacy index and maximum privacy index. c-PIDX can not only be used to measure privacy increment change, but also be used to compare privacy relative value.
In summary, c-PIDX is the best to measure privacy for social network actor model.
VI. CONCLUSION AND FUTURE WORKS
Many concerns have been raised regarding the security and privacy issues of social media. The risks, as well as the security and privacy issues of social media in business, public policy, and legislation need to be evaluated and studied. In this paper, we propose three privacy indexes for social network actor model, i.e., weighted privacy index (wPIDX), maximum privacy index (m-PIDX), and composite privacy index (c-PIDX). We also introduce a novel virtual attribute to describe combined attributes’ behavior. We further evaluate and demonstrate the effectiveness of these PIDXes for various user groups in different testing scenarios. Our tests and analysis show that composite privacy index, cPIDX, is the best to measure privacy for social network actor model. Hidden information and virtual attributes are also critical to measure privacy. A practical approach to evaluate w-PIDX, m-PIDX, and c-PIDX is also presented in the paper. Our future work includes evaluation of these three PIDXes for social network community model.
REFERENCES
[1] C. Morris, SONY: PlayStation Breach Involves 70 Million Subscribers, April 26, 2011, CNBC.com.
[2] Raj Kumar Nepali and Yong Wang, SONET: A Social Network Model for Privacy Monitoring and Ranking, The 2nd International Workshop on Network Forensics, Security and Privacy, July 08, 2013.
[3] J. Tang, T. Lou, and J. Kleinberg, “Inferring social ties across heterogeneous networks,” in WSDM’12, 2012, pp. 743–752.
[4] E. Zheleva and L. Getoor, “To join or not to join?: The illusion of privacy in social networks with mixed public and private user profiles,” in WWW 2009, 2009, pp. 531–540.

[5] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel, “A Practical Attack to De-anonymize Social Network Users,” 2010 IEEE Symposium on Security and Privacy, pp. 223–238, 2010.
[6] A. Narayanan and V. Shmatikov, “De-anonymizing Social Networks,” in 2009 30th IEEE Symposium on Security and Privacy, 2009, pp. 173–187.
[7] B. Zhou and J. Pei, “Preserving Privacy in Social Networks Against Neighborhood Attacks,” in 2008 IEEE 24th International Conference on Data Engineering, 2008, pp. 506–515.
[8] G. Brown, T. Howe, M. Ihbe, A. Prakash, K. Borders, and A. Arbor, “Social Networks and Context-Aware Spam,” in CSCW’08, 2008, pp. 403–412.
[9] L. Sweeney, “K-anonymity: a model for protecting privacy,” International Journal on uncertainity, Fuzziness and knowledge-based system, vol. 10, no. 5, pp. 557–570, 2002.
[10] A. Machanavajjhala, J. Gehrke, D. Kifler, and M. Venkitasubramaniam, “L-diversity: Privacy beyond k-anonymity,” in Proceedings of the 22nd IEEE International Conference on Data Engineering, 2006.
[11] F. Buccafurri, G. Lax, and V. Graziella, “Privacy-Preserving Resource Evaluation in Social Networks,” in Tenth Annual International Conference on Privacy, Security and Trust, 2012, pp. 51–58.
[12] J. Anderson, C. Diaz, F. Stajano, K. U. Leuven, and J. Bonneau, “Privacy-Enabling Social Networking over untrusted networks,” in WONS, 2009, pp. 2–7.
[13] D. Starin, R. Baden, A. Bender, N. Spring, and B. Bhattacharjee, “Persona?: An Online Social Network with User-Defined Privacy Categories and Subject Descriptors,” in SIGCOMM’09, 2009, pp. 135–146.
[14] A. Yamada, T. H. Kim, and A. Perrig, “Exploiting Privacy Policy Conflicts in Online Social Exploiting privacy policy conflicts in online social networks,” 2012.
[15] Y. Liu, K. P. Gummadi, and A. Mislove, “Analyzing Facebook Privacy Settings : User Expectations vs . Reality,” in IMC’ 11, 2011.
[16] J. Becker and H. Chen, “Measuring Privacy Risk in Online Social Networks,” in Web 2.0 security and privacy Workshop, 2009.
[17] E. M. Maximilien, T. Grandison, T. Sun, D. Richardson, S. Guo, and K. Liu, “Privacy-as-a-Service?: Models , algorithms , and results on the facebook platform,” in Web 2.0 Security and privacy workshop, 2009.
[18] K. U. N. Liu, “A Framework for Computing the Privacy Scores of Users in Online Social Networks,” Knowl. Discov. Data, vol. 5, no. 1, pp. 1–30, 2010.
[19] N. Talukder, M. Ouzzani, A. K. Elmagarmid, H. Elmeleegy, and M. Yakout, “Privometer: Privacy protection in social networks,” 2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010), pp. 266–269, 2010.
[20] J. Bonneau and S. Priebusch, “The Privacy Jungle : On the Market for Data Protection in Social Networks,” in The Eighth Workshop on the Economics of Information Security, 2009, pp. 1–45.
[21] C. Akcora, B. Carminati, and E. Ferrari, “Privacy in Social Networks: How Risky is Your Social Graph?,” in 2012 IEEE 28th International Conference on Data Engineering, 2012, pp. 9–19.
[22] L. Sweeney, “Uniqueness of simple demographics in the U. S. population,” in Data privacy Lab white paper series LIDAP-WP4, 2000.
[23] Golle, Philippe. "Revisiting the uniqueness of simple demographics in the US population." Proceedings of the 5th ACM workshop on Privacy in electronic society. ACM, 2006.
[24] Ackerman, M. S., Cranor, L. F., & Reagle, J. (1999, November). Privacy in e-commerce: examining user scenarios and privacy preferences. In Proceedings of the 1st ACM conference on Electronic commerce (pp. 1-8). ACM.

664

