Coprivacy: Towards a Theory of Sustainable Privacy
Josep Domingo-Ferrer
Universitat Rovira i Virgili UNESCO Chair in Data Privacy Department of Computer Engineering and Mathematics Av. Pa¨ısos Catalans 26, E-43007 Tarragona, Catalonia
josep.domingo@urv.cat
Abstract. We introduce the novel concept of coprivacy or co-operative privacy to make privacy preservation attractive. A protocol is coprivate if the best option for a player to preserve her privacy is to help another player in preserving his privacy. Coprivacy makes an individual’s privacy preservation a goal that rationally interests other individuals: it is a matter of helping oneself by helping someone else. We formally deﬁne coprivacy in terms of Nash equilibria. We then extend the concept to: i) general coprivacy, where a helping player’s utility (i.e. interest) may include earning functionality and security in addition to privacy; ii) mixed coprivacy, where mixed strategies and mixed Nash equilibria are allowed with some restrictions; iii) correlated coprivacy, in which Nash equilibria are replaced by correlated equilibria. Coprivacy can be applied to any peer-to-peer (P2P) protocol. We illustrate coprivacy in P2P user-private information retrieval, and also in content privacy in on-line social networking.
Keywords: Coprivacy, Data privacy, User-private information retrieval, Content privacy in social networks, Game theory.
1 Introduction
The motivation of the coprivacy concept and its incipient theory presented in this paper is one of double sustainability in the information society:
1. Privacy preservation is essential to make the information society sustainable. This idea, which we already introduced in [6], should lead to clean information and communications technologies (ICT) oﬀering functionality with minimum invasion of the privacy of individuals. Such an invasion can be regarded as a virtual pollution as harmful in the long run to the moral welfare of individuals as physical pollution is to their physical welfare.
2. Privacy preservation itself should be sustainable, and be achieved as eﬀortlessly as possible as the result of rational co-operation rather than as an expensive legal requirement. Indeed, even if privacy was acclaimed as a fundamental right by the United Nations in article 12 of the Universal Declaration
J. Domingo-Ferrer and E. Magkos (Eds.): PSD 2010, LNCS 6344, pp. 258–268, 2010. c Springer-Verlag Berlin Heidelberg 2010

Coprivacy: Towards a Theory of Sustainable Privacy 259
of Human Rights (1948), relying on worldwide legal enforcement of privacy is nowadays quite unrealistic and is likely to stay so in the next decades: as noted in [18], privacy needs a strong democratic society. However, unlike law, technology is global and can enforce privacy worldwide, provided that privacy is achieved as the result of rational cooperation. This is the objective of the coprivacy concept and theory presented in this paper.
Two major pollutants of privacy are privacy-unfriendly security and privacyunaware functionality. Privacy-unfriendly security refers to the tendency of sacriﬁcing privacy with the excuse of security: this is done by governments (e.g. the former UK security and intelligence co-ordinator asserted in 2009 that anti-terror ﬁght will need privacy sacriﬁce) and by corporations (e.g. biometrics enforced on customers with the argument of ﬁghting identity theft). Privacy-unaware (let alone privacy-unfriendly) functionality is illustrated by search engines (Google, Yahoo, etc.), social networking web services, Web 2.0 services (e.g. Google Calendar, Streetview, Latitude) and so on, which concentrate on oﬀering enticing functionality for users while completely disregarding their privacy. At most, privacy vs third parties is mentioned, but not privacy of the user vs the service provider itself, who becomes a big brother in the purest Orwellian sense.
1.1 Contribution and Plan of This Paper
The environmental analogy above can be pushed further by drawing inspiration on the three “R” of environment: reducing, reusing and recycling.
Reducing. Re-identiﬁable information must be reduced. This is the idea behind database anonymization: e.g. k-anonymization [17] by means of microdata masking methods (e.g., [4]) reduces the informational content of quasi-identiﬁers. Reduction is also the idea behind ring and group signatures [3,11], which attempt to conciliate message authentication with signer privacy by reducing signer identiﬁability: the larger the group, the more private is the signer. Just as in the environment there are physical limits to the amount of waste reduction, in the privacy scenario there are functionality and security limits to reduction: completely eliminating quasi-identiﬁers dramatically reduces the utility of a data set (functionality problem); deleting the signature in a message suppresses authentication (security problem). A useful lesson that can be extracted from reduction is privacy graduality: privacy preservation is not all-or-nothing, it is a continuous magnitude from no privacy to full privacy preservation.
Reusing. The idea of reusing is certainly in the mind of impersonators mounting replay attacks, but it can also be used by data protectors to gain privacy. Such is the case of re-sampling techniques for database privacy: an original data set with N records is re-sampled M times with replacement (where M can be even greater than N ) and the resulting data set with M records is released instead of the original one. This is the idea behind synthetic data generation via multiple imputation [16]. However, as it happened for

260 J. Domingo-Ferrer
reduction there are functionality limitations to data reuse: the more reuse, the less data utility. Recycling. The idea of recycling is probably more intriguing and far less explored than reducing and reusing. Adapted to the privacy context, recycling can be regarded as leveraging other people’s eﬀorts to preserve their privacy to preserve one’s own privacy. Of course, there is a functionality toll to privacy recycling: one must adjust to other people’s ways. Nonetheless, we believe that recycling has an enormous potential in privacy preservation, as it renders privacy an attractive and shared goal, thereby making it easier to achieve and thus more sustainable. In this spirit, we next introduce a new recycling concept, called coprivacy, around which this proposal is centered.
Section 2 deﬁnes coprivacy and some of its generalizations. Section 3 illustrates coprivacy in the context of peer-to-peer (P2P) user-private information retrieval. Section 4 illustrates correlated coprivacy applied to attribute disclosure in social networks. Section 5 lists conclusions and open research issues.

2 Coprivacy and Its Generalizations

We introduce in this section the novel concept of coprivacy in a community of
peers, whereby one peer recycles to her privacy’s beneﬁt the eﬀorts of other peers
to maintain their own privacy. Informally, there is coprivacy when the best option
for a peer to preserve her privacy is to help another peer in preserving his privacy.
The great advantage that coprivacy makes privacy preservation of each speciﬁc
individual a goal that interests other individuals: therefore, privacy preservation
becomes more attractive and hence easier to achieve and more sustainable. A
formal deﬁnition of coprivacy follows. Let P1, · · · , PN be the players in a game. Denote by Si the set of player P i’s
possible strategies. For each strategy sij ∈ Si, let ui(sij) be the privacy utility of sij for P i, where a higher utility means higher overall privacy preservation for P i vs the other players. Further, let

siu∗i

=

arg max
sij ∈Si

ui(sij )

be the best strategy for P i.

Deﬁnition 1 (Coprivacy). Let Π be a game with peer players P 1, · · · , P N , and an optional system player P 0. Each player may have leaked a diﬀerent amount
of private information to the rest of players before the game starts. The game is as follows: i) P 1 selects one player P k with k ∈ {0} ∪ {2, · · · , N } and submits a request to P k; ii) If k = 0, P 0 always processes P 1’s request; if k > 1, P k decides whether to process P 1’s request (which may involve accessing the system player on P 1’s behalf ) or reject it. The players’ strategies are S0 = {s01} (process P 1’s request); S1 = {s10, s12, · · · , s1N }, where s1j means that P 1 selects P j; for i > 1, Si = {si1, si2}, where si1 means processing P 1’s request and si2 rejecting it. Game

Coprivacy: Towards a Theory of Sustainable Privacy 261
Π is said to be coprivate with respect to the set U = (u1, · · · , uN ) of privacy utility functions if s1u∗1 = s1k for some k > 1 such that sku∗k = sk1 , that is, if a peer P k exists such that (s1k, sk1) is a pure strategy Nash equilibrium [14,15] between P 1 and P k.
An intuition on the above deﬁnition is that there is coprivacy if the best strategy for player P 1 to preserve her privacy is to ask some player P k for help, and the best strategy for player P k to preserve his privacy is to provide the help requested by P 1. Note that the notions of privacy utility function and therefore of coprivacy are based on the aforementioned privacy graduality: one can have a varying degree of privacy preservation, hence it makes sense to trade it oﬀ. A quantiﬁcation of coprivacy follows:
Deﬁnition 2 (δ-Coprivacy). Given δ ∈ [0, 1], the game of Deﬁnition 1 is said to be δ-coprivate with respect to the set U = (u1, · · · , uN ) of privacy utility functions if the probability of it being coprivate for U is at least δ.
The following extensions of coprivacy are conceivable:
– General coprivacy can be deﬁned by replacing the set U of privacy utility functions in Deﬁnition 1 with a set U of general utility functions for peer players P k combining privacy preservation with security and/or functionality. In general coprivacy, the interests of peers include, in addition to privacy, functionality and/or security.
– General δ-coprivacy can be deﬁned by replacing U with U in Deﬁnition 2. – Mixed coprivacy results if one allows mixed strategies for players and
replaces the requirement of pure strategy Nash equilibrium in Deﬁnition 1 by a mixed strategy Nash equilibrium. The good point of mixed coprivacy is that a theorem by Nash [14] guarantees that any game with a ﬁnite set of players and a ﬁnite set of strategies has a mixed strategy Nash equilibrium, and is therefore mixedly coprivate. – Correlated coprivacy results if one replaces the requirement of pure Nash equilibrium in Deﬁnition 1 by a correlated equilibrium. Indeed, the outcome of independent rational behavior by users, provided by Nash equilibria, can be inferior to a centrally designed outcome. Correlated equilibria resulting from coordination of strategies may give a higher outcome. We will illustrate this in Section 4 below. – The above extensions can be combined to yield mixed general coprivacy and correlated general coprivacy. Since mixed coprivacy is always achievable if any mixed strategy is valid for any player, mixed δ-coprivacy and mixed general δ-coprivacy only make sense when players have boundary conditions that deﬁne a subset of feasible mixed strategies. The same holds for correlated coprivacy, which is also always achievable.
If a privacy preservation problem can be solved by using a protocol based on a coprivate game, the advantage is that it is in a player’s rational privacy interest to help other players to preserve their privacy.

262 J. Domingo-Ferrer
3 Coprivacy in P2P User-Private Information Retrieval
Private information retrieval (PIR) is normally modeled as a game between two players: a user and a database. The user retrieves some item from the database without the latter learning which item was retrieved. Most PIR protocols are illsuited to provide PIR from a search engine or large database, not only because their computational complexity is linear in the size of the database, but also because they (unrealistically) assume active cooperation by the database in the PIR protocol.
Pragmatic approaches to guarantee some query privacy have therefore been based so far on two relaxations of PIR: standalone and peer-to-peer (P2P). In the standalone approach, a program running locally in the user’s computer either keeps submitting fake queries to cover the user’s real queries (TrackMeNot, [12]) or masks the real query keywords with additional fake keywords (GooPIR, [7]). In the P2P approach, a user gets her queries submitted by other users in the P2P community; in this way, the database still learns which item is being retrieved, but it cannot obtain the real query histories of users, which become diffused among the peer users, thereby achieving user-private information retrieval (UPIR). We ﬁrst proposed a P2P UPIR system in [8].
Consider a system with two peers P 1 and P 2, who are interested in querying a database DB playing the role of system player P 0. If P 1 originates a query for submission to DB, she can send the query directly to DB or ask P 2 to submit the query on P 1’s behalf and return the query results. The roles of P 1 and P 2 are exchangeable.
More formally, for i, j ∈ {1, 2} and i = j, the strategies available for a requesting P i are:
Sii: P i submits her query directly to DB; Sij: P i forwards her query to P j and requests P j to submit the query on P i’s
behalf.
When receiving P i’s query, P j has two possible strategies:
T ji: P j submits P i’s query to DB and returns the answer to P i; T jj: P j ignores P i’s query and does nothing.
Let Xi(t) be the set of queries originated by P i, let Y i(t) be the set of queries submitted to DB and Y ij (t) be the set of queries forwarded by P i to P j with j = i up to time t. The privacy utility function for P i should reﬂect the following intuitions: (i) the more “distant” is Xi(t) from Y i(t), the more private is P i vs DB; (ii) the more “distant” is Xi(t) from Y ij (t), the more private is P i vs P j. Given a distance d(·, ·) between sets of queries, plausible utilities for a requesting P i under strategies Sii and Sij at time t + 1 are:
USii(t + 1) = (d(Xi(t + 1), Y i(t + 1)))αi,DB (d(Xi(t + 1), Y ij (t)))αi,j
USij (t + 1) = (d(Xi(t + 1), Y i(t)))αi,DB (d(Xi(t + 1), Y ij (t + 1)))αi,j

Coprivacy: Towards a Theory of Sustainable Privacy 263
where αi,DB and αi,j are weights in [0, 1] denoting how critical is for P i privacy in front of DB and j, respectively. The utilities for the requested player P j follow.
UT ji(t + 1) = (d(Xj (t + 1), Y j (t + 1)))αj,DB (d(Xj (t), Y ji(t)))αj,i
Since P j does nothing under T jj, we have
UT jj (t + 1) = UT jj (t) = (d(Xj (t), Y j (t)))αj,DB (d(Xj (t), Y ji(t)))αj,i
If the α-values are all identical, the above privacy utilities are maximized when the distance from the set of originated queries to the set of submitted queries is equal to the distance from the set of originated queries to the set of forwarded queries.
Assume all α values are identical. Assume also that Xi(t) and Y i(t) are “closer” than Xi(t) and Y ij (t). Since maximum privacy utility is obtained when the within-pair distances are equal to each other, the interest of P i is to increase the distance between Xi(t) and Y i(t), that is, to submit a new query via P j (strategy Sij); formally, we have USij (t+1) > USii(t+1). Assume also that Xj(t) and Y j(t) are “closer” than Xj(t) and Y ji(t). Hence, the interest of P j is to increase the distance between Xj(t) and Y j(t) and this can be done by accepting to submit P i’s query to DB (strategy T ji); formally, UT ji(t + 1) > UT jj(t + 1). Under both closeness assumptions above, (Sij, T ji) is a pure-strategy Nash equilibrium between P i and P j and there is coprivacy between P 1 and P 2.
We give a detailed formalization and empirical results for the N -player P2P user-private information retrieval game in the manuscript [9].

4 Correlated Coprivacy in Social Networks

Social networking web sites or, for short, social networks (SNs) have become an important web service with a broad range of applications: collaborative work, collaborative service rating, resource sharing, friend search, etc. Facebook, MySpace, Xing, etc., are well-known examples. In an SN, a user publishes and shares information and services.
There are two types of privacy in SNs:

– Content privacy. The information a user publishes clearly aﬀects her privacy. Recently, a privacy risk score [13] has been proposed for the user to evaluate the privacy risk caused by the publication of a certain information. Let the information attributes published by the users in an SN be labeled from 1 to n. Then the privacy score risk of user j is

n

P R(j) =

βik × V (i, j, k)

i=1 k=1

where V (i, j, k) is the visibility of user j’s value for attribute i to users which are at most k links away from j and βik is the sensitivity of attribute i vs those users.

264 J. Domingo-Ferrer

– Relationship privacy. In some SNs, the user can specify how much it trusts other users, by assigning them a trust level. It is also possible to establish several types of relationships among users (like “colleague of”, “friend of”, etc.). The trust level and the relationship type are used to decide whether access is granted to resources and services being oﬀered (access rule). The availability of information on relationships (trust level, relationship type) has increased with the advent of the Semantic Web and raises privacy concerns: knowing who is trusted by whom and to what extent discloses a lot about the user’s thoughts and feelings. For a list of related abuses see [2]. In [5], we described a new protocol oﬀering private relationships in an SN while allowing resource access through indirect relationships without requiring a mediating trusted third party.

We focus here on content privacy in SNs. A possible privacy-functionality score for user j reﬂecting the utility the user derives from participating in an SN is the amount of information the user learns from the other SN users divided by the amount the user discloses to them. This rational view of disclosure suits better SNs for professional contact (where employers and professionals target their disclosures) than SNs for personal contact (where users often disclose a lot without requiring much in return). A formalization of this privacy-functionality score is

P RF1(j) =

N j =1,j =j

n i=1

k=1 βikV (i, j , k)I(j, j , k)

1 + P R(j)

=

N j =1,j =j

n i=1

k=1 βikV (i, j , k)I(j, j , k)

1+

n i=1

k=1 βikV (i, j, k)

where I(j, j , k) is 1 if j and j are k links away from each other, and it is 0 otherwise.
Note that:

– P RF1(j) decreases as the privacy score P R(j) in its denominator increases, that is, as user j discloses more of her attributes;
– P RF1(j) increases as its numerator increases; this numerator adds up the components of privacy scores of users j = j due to those users disclosing attribute values to j.

The dichotomous version of the above privacy-functionality score, for the case where an attribute is simply either made public or kept secret, is:

P RF2(j) =

N j =1,j =j

n i=1

βiV

(i,

j

)

1 + P R(j)

=

N j =1,j =j

n i=1

βiV

(i,

j

)

1+

n i=1

βi

V

(i,

j

)

(1)

Coprivacy: Towards a Theory of Sustainable Privacy 265

If we regard P RF1(j) (resp. P RF2(j)) as a game-theoretic utility function [19], the higher P RF1(j) (resp. P RF2(j)), the higher the utility for user j.
For instance, take a strategy vector s = (s1, · · · , sN ) formed by the strategies independently and selﬁshly chosen by all users and consider the dichotomous case, that is, let the utility incurred by user j under strategy s be uj(s) = P RF2(j). It is easy to see (and it is formally shown in [10]) that rational and independent choice of strategies leads to a Nash equilibrium where no user oﬀers any information on the SN, which results in the SN being shut down. See Example 1 below.
A similar pessimistic result is known for the P2P ﬁle sharing game, in which the system goal is to leverage the upload bandwidth of the downloading peers: the dominant strategy is for all peers to attempt “free-riding”, that is, to refuse to upload [1], which causes the system to shut down.
Example 1. The simplest version of the above game is one with two users having each one attribute, which they may decide to keep hidden (a strategy denoted by H, which implies visibility 0 for the attribute) or publish (a strategy denoted by P , which implies visibility 1). Assuming a sensitivity β = 1 for that attribute and using uj(s) = P RF2(j), the user utilities for each possible strategy vector are as follows:
u1(H, H) = 0; u1(H, P ) = 1; u1(P, H) = 0; u1(P, P ) = 1/2

u2(H, H) = 0; u2(H, P ) = 0; u2(P, H) = 1; u1(P, P ) = 1/2

This simple game can be expressed in matrix form:

User 2 User 1
H
P

H 0
0 1
0

P 0
1 1/2
1/2

The above matrix corresponds to the Prisoner’s Dilemma [19], perhaps the bestknown and best-studied game. Consistently with our argument for the general case, it turns out that (H, H) is a dominant strategy, because:

u1(H, P ) = 1 ≥ u1(P, P ) = 1/2; u1(H, H) = 0 ≥ u1(P, H) = 0

u2(P, H) = 1 ≥ u1(P, P ) = 1/2; u2(H, H) = 0 ≥ u2(H, P ) = 0
The second and fourth equations above guarantee that (H, H) is a Nash equilibrium (in fact, the only one). The Prisoner’s Dilemma with N > 2 users is known as the Pollution Game [19] and corresponds to the dichotomous SN game considered above.

266 J. Domingo-Ferrer
The outcome of independent rational behavior by users, provided by Nash equilibria and dominant strategies, can be inferior to a centrally designed outcome. This is clearly seen in Example 1: the strategy (P, P ) would give more utility than (H, H) to both users. However, usually no trusted third-party accepted by all users is available to enforce correlated strategies; in that situation, the problem is how User 1 (resp. User 2) can guess whether User 2 (resp. User 1) will choose P .
Using a solution based on cryptographic protocols for bitwise fair exchange of secrets would be an option, but it seems impractical in current social networks, as it would require a cryptographic infrastructure, unavailable in most SNs.
A more practical solution to this problem may be based on direct reciprocity (i.e. tit-for-tat) or reputation, two approaches largely used in the context of P2P ﬁle-sharing systems. We describe in [10] two correlated equilibrium protocols based on tit-for-tat and reputation, respectively. They are intended as “assistants” to the human user of the SN in deciding whether to disclose an attribute to another user; however, the ultimate decision belongs to the human, who may quit and renounce to reach the equilibrium.
Those correlated equilibrium protocols oﬀer correlated general coprivacy, referred to a utility combining privacy and functionality.
5 Conclusions and Research Directions
We have introduced in this paper the novel concept of coprivacy, as well as an incipient generalization theory of it. The main contribution of coprivacy is to make data privacy an attractive feature, especially in peer-to-peer applications:
– In many situations, players can better preserve their own privacy if they help other players in preserving theirs. We say that those situations can be handled by so-called coprivate protocols.
– In other situations, the utility of players consists of a combination of privacy plus security and/or functionality. If they can increase their own utility by helping others in increasing theirs, the situation can be handled by a generally coprivate protocol.
We have shown that P2P private information retrieval can be solved with a coprivate protocol. Furthermore, we have shown that content privacy in social networks can be solved with a generally coprivate protocol.
Future research directions include developing the theory of coprivacy in the following non-exhaustive directions:
– Develop a theory of coprivacy which, given a privacy preservation problem and a parameter δ ∈ [0, 1], can answer under which conditions a δ-coprivate game (i.e. protocol) that solves the problem exists.
– Elaborate a theory of general coprivacy which also takes security and functionality into account. In this generalization, the Nash or the correlated equilibrium that characterizes coprivacy is to be reached by considering utilities which combine the privacy with the security and/or the functionality obtained by the players.

Coprivacy: Towards a Theory of Sustainable Privacy 267
– Elaborate a theory of mixed coprivacy to characterize when mixed strategies and therefore mixed coprivacy make sense for utilities about privacy, security and functionality.
– Create new cryptographic protocols to implement the privacy graduality needed in coprivacy. Speciﬁcally, ad hoc broadcast encryption and anonymous ad hoc broadcast encryption inspired in [20], (n, N )-anonymity signatures and some multiparty computation protocols for social networks are needed.
Acknowledgments and Disclaimer
This work was partly funded by the Spanish Government through projects TSI2007-65406-C03-01 “E-AEGIS” and CONSOLIDER INGENIO 2010 CSD2007-00004 “ARES”, and by the Government of Catalonia through grant 2009 SGR 1135. The author is partly supported as an ICREA-Acad`emia researcher by the Government of Catalonia. He holds the UNESCO Chair in Data Privacy, but the views expressed in this paper are his own and do not commit UNESCO.
References
1. Babaioﬀ, M., Chuang, J., Feldman, M.: Incentives in peer-to-peer systems. In: Nisan, N., Roughgarden, T., Tardos, E´., Vazirani, V.V. (eds.) Algorithmic Game Theory, pp. 593–611. Cambridge University Press, Cambridge (2007)
2. Barnes, S.B.: A privacy paradox: social networking in the United States. First Monday 11(9) (2006)
3. Chaum, D., van Heyst, E.: Group signatures. In: Davies, D.W. (ed.) EUROCRYPT 1991. LNCS, vol. 547, pp. 257–265. Springer, Heidelberg (1991)
4. Domingo-Ferrer, J., Seb´e, F., Solanas, A.: A polynomial-time approximation to optimal multivariate microaggregation. Computers & Mathematics with Applications 55(4), 717–732 (2008)
5. Domingo-Ferrer, J., Viejo, A., Seb´e, F., Gonz´alez-Nicol´as, U´ .: Privacy homomorphisms for social networks with private relationships. Computer Networks 52, 3007– 3016 (2008)
6. Domingo-Ferrer, J.: The functionality-security-privacy game. In: Torra, V., Narukawa, Y., Inuiguchi, M. (eds.) MDAI 2009. LNCS, vol. 5861, pp. 92–101. Springer, Heidelberg (2009)
7. Domingo-Ferrer, J., Solanas, A., Castell`a-Roca, J.: h(k)-Private information retrieval from privacy-uncooperative queryable databases. Online Information Review 33(4), 720–744 (2009)
8. Domingo-Ferrer, J., Bras-Amor´os, M., Wu, Q., Manj´on, J.: User-private information retrieval based on a peer-to-peer community. Data and Knowledge Engineering 68(11), 1237–1252 (2009)
9. Domingo-Ferrer, J., Gonz´alez-Nicol´as, U´ .: Peer-to-peer user-private information retrieval: a game-theoretic analysis (2010) (manuscript)
10. Domingo-Ferrer, J.: Rational privacy disclosure in social networks. In: Proc. of MDAI 2010. LNCS (2010 to appear)

268 J. Domingo-Ferrer
11. Groth, J.: Fully anonymous group signatures without random oracles. In: Kurosawa, K. (ed.) ASIACRYPT 2007. LNCS, vol. 4833, pp. 164–180. Springer, Heidelberg (2007)
12. Howe, D.C., Nissenbaum, H.: TrackMeNot: Resisting surveillance in web search. In: Lessons from the Identity Trail, pp. 409–428. Oxford University Press, Oxford (2009)
13. Liu, K., Terzi, E.: A framework for computing the privacy scores of users in online social networks. In: Proc. of ICDM 2009-The 9th IEEE International Conference on Data Mining, pp. 288–297 (2009)
14. Nash, J.: Non-cooperative games. Annals of Mathematics 54, 289–295 (1951) 15. Nisan, N., Roughgarden, T., Tardos, E´., Vazirani, V.V. (eds.): Algorithmic Game
Theory. Cambridge University Press, Cambridge (2007) 16. Rubin, D.B.: Discussion on statistical disclosure limitation. Journal of Oﬃcial
Statistics 9(2), 461–468 (1993) 17. Samarati, P.: Protecting respondents’ identities in microdata release. IEEE Trans-
actions on Knowledge and Data Engineering 13(6), 1010–1027 (2001) 18. Solove, D.J.: Understanding Privacy. Harvard University Press, Cambridge (2008) 19. Tardos, E´., Vazirani, V.V.: Basic solution concepts and computational issues. In:
Nisan, N., Roughgarden, T., Tardos, E´., Vazirani, V.V. (eds.) Algorithmic Game Theory, pp. 3–28. Cambridge University Press, Cambridge (2007) 20. Wu, Q., Mu, Y., Susilo, W., Qin, B., Domingo-Ferrer, J.: Asymmetric group key agreement. In: Joux, A. (ed.) EUROCRYPT 2009. LNCS, vol. 5479, pp. 153–170. Springer, Heidelberg (2010)

