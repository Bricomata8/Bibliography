Pedestrian route prediction from GPS logs
A Thesis submitted for the degree of Doctor of Philosophy
Gavin Smith
Bachelor of Information Technology (Honours) (Advanced Computer and Information Science), University of South Australia
School of Computer and Information Science University of South Australia January 2012

Contents

1 Introduction

1

1.1 Problem deﬁnition: route prediction . . . . . . . . . . . . . . . . . . . . . 3

1.2 Challenges of pedestrian route prediction . . . . . . . . . . . . . . . . . . 5

1.2.1 Challenges of incidentally-collected data . . . . . . . . . . . . . . 6

1.2.2 Challenges in feature selection and encodings . . . . . . . . . . . . 7

1.2.3 Challenges in developing pedestrian route prediction algorithms . 8

1.2.4 Evaluation challenges . . . . . . . . . . . . . . . . . . . . . . . . . 9

1.2.5 Data acquisition challenges . . . . . . . . . . . . . . . . . . . . . . 9

1.3 Overview and list of contributions . . . . . . . . . . . . . . . . . . . . . . 10

2 Movement prediction in the context of GPS data

14

2.1 Movement Prediction: A review . . . . . . . . . . . . . . . . . . . . . . . 15

2.1.1 An introduction to movement prediction models . . . . . . . . . . 17

2.1.2 Partially data-driven models . . . . . . . . . . . . . . . . . . . . . 22

2.1.3 Fully data-driven models . . . . . . . . . . . . . . . . . . . . . . . 27

2.2 GPS Data: Encoding for route prediction . . . . . . . . . . . . . . . . . . 42

2.2.1 Encoding location from raw GPS data . . . . . . . . . . . . . . . 43

2.2.2 Properties of noise within a data set from mobile phone GPS data 46

2.2.3 Trail identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . 49

2.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

2.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3 Evaluating route prediction algorithms

54

3.1 Metrics for evaluating prediction quality . . . . . . . . . . . . . . . . . . 58

3.1.1 Average log loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

3.1.2 Accuracy as a proportion . . . . . . . . . . . . . . . . . . . . . . . 60

3.1.3 Other measures without pointwise distance functions . . . . . . . 60

i

3.1.4 Accuracy as average error, RMSE & MAE . . . . . . . . . . . . . 61 3.1.5 Hausdorﬀ distance based metrics . . . . . . . . . . . . . . . . . . 62 3.1.6 Froehlich-Krumm distance vs MAE . . . . . . . . . . . . . . . . . 65 3.1.7 A solution: The average Fr´echet Distance . . . . . . . . . . . . . . 67 3.1.8 Aggregating prediction scores . . . . . . . . . . . . . . . . . . . . 77 3.1.9 Summary: A distance metric for movement prediction . . . . . . . 80 3.2 Testing methodology: Comparing predictors . . . . . . . . . . . . . . . . 81 3.2.1 Basic approaches to estimating the mean and variance . . . . . . 84 3.2.2 Estimators of mean error under systematic data reuse . . . . . . . 85 3.2.3 Estimators of variance under systematic data reuse . . . . . . . . 87 3.2.4 Empirical observations: Variance estimators in evaluating move-
ment predictors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 3.2.5 Tests for comparing predictors . . . . . . . . . . . . . . . . . . . . 98 3.2.6 Summary: Recommended tests for the hypothesis testing between
two predictors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 3.3 Descriptive statistics and visualizations . . . . . . . . . . . . . . . . . . . 100 3.4 Evaluating movement prediction from GPS logs: A summary . . . . . . . 102

4 Eﬃcient Prediction: a Naive Bayes’ Model

107

4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

4.2 Model deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

4.2.1 A Naive Bayes Model . . . . . . . . . . . . . . . . . . . . . . . . . 112

4.2.2 Encoding direction . . . . . . . . . . . . . . . . . . . . . . . . . . 115

4.2.3 Encoding negative information . . . . . . . . . . . . . . . . . . . . 117

4.3 Prediction mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

4.3.1 Predicting beyond the next time step . . . . . . . . . . . . . . . . 120

4.3.2 The exact number of time steps to predict for . . . . . . . . . . . 122

4.3.3 Possibility of no predictions . . . . . . . . . . . . . . . . . . . . . 122

4.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

4.4.1 Baseline methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

4.4.2 Methods evaluated . . . . . . . . . . . . . . . . . . . . . . . . . . 128

4.4.3 Evaluation methodology . . . . . . . . . . . . . . . . . . . . . . . 130

4.4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

ii

5 Beyond conditional probability and ranked lists

143

5.1 Prediction as a form of association rule mining . . . . . . . . . . . . . . . 144

5.2 Objective functions beyond support . . . . . . . . . . . . . . . . . . . . . 145

5.3 Holistic Route Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

5.3.1 Constructing the Model . . . . . . . . . . . . . . . . . . . . . . . 150

5.3.2 Generating Predictions . . . . . . . . . . . . . . . . . . . . . . . . 152

5.3.3 Matching functions as a parameter . . . . . . . . . . . . . . . . . 155

5.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

5.4.1 Rule selection via Spatial Matching . . . . . . . . . . . . . . . . . 161

5.4.2 Rule selection via Hybrid Matching . . . . . . . . . . . . . . . . . 162

5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

5.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

6 Modelling order: Augmented Cover Trees

168

6.1 Modelling order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

6.2 Multiple tree methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

6.2.1 Quadtrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

6.2.2 KD-Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

6.2.3 R-Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

6.2.4 Metric Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

6.2.5 Discussion of tree-based indexes . . . . . . . . . . . . . . . . . . . 177

6.3 An introduction to Cover Trees . . . . . . . . . . . . . . . . . . . . . . . 177

6.3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178

6.3.2 Cover tree data structure . . . . . . . . . . . . . . . . . . . . . . . 178

6.3.3 Constructing a Cover Tree . . . . . . . . . . . . . . . . . . . . . . 179

6.3.4 Nearest Neighbour queries . . . . . . . . . . . . . . . . . . . . . . 181

6.3.5 Other cover tree operations . . . . . . . . . . . . . . . . . . . . . 181

6.3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

6.4 Augmented Cover Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

6.4.1 Problem deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . 182

6.4.2 Augmented cover tree data structure . . . . . . . . . . . . . . . . 185

6.4.3 Complexity analysis . . . . . . . . . . . . . . . . . . . . . . . . . 187

6.5 Models using Cover Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 187

6.6 Movement Prediction using the Augmented Cover Tree: Experimental

Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190

iii

6.6.1 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . 192 6.6.2 Runtime performance results . . . . . . . . . . . . . . . . . . . . . 192 6.6.3 Prediction accuracy results . . . . . . . . . . . . . . . . . . . . . . 193 6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199

7 Click logs: Beyond GPS data

202

7.1 An Introduction to Click-through Data . . . . . . . . . . . . . . . . . . . 203

7.1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

7.2 Potential factors aﬀecting image click-through data . . . . . . . . . . . . 206

7.3 Experimental Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

7.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

7.4.1 Statistical analysis of the factors . . . . . . . . . . . . . . . . . . . 215

7.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

8 Conclusion and Future Work

220

8.1 Conclusion and Contributions . . . . . . . . . . . . . . . . . . . . . . . . 220

8.1.1 Evaluation Methodology: Best practices . . . . . . . . . . . . . . 221

8.1.2 A novel approach to prediction under minimal resources . . . . . 222

8.1.3 Investigations into mining movement pattens: beyond conditional

probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222

8.1.4 Investigations into novel encodings and a corresponding predictor 223

8.1.5 Beyond GPS data: an evaluation of other data sources . . . . . . 224

8.2 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

8.2.1 Improvements to the evaluation methodology . . . . . . . . . . . 225

8.2.2 Additional modelling investigations . . . . . . . . . . . . . . . . . 225

8.2.3 Utilization of the batch query algorithm proposed for Cover trees 225

8.2.4 Additional sequence/time relaxed encodings . . . . . . . . . . . . 226

8.2.5 Evaluation of the algorithms over diﬀerent data sets . . . . . . . . 226

8.2.6 Application of prediction techniques beyond GPS data . . . . . . 226

8.2.7 Evaluation of the eﬀectiveness in real world applications . . . . . 227

8.3 Final remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

A R code: the truncated average discrete Fr´echet distance

247

iv

List of Figures
1.1 Prediction process with the corresponding symbols used in this thesis . . 4
2.1 First order Markov model . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 Markov model of order 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3 First order vs. Hidden Markov Model . . . . . . . . . . . . . . . . . . . . 23 2.4 Bayesian network model proposed in [118] . . . . . . . . . . . . . . . . . 25 2.5 Example showing the importance of considering trails separately . . . . . 29 2.6 Example of a Probabilistic Suﬃx Tree (PST) . . . . . . . . . . . . . . . . 32 2.7 D-SCENT data set: Trail length histograms . . . . . . . . . . . . . . . . 49
3.1 Basic evaluation procedure and corresponding symbols used in this thesis 56 3.2 Distance measures: Example where trail point order matters . . . . . . . 66 3.3 Comparing paths: Froehlich-Krumm distance vs MAE (1) . . . . . . . . 68 3.4 Comparing paths: Froehlich-Krumm distance vs MAE (2) . . . . . . . . 69 3.5 Comparing paths: Froehlich-Krumm distance vs MAE (2) . . . . . . . . 70 3.6 An example where the metric from [69] underestimates the distance . . . 71 3.7 Comparing paths: Froehlich-Krumm vs. Average Discrete Fr´echet . . . . 72 3.8 Example: potential couplings as computed by the average Fr´echet distance 75 3.9 Example: Resampling in the presence of missing samples . . . . . . . . . 76 3.10 Empirical evidence of the prediction error distribution . . . . . . . . . . . 104 3.11 Visualizing performance by varying the error function relevance parameter 105 3.12 Example of a box plot showing the sample level generalization error. . . . 105 3.13 An example side-by-side histogram plot showing three predictors . . . . . 106 3.14 An example side-by-side histogram plot showing a subset of the data . . 106
4.1 Example: Markov predictor vs. proposed predictor, Markov win . . . . . 111 4.2 Example: Markov predictor vs. proposed predictor, Markov loss . . . . . 112 4.3 Example: The importance of the input’s history . . . . . . . . . . . . . . 113
v

4.4 Example: The importance of encoding direction . . . . . . . . . . . . . . 116 4.5 Example: The proposed directional encoding scheme . . . . . . . . . . . 117 4.6 Example: Beneﬁt of encoding negative information . . . . . . . . . . . . 117 4.7 Example: The importance of the start location of a trail . . . . . . . . . 119 4.8 Example: Determining prediction length via a recursive approach . . . . 123 4.9 Parameter selection for the predictor from [137] for 5m quantization . . . 127 4.10 Parameter selection for the predictor from [137] for 10m quantization . . 127 4.11 Results: Accuracy, varying the error function relevance parameter . . . . 131 4.12 Results: Histograms of the individual predictions (5m quantization) . . . 134 4.13 Results: Histograms of the individual predictions (10m quantization) . . 134 4.14 Results: Sample generalization error box plots (5m quantization, α = 2) . 135 4.15 Results: Sample generalization error box plots (5m quantization, α = 3) . 135 4.16 Normal probability plots and D’Agostino normality tests . . . . . . . . . 137
5.1 An illustration of the quantization process . . . . . . . . . . . . . . . . . 151 5.2 Example: Reduction of errors due to small route deviations . . . . . . . . 153 5.3 Representation of the probability density function output of the model . 154 5.4 Results: Spatial predictors, sample generalization error box plots . . . . . 162 5.5 Results: Hybrid predictors, sample generalization error box plots . . . . . 164 5.6 Results: Intersect predictor, sample generalization error box plot, includ-
ing when no objective value function is used . . . . . . . . . . . . . . . . 166
6.1 A graphical representation of the Cover Tree properties . . . . . . . . . . 179 6.2 Examples where the Hausdorﬀ distance criteria would select the intuitive
solution (a) and vice versa (b) . . . . . . . . . . . . . . . . . . . . . . . . 184 6.3 Example: Multiple observations matched. . . . . . . . . . . . . . . . . . . 186 6.4 Parameter selection for the predictor from [137] for 5m quantization . . . 191 6.5 Results: Operations per prediction. Brute force vs. Augmented Cover Tree194 6.6 Results: Operations per prediction. Augmented Cover Tree . . . . . . . . 194 6.7 Results: Performance, varying the error function relevance parameter . . 196 6.8 Results: Histograms of the individual predictions . . . . . . . . . . . . . 196 6.9 Results: Sample generalization error box plots (α = 5) . . . . . . . . . . 197 6.10 Normal probability plots and DA´ gostino normality tests . . . . . . . . . 198
7.1 Screen shot of the initial participant questionnaire. . . . . . . . . . . . . 211 7.2 Screen shot of the pre-topic questionnaire for the topic straight road. . . . 213
vi

List of Tables
1 Common mathematical symbols . . . . . . . . . . . . . . . . . . . . ix 2 Common mathematical symbols continued. . . . . . . . . . . . . . . x 3.1 Comparison of the estimated variance from diﬀerent variance estimators . 97 4.1 Pairwise comparisons of the predictors (α = 2) . . . . . . . . . . . . . . . 139 4.2 Pairwise comparisons of the predictors (α = 2) . . . . . . . . . . . . . . . 140 5.1 Example of potential misleading conditional probabilities . . . . . . . . . 147 6.1 Pairwise comparisons of the predictors (α = 5) . . . . . . . . . . . . . . . 200 7.1 The six categories evaluated and their corresponding topics . . . . . . . . 212 7.2 Mean click-through relevance proportions (system precision of 16.67%) . 214 7.3 Mean click-through relevance proportions (system precision of 83.33%) . 215 7.4 Predicted means for the system precision and category interactions . . . 217
vii

Table of Abbreviations

CRAWDAD Community Resource for Archiving Wireless Data At Dartmouth

CTW

Context-Tree Weighting

DBSCAN Density-Based Spatial Clustering of Applications with Noise

EM

Expectation Maximization

EMD

Earth Movers Distance

EPSRC

Engineering and Physical Sciences Research Council

GLMM

Generalized Linear Mixed Model

GPS

Global Positioning System

HDC

Hausdorrf Distance Criteria

HMM

Hidden Markov Model

ICA

Independent Component Analysis

KD-Trees k-Dimensional Tree

LZ

Denotes compression algorithms by Abraham Lempel and Jacob Ziv

M-Tree

Metric Tree

MAE

Mean Absolute Error

MDP

Markov Decision Process

NP-Hard Non-deterministic Polynomial-time Hard

OSGB

Ordnance Survey of Great Britain

PCA

Principal Component Analysis

PMF

Probability Mass Function

PPM

Prediction by Partial Match

PQM

Probabilistic Quality Measure

PST

Probabilistic Suﬃx Tree

RLD

Relative Linkage Disequilibrium

RMSE

Root of the Mean Square Error

ROC

Receiver Operator Curves

SPM

Sampled Pattern Matching

TMC

Total minimum Distance Criteria

TREC

Text REtrieval Conference

WLAN

Wireless Local Area Network

viii

Common mathematical symbols

This list is provided for clarity, with all symbols more fully introduced in-text. All other symbols should be considered to be only locally deﬁned as per the in-text declarations. Angle brackets denote ordered sets and curly braces denote unordered sets. In this thesis location is encoded both as a continuous quantity in a two-dimensional plane and as a set of symbols representing a partitioning of a ﬁxed spatial area. This is distinguished consistently though a slight notation change for clarity. When a symbol refers to the latter encoding this is denoted by using a sans-serif font. This is shown explicitly in this table.

Table 1: Common mathematical symbols

Symbol deﬁnition

Description

D = {H1, . . . , H|D|}



 

h1, . . . , h|H|

or

H=

 

h1, . . . , h|H|

h

A set of historic trails (the unordered data set of historic trail observations) A historic trail observation (an ordered set of point observations) A point observation (feature vector) including continuous location coordinates. E.g. h = [x, y]

A point observation (feature vector) where the continuous

location has been mapped to a symbol s ∈ S where S is a h
ﬁnite set of symbols and each s represents a non-overlapping

spatial region. E.g. h = [s]

K = {K1, . . . , K|K|} where K ∈ D

A subset of the total historic trail observations used to train a prediction algorithm.

T = {T1, . . . , T|T |} where T ∈ D

A subset of the total historic trail observations used to test a prediction algorithm.

ix

Table 2: Common mathematical symbols continued. . .

Symbol deﬁnition

Description

A predictor trained using the observations in K. The ·

P(K, ·)

represents an arbitrary input observation. Replacing the ·

a given E provides a prediction R.

An observed ordered set of point observations making a



 

e1, . . . , e|E|

or

E=

 

e1, . . . , e|E|

partial trail as seen so far (evidence) used as input to a prediction algorithm. When evaluating predictors E is one part of a historic observation, T ∈ T with the subscripts

referring to identical indexes within T .

e

a point observation (feature vector) of the same type as h

e

a point observation (feature vector) of the same type as h



 

r|E|+1, . . . , r|R|

or

For a given E and P(K, E), R is the prediction result which

R=

 

r|E|+1, . . . , r|R|

is an ordered set of predicted point observations.

r

a point observation (feature vector) of the same type as h

r

a point observation (feature vector) of the same type as h



 

f|E|+1, . . . , f|T |

F=

 

f|E|+1, . . . , f|T |

A known continuation of an partially observed trail, E. or Used in the evaluation of predictors F is the remainder
of the trail T from the test set T from which E was constructed.

f

a point observation (feature vector) of the same type as h

f

a point observation (feature vector) of the same type as h

x

Abstract
Pedestrian movement prediction and in particular, route prediction, can provide valuable information to location-aware services, enabling, for instance, ahead-of-time notiﬁcations speciﬁc to the route, data preparation and eﬃcient pre-fetching and data caching. Predicting human movement, however, is a complex task and this thesis extends stateof-the-art in a promising approach, namely prediction based on the construction and application of statistical models and data mining techniques to large volumes of historic mobility traces. Such data is rapidly becoming available in vast quantities, with movement data able to be contributed back by a growing number of location-aware consumer devices such as smart phones. The approach predicts future movement based on patterns in large sets of past movement, based on assumption that human movement is somewhat regular, an assumption which is both intuitive appealing and has been veriﬁed in recent studies. This thesis provides a comprehensive body of work on ﬁne-grained pedestrian level route prediction, for which limited prior research exists, with most previous investigations focusing on (1) prediction after quantization to regions of interest, (2) predicting single destinations, either ﬁnal- or next-step, or (3) focusing on the transportation domain where movement is constrained to a known road network.
Following an introduction motivating and highlighting the challenges of pedestrian route movement prediction (Chapter 1), the ﬁrst part of this thesis reviews previously proposed techniques with respect to movement prediction in diﬀerent situations in the context of pedestrian route prediction from ﬁne-grained logs (Chapter 2). Following this the evaluation of prediction techniques is examined and extended. Noting a lack of consistency in evaluation (for example, only one third of relevant literature share a common evaluation metric) and lack of statistical analysis, a set of current best practices are developed providing simple guidelines for researchers wanting to compare movement route prediction algorithms and additionally providing a solid framework for evaluations throughout the thesis (Chapter 3). The state-of-the-art in prediction models for route prediction is then extended with the examination of modelling techniques and the subsequent development and evaluation of novel prediction algorithms. Speciﬁcally the reliance on accurate sequence information in historic and input routes in current state-of-the-art predictors is questioned noting that, when considering data aggregated from sources such as consumer grade global positioning systems (GPS), recording errors and other

inaccuracies often violate this assumption. Acknowledging this, and the computational challenges incurred by not assuming accurate sequence information, two novel predictors are proposed in Chapter 4 and Chapter 6, respectively aimed at resource limited and high powered devices. The results show that viable solutions exist to the computational challenges and more accurate predictions can be obtained in this fashion. In conjunction with the proposed models an investigation is made into the use of measures other than conditional probability for measuring the utility of predictions in large data sets where the input matches multiple diﬀerent historic routes equally well (Chapter 5) concluding that while conditional probability is a good choice it is not the only choice with a number of other measures performing equally as well with potential theoretical beneﬁts. Finally an alternative form of navigational information from the virtual domain, image search click-through data, is investigated via an in-depth user study showing that incidentally, left trails by users within this space are accurate enough to use as a basis for predictions (Chapter 7).
ii

Declaration
I declare that: • this thesis presents work carried out by myself and does not incorporate without acknowledgment any material previously submitted for a degree or diploma in any university; • to the best of my knowledge it does not contain any materials previously published or written by another person except where due reference is made in the text; and all substantive contributions by others to the work presented, including jointly authored publications, is clearly acknowledged.
Signed:. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Date: 17 January 2012

Acknowledgements
Throughout my PhD I’ve been lucky enough to spend a great deal of time outside of the University of South Australia and Australia in general, meeting and working with a large number of people who are both interesting and generally just good fun. Therefore, in opening my acknowledgements I’d like to extend thanks to everyone I’ve met in my travels, at home and abroad, overall it has simply been a great three years.
From the University of South Australia, I’d like to ﬁrstly thank my primary supervisor, Helen Ashman for giving me the motivation to undertake this PhD in the ﬁrst place, for providing me with the opportunity and contacts from which the majority of my travel was based and her constant support and advice. I’d also like to thank Ivan Lee, my secondary supervisor, for his support and general discussions. Then of course I’d like to say a big thanks to those I have worked with and along side me in our group and at UniSA in general. Of everyone I would like to explicitly thank Jan-Felix Schmakeit who has helped me out on many occasions and generally been good fun to both work and travel with. Finally I’d like to extend a huge thanks to Chris Brien, who aided in the statistical design and evaluation of my ﬁrst major study, increasing my motivation that has led me to acquire a much more in-depth statistical understanding, which is reﬂected in this thesis with a chapter dedicated to evaluation methods. On this note I’d also like to thank Cees Vandereijk from The University of Nottingham who also provided statistical advice while I was visiting.
Throughout my travels I have been fortunate enough to met a number of exceptional people of who I am in now in regular contact and who have played a large hand in the direction my thesis has taken. Of primary note is James Goulding, Tim Brailsford and Mark Truran, all of whom I can not thank enough for their support, collaboration, interest, encouragement, advice and, importantly, ensuring that my time in the UK (and in fact a number of other countries) was a great deal of fun. Additionally I’d like to thank John Miller and the team for having me as an international student volunteer at two World Wide Web conferences, it was a blast.
I’d also like to thank the people I worked with and spent time with at Southampton University and speciﬁcally Jonathon Hare and Paul Lewis for having me in their group for three months. Similarly I’d like to thank everyone in the Horizon Digital Economy Institute at The University of Nottingham where I spent six months - a great place to

work due to great people. My PhD, my travel and the subsequent opportunities would not have been possible without the ﬁnancial support from the APA Scholarship funded by the Australian government and the scholarship topup and personal development fund provided by the University of South Australia. In addition I’d like to say a huge thanks to the board, contributors and everyone else who continually make the Maurice de Rohan International Scholarship possible, of which I was a recipient. The time abroad it enabled made a signiﬁcant impact on my PhD for which I am grateful. Last but not least I’d like to thank my family and my girlfriend, Anne Quandt for their invaluable support.
vi

Chapter 1
Introduction
Movement and location play a large part in our daily lives providing new opportunities for location based applications. In particular, the adoption of global positioning system chips (GPS) within mobile phones provides meaningful locational data at the level of the individual, in contrast to the previous, coarse-grained cell tower or wi-ﬁ location data. This increased precision has, and continues to, enable an increasing range of tailored services, such as Google maps, location-aware search and navigation support systems, assisting our day-to-day tasks and providing novel leisure applications (e.g. foursquare). While understanding the user’s immediate locations allows a great deal of applications, understanding and reasoning about possible future locations, and ideally route information, provides a much greater opportunity for personalised services and more generally intelligent mobile agents. Understanding future possible locations and routes provides invaluable information to help services to identify what information they should deliver, when, if at all, the information should be presented and importantly what information should be prepared and sent ahead of time to the mobile device due to operating constraints such as connectivity, bandwidth and battery. Human movement, however, is a complex phenomenon and its prediction non-trivial. Occurring in a complex and ever-changing environment, systems based solely on rules are exceedingly complex to develop if not impossible. As such one of the most promising approaches to predicting human movement has been to take advantage of aggregated human intelligence left by the users of these location-aware devices. In this way users of location-aware services not only utilise the services but also contribute back to it implicitly via their owner’s movement decisions. Based on the movement logs kept by these devices, or devices with which they make contact, research into movement prediction via
1

statistical models utilising this information has been steadily increasing. Many diﬀerent approaches exist, however, with approaches diﬀering in varying aspects including what type of location data they use, the features they extract, the speciﬁc structure and hence assumptions they enforce as well as vast diﬀerences in the way erroneous data is dealt with. Based on the idea of aggregating and using the encapsulated intelligence provided implicitly by those contributing each individual movement pattern, the approaches can be considered to be utilising the wisdom of the crowds, relying on the fact that human movement is somewhat regular. Such an assumption has solid grounds, with a recent high proﬁle study [77] into understanding human mobility patterns demonstrating that “human trajectories show a high degree of temporal and spatial regularity”.
Work in the ﬁeld initially focused on applications using the coarse-grained location information provided by cell towers to predict mobile user’s movements in order to ameliorate coordination of cell service provisioning (e.g. [38, 148]) and later WLAN hand-oﬀs (e.g. [127, 146, 182]). Based on these coarse-grained location information sources and consequently making coarse-grained predictions, researchers were able to show acceptable levels of next-location prediction accuracy with relatively simple models (see, for example, the comparison of techniques in [182]).
More recently a large body of work has developed from the use of logged GPS trails for movement prediction in the ﬁeld of transportation where satellite navigation devices have been become commonplace. In this ﬁeld prediction techniques have been motivated by applications such as improving transportation system management and traﬃc ﬂows (e.g. [102]), ahead-of-time notiﬁcations about upcoming traﬃc hazards or providing well-timed information about upcoming points of interest [69] and improving hybrid vehicle eﬃciency by incorporating upcoming road conditions [102].
Location-aware personal mobile devices with embedded GPS, such as smartphones, are now common and have additionally fuelled research into movement prediction at the level of the individual. It is in this domain that this thesis is focused. Diﬀering from previous contexts in both scale and complexity, new techniques have been proposed in an expanding ﬁeld of research which has seen the idea of the intelligent mobile agents further explored. Examples have included locative reminder systems [132], location based advertising [109] and intelligent navigation aids to alert people with cognitive disabilities when they had deviated from their normal route [150].
Current research has only touched on the possible applications enabled by the availability
2

of highly accurate movement predictions, with many of the applications having vastly diﬀering properties. For instance the use of coarse-grained cell tower location information is only valid for certain applications where the resulting coarse-grained predictions are acceptable. Inherently diﬀerent to prediction from ﬁne-grained prediction data, the data is characterised by shorter contexts with next step predictions relatively more important. Additionally prediction of vehicle routes has many diﬀerent properties to predicting pedestrian routes, with the road systems presenting a much more constrained environment allowing the precision of the incoming GPS readings to the corrected using techniques such as map matching. Additionally the road networks help to limit the possible prediction routes. Finally applications that desire route predictions place extra demands on the prediction system preventing the high level quantization of data into logical locations which simpliﬁes the problem in terms of both computational complexity additionally abstracting away issues of noise inherent in the use of consumer grade sensors. For instance [137] investigate prediction by ﬁrst identifying regions of interest such as train stations and museums.
1.1 Problem deﬁnition: route prediction
With the exception of chapter 7, which considers an alternative data source for which route prediction is of interest, this thesis primarily addresses the problem of route prediction from ﬁne-grained location data. In this thesis it is assumed that each individuals’ location data streams are able to be segmented into logical trails. Trails refer to sequences of point observations that start and end at known or inferred locations. New methods for the inference of such locations is beyond the scope of this thesis and hence segmentation is not speciﬁcally addressed here. However, many techniques exist including the use of loss of signal [132], pauses in movement [69] or other more involved learning methods [119]. These existing techniques are discussed more fully in chapter 2 for completeness as part of section 2.2 on encoding GPS data.
Trails rather than per person location sequences are used as they enable the encoding of the deﬁnition of a route as a pre-processing step. This allows the thesis to focus on the prediction algorithms by employing a consistent deﬁnition of a route and the same pre-processing step.
Speciﬁcally the problem addressed in this thesis is:
3

Figure 1.1: Figure visually showing the prediction process labelled with the corresponding symbols used throughout this thesis.
• Given a set of historic trails: D = {H1, . . . , H|D|} where a trail H = h1, . . . , h|H| and each point is equal to some feature vector (minimally encoding location), for instance: h = [x, y]
• And given an observed input trail E = e1, . . . , e|E|
• Provide a resulting prediction R = r|E|+1, . . . , r|R| , |R| ≥ 1 of arbitrary length indicating the route that is expected to be taken, where r is a feature vector encoding only locational information (e.g. r = [x, y]).
The historic trails are typically used by the prediction algorithms as training data, K. In real world use all historic data would be used as training data, so K = D. However, as will be discussed in chapter 3, for evaluation purposes this is not the case and so throughout this thesis the separate symbol K is used to denote the training data set.
Figure 1.1 visually illustrates this.
Note that H, E and R are all ordered sets. The deﬁnition of route here is made explicit by the deﬁnition of rk as a feature vector of locational data (e.g. [x, y] or [longitude, latitude]) only. The exclusion of other features in the output, and hence the evaluation1 of route prediction algorithms, is intentionally made to focus on route prediction and not on predicting the general context2 or on predicting explicitly timed
1see chapter 3 2Where the context is deﬁned as the information encoded in the feature vector pj.
4

routes. Such a decision is taken so as to not conﬂate the accuracy of algorithms with respect to the prediction of a route with the prediction of the timings of the route. This is important due to the relevance of route prediction without timing information. For instance routes are generally considered to be continuous travel by the individual3, may be quite short and timing information can be generated post-hoc if required (e.g. based on the pedestrians current speed). An example of an application where timing information is less important is location-based advertising, where predicting accurately that pedestrian will go past the potential advertisers shop is more important than the exact time that person will go past, especially considering you know they will go past soon.
Finally it is of note that in the deﬁnition of route prediction the input context is not restricted. While in general it is likely that by including additional contextual information an increase in prediction performance could be obtained, in this thesis only the encoding of time and locational information is used. This focus aims to increase the performance of the prediction algorithms under the minimal amount of information expected. It is of note that in the case of the novel predictor proposed in chapter 6 the extension to multiple contexts, both in the input and output, is trivial. Extensions of the other algorithms are left as future work.
1.2 Challenges of pedestrian route prediction
This thesis primarily focuses on the use of GPS data as the movement data from which predictive systems are developed, deviating only in chapter 7 where a diﬀerent source of movement data is evaluated with respect to the ﬁrst challenge discussed below, the challenge of mining incidentally-collected data. Despite this focus it is worth highlighting that many of the challenges, such as noise and feature encoding, apply more broadly to forms of data other than GPS location sequences. Speciﬁcally, only generic locational data is used as input to the proposed and evaluated algorithms, enabling the direct application of the algorithms to diﬀerent locational data. Therefore, while the majority of the discussion takes place in the context of GPS data, the approaches can be applied more generally.
3This is generally enforced by the way the trails are created from the raw GPS streams in the ﬁrst instance with a common approach deﬁning trail start and end points based on points of stoppage.
5

Challenges with respect to modelling and predicting pedestrian movement from GPS data can be roughly grouped into ﬁve diﬀerent categories. Firstly there are the general challenges associated with mining incidentally-collected data. These include challenges relating to noise within the data and varying sample rates. Secondly there is the challenge of selecting what data to record and process (feature selection) and the tighly coupled challenge of how to encode the selected features (feature encoding). For example, in the case of location data, should location information be on a continuous scale? Should a set of logical locations be extracted? Or should a basic grid quantization be used?. The third challenge is the development of the predictive algorithms themselves. This includes how they internally represent (model) the historic data and the algorithmic mechanisms that allow predictions to be made both accurately and in a timely fashion. The fourth challenge is that of evaluation. In other words, given a prediction algorithm, how should it be evaluated? While all previous research has addressed this, no real consensus has been reached in the context of route prediction. Finally the ﬁfth challenge surrounds data collection, with the primary concern being privacy.
1.2.1 Challenges of incidentally-collected data
Incidentally-collected data has a set of unique challenges in many cases. This is especially true in the case of GPS data which is the data type focused on in the majority of this thesis, but also in other cases, as highlighted by the examination of logs indicating movement among search results on the World Wide Web in chapter 7. In both these cases the incidentally-collected data is prone to signiﬁcant and uncontrolled levels of noise, since the data collection mechanisms were not developed speciﬁcally with the application of prediction algorithms in mind.
With respect to GPS, consumer grade embedded units typically found in most mobile devices are of varying quality and therefore produce location readings of varying accuracy. GPS accuracy is also sensitive to a large range of factors including satellite positions, signal obstructions (such as buildings), atmospheric conditions and even where the device is held/placed on the user [89, 189]. In general it is also expected that a wide range of diﬀerent devices will contribute to the data set. This only exacerbates the previous issues, adding complexities associated with varying sample rates and the possibility of various error correction mechanisms built into speciﬁc devices such as automatic dead reckoning when the signal is lost (as highlighted in [11] for example). While it is pos-
6

sible to construct discrete prediction systems for individual devices, such a practice is suboptimal since the statistical prediction models generally perform better with more data, and always perform poorly with very little. Considering the vast array of consumer devices, the latter may very well occur in a real system. In summary, since the data must be assumed to be heterogeneous in device type and generally of relatively poor quality, very minimal guarantees with respect to correctness, sampling rate or in device error handling can be made. Ways of addressing these issues are discussed in chapter 2, section 2.2.
With respect to movement data among search results, as considered in chapter 7, the challenges of incidentally-collected data are generally characterized by errors not in the recording of the clicked resource, but in the persons unintended movement with respect to their current task. Such false navigational steps are much less common in spatial movement where the user can typically see (at least in the short term) and/or have a good idea where they are going and where the physical eﬀort of false steps is much higher. In navigation among search results the assumption that people will not make false steps is tenuous [99], with people often making decisions out of curiosity and/or with very little information about the subsequent page. Since the utility of many prediction applications is dependant on making useful predictions with respect to a goal (i.e. an information/destination goal) noise of this kind can be particularly detrimental. Therefore, the severity of these issues and the subsequent potential use of this form of data for movement prediction is discussed in Chapter 7. This step is an important pre-cursor to the application of prediction algorithms similar to those presented in this thesis. As noted in Chapter 8, however, such application is left as future work.
1.2.2 Challenges in feature selection and encodings
Prediction systems can only reason about features they know. These features are generally either extracted from the real world either via sensors or provided as input. As such a major challenge is identifying the features from which the models reason, more succinctly the question, what factors need to be modelled in order to make good predictions?. For instance, should only location (e.g. latitude and longitude coordinates) be modelled? What about sequence? What about weather? What about personal attributes like age? While all are interesting questions this thesis focuses on the issue of modelling sequence (and consequently to a lesser extent time) since it has typically been embedded in the
7

structure of previous prediction algorithms making its investigation more complex than altering feature vectors in existing prediction systems. Entwined with feature selection is the issue of how each individual feature is encoded.
A common choice of encoding location, for instance, is to determine signiﬁcant locations via clustering or other means (e.g. [11, 137]). This helps not only to reduce issues of noise with respect to the raw GPS readings, but also helps reduce the computational cost of the prediction algorithms by reducing the number of distinct items given as input. Such clustering, however, does not meet the requirement to provide route prediction as prediction algorithms based on this data can only predict these locations and not the routes in between. As such other options must be considered and computational issues dealt with via other means. In this thesis two speciﬁc instances of feature encoding are discussed. The ﬁrst relates to encoding location with respect to determining equality, speciﬁcally in the presence of noise. This is discussed primarily in chapter 2, section 2.2.1. The second instance is in conjunction with feature selection when considering the use of sequence as a feature. This is primarily addressed in chapter 6 although as feature selection and encoding is part of all prediction algorithms it is discussed throughout the thesis as required.
1.2.3 Challenges in developing pedestrian route prediction algorithms
This is the core challenge of this thesis, incorporating the challenges incidentally-collected data and feature selection and encoding. In addition to these aforementioned challenges, the development of movement prediction algorithms has non-trivial challenges relating speciﬁcally to the internal model that each algorithm uses to make a prediction. In this thesis data driven models of various forms are considered and hence this challenge relates to the model the algorithm builds based on the historic data given as a training set. The challenges in this area relate to how much historical context can be considered by the algorithm and how such context is used to ultimately improve the quality of the predictions. An often conﬂicting but equally important challenge is the challenge of ensuring that the algorithm is tractable.
In comparison to previous work using coarse-grain movement data, ﬁned-grain route prediction involves larger contexts. In addition while much previous work has utilized
8

a limited number of discrete locations, such techniques are not applicable and either the continuous nature of the data or vast numbers of discrete locations must be considered.
1.2.4 Evaluation challenges
Evaluation is clearly an important task. However, with respect to movement prediction, numerous diﬀerent evaluation metrics have been used, with many not being able to generalise to the output from other types of prediction (e.g. next-step versus route prediction). Clearly this is far from perfect, preventing even approximate comparisons of work within the domain. In chapter 3 a number of evaluation metrics are examined and a solution proposed for the class of movement prediction algorithms that provide some notion of sequence. In addition the challenges with respect to systematic data reuse in evaluation, as is generally required due to relatively small data sets, is discussed noting that normal statistical procedures can not be used since systematic data reuse results in dependent samples. To this end a set of current best practice procedures for the statistical analysis of movement prediction algorithms is outlined in chapter 3.
1.2.5 Data acquisition challenges
There are many data acquisition challenges, some of which are subject to much debate. In this thesis, the challenges of data acquisition are not addressed, with the exploration of these issues signiﬁcantly more involved than could adequately be covered within the bounds of this thesis. Of primary concern is the often-raised problem of privacy and the challenges and ethical considerations that must be taken into account when collecting such data. While in most cases this data is already collected either by explicit optin schemes (such as Foursquare4) or via the terms and conditions of the use of other location services (such as the location services part of Apple products5) much discussion has arisen, with examples of concerns such as “Revealing the location of your home to people you do not want to give your address to” and “Being stalked” reported in user studies in [187]. In response many privacy preserving schemes have been proposed (e.g. see [90] with respect to GPS traces). For a collection of work in the area see, for example, [18]. In this thesis the dataset used was the D-SCENT simulation dataset
4http://foursquare.com/ 5http://www.apple.com/privacy/ accessed 16th March 2011, section: Location-Based Services
9

[168]. The D-SCENT dataset was generated from an augmented reality simulation that was developed as part of the D-SCENT project funded by EPSRC at The University of Nottingham in the UK. Participants took on the role of workers constructing an Olympic site, performing a host of purchasing and building tasks. The simulation area was ﬁxed and featured 12 locations covering a 80, 000m2 spatial area. Sixty participants interacted with the game via G1 smart phones and their GPS data was collected every 5 seconds by a central server, with a log only being kept if the participant had recorded movement of at least 5 meters. The resulting data set contains over 30,000 GPS position readings. Therefore in the creation of this data set many of the normal privacy issues were avoided, with participants only being recorded on supplied devices within the ﬁxed area during the game to which they had consented to play. It is important to note that while tasks undertaken by participants were artiﬁcial, their movement across the simulation area was completely unconstrained and reﬂects dense, real spatial behaviour. However, for research to adequately progress real world data sets are required, and must be open, at least to the research community. In this case the issues of privacy have to be fully taken into account. This has already begun with repositories such as CRAWDAD6, although currently only one data set [162] relates to GPS logs from pedestrian level movement. Compared to the D-SCENT data it is signiﬁcantly smaller in terms of participants (between 8 - 32) and over a larger area meaning that in the overall data set the traces signiﬁcantly more spread out providing less chance for the algorithms to learn compared to the D-SCENT data and what would be expected in reality.
1.3 Overview and list of contributions
This thesis extends state-of-the-art in movement prediction from GPS data, explicitly focusing on low level route prediction as opposed to either next step, next goal, destination or region of interest predictions. While considered in the case of vehicle movement, low level route prediction has not seen comprehensive investigation in the case of pedestrian prediction for which GPS readings tend to be less accurate, cannot be snapped back to a known movement grid and has no restrictions on the types of movement patterns.
Throughout this thesis the core challenge of the development and examination of prediction algorithms is addressed and evaluated empirically though the comparison of the
6http://crawdad.cs.dartmouth.edu/
10

proposed algorithms to baseline algorithms from the literature (chapters 4 - 6). Ensuring a correct comparison and in light of a lack of a set of standards for evaluating movement prediction algorithms chapter 3 develops a framework for such empirical comparisons.
Informing the development of these successful algorithms a critical review of previous work in the ﬁeld is presented in chapter 2. This chapter considers the core challenge of developing pedestrian route prediction algorithms in section 2.1. Additionally the challenge of using incidentally collected data is considered through the examination of noise within movement datasets in section 2.2. This is achieved though a review of data sets reported in the literature and through the examination of the data set used throughout this thesis. The success of the subsequently developed prediction algorithms attest to the utility of this chapter.
Finally the thesis once more considers the challenge of using incidentally collected data, with ﬁnal chapter considering the utility of another form of movement data where the proposed algorithms have potential application, providing a clear conclusion based on a user study involving 67 participants.
The chapter-by-chapter contributions of this thesis are detailed below:
• Chapter 2 provides an in-depth critical discussion of previous methods in movement prediction, examining their applicability to the speciﬁc case of pedestrian route prediction for which minimal past literature exists. Additionally the properties of noise speciﬁc to GPS data is examined in-depth both with respect to observations from past literature and observations from the new data set utilised in this thesis.
• Chapter 3 addresses the absence of a set of standard practices in evaluation within the ﬁeld of movement prediction from logs of low-level behaviour. This is highlighted by the fact that within papers in the ﬁeld only a third share a common evaluation metric with another paper or compare results to other techniques and none report any levels of statistical signiﬁcance associated with their ﬁndings. This is addressed by motivating and contributing a set of current best practice procedures for the task of statistical analysis of movement prediction results, including the recommendation of an alternative distance metric which provides a more accurate measure of the distance between two routes.
• Chapter 4 contributes a computationally eﬃcient algorithm for prediction pedes-
11

trians routes from GPS data, motivated by the beneﬁts of utilising movement prediction in mobile devices with limited resources. While computationally eﬃcient, the algorithms shows performance close to that of more complex, full order predictions, in experiments utilising 5m grid based quantization.
• Chapter 5 contributes an investigation into the use of measures other than conditional probability for measuring the utility of predictions in large data sets where the input matches multiple diﬀerent historic routes equally well. The investigation is driven by the theoretical arguments put forward in numerous papers from the data mining community (see, for example [71]). In addition a new form of prediction output, probability density functions (heatmaps) is proposed and a hybrid approach to combining the spatial matching score (between the input and historic routes) and the measure of utility of the prediction is examined.
• Chapter 6 makes two signiﬁcant contributions. The ﬁrst is with respect to modelling challenges in the form of investigations into the relaxed modelling of time/sequence by treating it as part of the feature vector from which it can be modelled arbitrarily. The second is in the form of a novel prediction algorithm allowing eﬃcient computation of such models using multiple iterators over an augmented version of the cover tree proposed in [21]. Results show that certain time-relaxed encodings can outperform current state-of-the-art predictors.
• Chapter 7 looks toward alternative sources of reliable, but incidentally collected and logged navigation data contributing an in-depth user study involving over 67 participants. Speciﬁcally the chapter examines the reliability of click-through data from web based image search as informed movement in the informational space provided by the search engine. The study is provided in light of prior studies into document search based click-through data showing that the movement (clicks) is poorly informed with only 58% of the recorded clicks moving to pages considered relevant. In contrast, in this contribution, the alternative form of click-through data from image search is investigated showing a notably higher level of reliability, with clicks being reliable on average 84% of the time.
It is of note that chapter 7 was performed at an earlier stage in the PhD candidacy. The chapter details and evaluates the utility of another form of navigational data that can be collected implicitly, this time on the World Wide Web. While showing promise a large enough data set was not available during the candidacy and therefore GPS data was
12

used, leading to the contributions outlined above in the other chapters. Publications relating to work based on chapter 7 are listed below:
1. G. Smith and H. Ashman. Evaluating implicit judgements from image search interactions. In Proceedings of the Web Science Conference: Society On-Line, Athens, 2009.
2. G. Smith, T. Brailsford, C. Donner, M. Truran, J. Goulding and H. Ashman, Disambiguation from Web search selections. In Proceedings of the 2009 workshop on Web Search Click Data, Barcelona, 2009.
3. H. Ashman, S. Chaprasit, G. Smith and M. Truran. Implicit association via crowdsourced coselection. To appear, ACM Conference on Hypertext and Hypermedia, Eindhoven, 2011.
Publications for the work in other chapters is pending.
13

Chapter 2
Movement prediction in the context
of GPS data
Human movement in general is a complex phenomenon with the choice of route and destination dependent on an array of complex factors. Despite this, the beneﬁts of movement prediction and recently their demonstrated degree of high temporal and spatial regularity [77] has prompted a wide range of prediction techniques. These techniques almost exclusively construct (learn) an internal model from a set of training data provided a head of time from which the prediction is made. With respect to the pedestrian route prediction problem deﬁned in section 1.1 this training data is the set of historic trails, D. Note that while the term model can be used in a variety of contexts, throughout this thesis the term refers to these internal models built by the diﬀerent prediction algorithms/approaches. Speciﬁcally section 2.1 provides a critical review and overview of related work in order to lay the foundations and aid in addressing the core challenge of this thesis, the developing pedestrian route prediction algorithms. This critical review of previous prediction approaches is then followed by a look at the speciﬁc problem domain addressed in the majority of this thesis, GPS data, in section 2.2. Speciﬁcally this section focuses ﬁrst on the need and consequence of encoding such data using techniques employed in prior work and secondly on the process of trail identiﬁcation. Both are generally preprocessing steps performed on the data before it is used by the predictive techniques with the characteristics, quality and form of the data eﬀecting the utility of the various prediction approaches. Therefore, this section directly considers the two challenges of using and encoding incidentally-collected data identiﬁed in chapter 1. In addition, in
14

this section, the characteristics of the data set used in this thesis is considered as an empirical example of the properties expected from real world data sets for pedestrian route prediction.
2.1 Movement Prediction: A review
Prior work in the broad domain of movement prediction has generally been in contexts in which either the data is able (or desired) to be quantized. Examples include prediction of destinations and the prediction of movement of mobile phones between cell towers. Alternatively a large body of work exists with respect to the prediction of vehicle movements where erroneous location readings can be corrected to some degree by making use of known road maps and assuming the vehicles will only travel on roads. In contrast this thesis considers movement prediction from GPS data, explicitly focusing on low level route prediction for which limited previous work exists. As such, while a large body of literature exists, it is not always clear which approaches are advisable in this speciﬁc context. It is this issue that is addressed in this section, providing a strong theoretical basis for the development of movement prediction algorithms subsequently presented in this thesis.
The discussion is started with an introduction into the most basic movement models which formed the initial proposals in the ﬁeld (subsection 2.1.1). These models are part of a larger category, or modelling framework, called Dynamic Bayesian Networks. Bayesian networks provide a general framework in which movement can be modelled based on observed data.
A Bayesian network is a probabilistic graphical model represented as a directed acyclic graph. Nodes in the graph represent variables, called random variables. Intuitively1 a random variable represents a probability function over a state space. For instance consider a spatial region S quantized into a set of locations via a three-by-three square grid such that S = s1, .., s9. A random variable, location, is a probability function over S indicating the probability of each state (s1, .., s9). Thinking of it another way, L can be considered a variable which takes the value si with the probability deﬁned by a probability function over S. Edges in the acyclic graph correspond to conditional dependencies between these random variables. In the basic case these random variables
1For a formal explanation see, for example, [144].
15

(e.g. location) is a single, ﬁxed, probability distribution. In the context of movement prediction, however, it is additionally desired to reason with respect to time.
Bayesian networks have been extended to enable reasoning with respect to time. Known as Dynamic Bayesian Networks random variables are constructed that represent a probability function over a state space (e.g. location) for a ﬁxed, typically relative, time slice (e.g. now, or the time before now). This results in a distinct random variable per time slice for a given state of interest (e.g. location). In this chapter such variables are denoted using the same symbol but with a superscript representing the time slice (e.g. St, St−1 with t representing a time index). The dependency of time is then modelled via edges in the graph. A simple example is shown in ﬁgure 2.1 and a more complex example shown later in ﬁgure 2.4. This large class of models adds an assumption to the general framework, namely that an event can cause another event in the future, but not viceversa [73]. It is important to note that this framework provides the ability to develop a wide variety of models. These models can which range from very general and almost entirely data-driven (under some basic assumptions), to well-speciﬁed models which utilize and rely on external knowledge of the world provided by the model designer. In all cases, however, the problem space is deﬁned in the structure of the model and as such these models can be thought of as partially data-driven models, where the parameters are data-driven but the model structure is not. More fully data-driven models have been proposed. The most-heavily used can be though of as an extension to simple Bayesian Network models (Markov models) allowing variable length contexts. These approaches make up part of a class of algorithms known as Universal Predictors.
The discussion is continued in section 2.1.2 the more complex Dynamic Bayesian Network models are discussed under the category of partially driven data models. Following this section 2.1.3 discusses more fully data driven models including Universal Predictors and an alternative paradigm for movement prediction methods, pattern matching. In contrast to Bayesian Network models and Universal Predictors, the pattern matching paradigm provides an alternative framework for addressing movement prediction with many parallels and shared data structures to a number of implementations of universal predictors. Conceptually simple, and allowing the application of domain knowledge to address issues of noise, the pattern matching paradigm has been applied in a number of recent state-of-the-art proposals to modelling movement.
Throughout this section a distinction is made between providing an overview and critical
16

review of the related work with respect to the task of pedestrian prediction. Speciﬁcally, for each class of approaches, the techniques are ﬁrst brieﬂy described providing the reader with a general overview. Following this a summary section provides a critical review of the approaches with respect to the core challenge addressed in this thesis, the development of pedestrian route prediction algorithms. This allows readers who are either familiar or not interested in the speciﬁcs of each approach to easily access the core discussion of this chapter.
2.1.1 An introduction to movement prediction models
The most straightforward movement models ﬁt within the framework of Dynamic Bayesian Networks and maintain the probability of the next state (for example, location) conditioned on a ﬁxed history length. Computationally cheap and straightforward to implement, the models can perform well but only if the process being modelled approximately meets the strict assumptions made with respect to the future’s dependence on the past. Importantly, these models provide the conceptual foundation for the discussion of the more complex models.
First order Markov Models
The most basic Dynamic Bayesian Network is a ﬁrst order Markov model. A ﬁrst order Markov model contains only one random variable (a global state of the system) and adheres to the Markov property that the next state only depends on the previous state. In the context of movement prediction the random variable typically (at least) represents the location and so a ﬁrst order Markov model builds a model to answer the question what is the probability of being in a spatial region next, given the current region?. Figure 2.1 shows the graphical representation of the process. Recall that in this chapter random variables over the same state space (e.g. the set of all possible discrete locations) but at diﬀerent time points are denoted using the same symbol but with a superscript representing the time slice with t representing a time index.
This basic form of a dynamic Bayesian network was one of the ﬁrst techniques proposed for movement prediction from GPS log data [11], although such techniques had been considered earlier with respect modelling movement at the coarse-grain level of mobile phone tower traces to aid in service provisioning [22]. The model, however, has
17

St

S t+1

Figure 2.1: First order Markov model. The conditional probability P (st+1|st) is modelled. t represents a time index.
a number of shortcomings. Of primary importance is their lack of predictive power [11]. This is due to the model’s inability to model dependencies beyond the last time step directly. By not modelling the dependencies the model loses the ability to discriminate between diﬀerent inputs where the diﬀerence occurs beyond the modelling horizon. Clearly such information loss in general is undesirable and has a detrimental eﬀect on performance.

Markov Models of ﬁxed order
The most basic extension of ﬁrst order Markov models takes into account ﬁxed histories of a pre-speciﬁed length, k, thereby modelling higher order dependencies of order k. Called kth order Markov models, the state of the model depends on the previous k time steps. An example a 3rd order Markov model is shown in ﬁgure 2.2. In the case of basic movement prediction where only location is considered these models answer the question what is the probability of being in a spatial region next given the current and k − 1 previous regions?

St−2, St−1, St

S t+1

Figure 2.2: Markov model of order 3. The conditional probability P (St+1|St, St−1, St−2) is modelled. t represents a time index.
Higher order Markov Models were investigated by [11, 22] and later [182] with respect to movement prediction. [22, pg 8] note that “lower order models mislead the algorithm designer by projecting an under-estimate of uncertainty” but show that there will be a limit whereby the improvement gained by increasing the order will be insigniﬁcant. This limit intuitively depends on the longest length in the historic data. The authors then go on to highlight that the order required is in fact the length of the input for a given
18

prediction. The LZ78 compression algorithm is then proposed to automatically arrive at this order for each prediction input. Models based on LZ78 and variable lengths in general are no longer deﬁned by the Markov property, and more over are fully datadriven models, in that the data is now being used to build the structure of the model. These types of models are further discussed in section 2.1.3.
The use of ﬁxed or variable length histories does come with a drawback, however, as the model state space is drastically increased (the number of states is equal to n × k! where n is the number of states and assuming k < n). Due to this, it is much more likely that the input will not match a state that has been seen before and a prediction will not be made. A solution to this problem is to fallback [182] to a lower order model and try again. Fallback here refers to the process of using a lower order model if a match cannot be found for the input in a higher order model. For example consider a 2nd Markov model where the random variable in question is location in a world with only nine possible locations s1, . . . , s9. Consider the following historic trails from which a 2nd order Markov model is built:

s4 → s2 → s3 s6 → s5 → s2 s6 → s5 → s2 s8 → s5 → s9

and the input:

s7 → s5

A 2nd order Markov model has not seen the transition s7 → s5 and hence will not make a

prediction. Knowing that no prediction could be made it is then possible to use a lower

order 1st order Markov model and use only the last seen symbol, s5, in the input. This

then

results

in

the

prediction

of

the

location

s2

(P (s2|s5)

=

2 3

vs

P (s9|s5)

=

1 3

).

In the case of ﬁxed length Markov model this would require additional probabilities to be stored, increasing the size of the model. In variable order approaches, as discussed in section 2.1.3, the process involves simply shortening the input and rerunning the algorithm and does not involve any additional storage costs.

19

Markov Decision Processes
Markov Decision processes (MDPs) are a mathematical framework for modelling decision making, rather than simply being a model. Markov Decision processes are characterized by:
• a set of known states (S), e.g. a set of locations represented by symbols s1, . . . , s|S|
• a set of known actions (A = set of all actions, Asi = set of all actions available from state si) that can be performed in each state, e.g. move right
• a set of transitions probabilities (P (si|sj, a)) mapping the result of taking an action (a ∈ A) from a state (si) to another state sj ∈ S
• a set of rewards (R(si, sj, a)) for transitioning from state si to sj via action a.
A Markov Decision process makes a number of assumptions:
1. Stationary preference: Consider two state sequences, generated by executing a set of actions, that share the same start state (e.g. s0, s1, s2 and s0, s1, s2). In any subsequences obtained by truncating the sequences by a common oﬀset the order preference should be unchanged. With respect to the example, this means that the order preference between the sequences s1, s2 and s1, s2 is the same as between s0, s1, s2 and s0, s1, s2.
2. Markvoian transition model: The probability of transitioning from a state given an action to another state only depends on the current state.
Given a known destination, MDPs provide a framework for optimally determining a plan of actions to perform at each state to reach the known destination. As such, given the set of Markovian transition probabilities which only describe single state transitions (a ﬁrst order Markov model), MDPs can be seen as a framework to stitch together the optimal solution given the assumptions.
Of relevance to pedestrian movement prediction is the use of MDPs in imitation learning [160] within the robotics literature. Imitation learning involves making observations which are assumed to be generated from another MDP which acts as a blackbox to the learner. Under this assumption an approximation of the reward function can be learnt resulting in an MDP of similar observable behaviour. If the observations were in fact generated by another MDP then the observations should be reproducible. Since pedestrian movement is not generated from MDP only an approximation of the behaviour
20

can be expected. The degree of the approximation depends on how closely the observed behaviour ﬁts the MDP assumptions. In [205] this technique was employed utilizing a novel learning technique. Learning a MDP from observations the authors aimed to predict the movement of pedestrians to aid in robot movement planning. Given the current state the destination probability over all states was calculated. Using this information and the plan of action (as calculated by the MDP) the probability of predictions of arbitrary length to the determined destination(s) given the current state were determined, hence generating predictions.
In the context of pedestrian route prediction, it is of note that MDPs can still only model the set of behaviours describable under the ﬁrst order Markov assumptions (state transitions are only dependent on the current state in the model). As a result MDPs can not utilize the full observed history of a partial trail to discriminate between more complex cases of movement behaviour. It is worth noting, however, that since the learnt model in the case of imitation learning is not necessarily the same as that created by applying the Markov property to the data directly and in some cases the MDP may exhibit superior predictive performance. This is because the only goal of a MDP is to mimic the behaviour detailed by the real movement patterns. However, this is not examined in this thesis.
Summary
In summary, ﬁxed length Markov models of any order are suboptimal predictors if the process being modelled does not match the order assumption made. Even if the ﬁxed length, k, is known at least k consecutive observations need to be made before a prediction can be made, thereby limiting the application of the system. Additionally, without specialized data structures and fallback mechanisms similar to those employed in variable length Markov models, higher order Markov models can easily fail to make predictions while requiring a large number of probabilities to be stored. This quickly becomes intractable for large k. Lower order Markov models, however, still provide the computationally cheapest prediction model on which tractable planning and learning algorithms have been developed. An example is Markov Decision Processes which address the issue of predicting multiple steps into the future in an optimal way given some modelling assumptions and a known destination rather than simple recursive calls to the prediction algorithm.
21

Considering the speciﬁc challenges of pedestrian route prediction it is unlikely that a ﬁxed order model will suﬃciently model pedestrian movement. The is because people do not make movement decisions based only on their current state, or a foreseeably ﬁxed number of states, but rather according to high level goals. Recognised in the literature, this has led to two major approaches in the development of prediction models with respect to ﬁne-grained movement prediction. The ﬁrst seeks to build completely datadriven models, assuming that peoples past behaviour is indicative of future behaviour in general. The second seeks to model certain aspects of the world that cause the behaviour. This is done using knowledge supplied a priori. Conceptually this may be the individual’s goal. Models following this approach have focused on a class of dynamic Bayesian networks that extend Markov models by introducing unobservable states along with the required corresponding model structure and inference algorithms. In contrast purely data-driven approaches have typically been characterized by variable length Markov models and other pattern matching techniques. In both cases there is also the assumption that enough data exists to build and adequately train the models.
2.1.2 Partially data-driven models
Partially data-driven models seek to model certain aspects of the world that cause the observed behaviour using external knowledge supplied a priori. At a high level an example is the individuals’ goal, and at a lower level includes things such as road segments and transport modes. Modelled as unobservable states, these higher level variables are modelled using a number of techniques of varying complexity. Techniques applied to movement prediction have included Hidden Markov Models using Kalman Filters for inference, hierarchical dynamic Bayesian networks and conditional random ﬁelds.
Hidden Markov Model and the Kalman Filter
A regular Markov model assumes the states and state transitions are known and that at any time the current state is known to be one of the model states. In contrast the Hidden Markov Model (HMM) assumes that the states can not be directly observed, and that a set of observable symbols provide evidence that the system is one of the unobservable states. These unobserved states must be supplied by the model designer. Transitions are still between the unobservable states, but these can not be calculated
22

directly from the data, since the historic data is only related to the observed symbols. Therefore a fully speciﬁed HMM requires:
1. a set of unobserved states
2. transition probabilities between the unobserved states
3. a set of observable symbols
4. a probability matrix describing the probability of seeing an observable symbol given the system is actually in a speciﬁc unobserved state
An example of a HMM compared to a standard ﬁrst order Markov model is shown in ﬁgure 2.3. In the ﬁgure recall that nodes correspond to random variables. In other words, probability distributions over a state space. In the case of X, the unobserved variables, the state space is the user-deﬁned set of unobserved states. In the case of E the observed variables, the state space is the user-deﬁned set of observable symbols.

Ot

Ot+1 (a)

Xt

X t+1

(b)

Et

E t+1

Figure 2.3: Graphical representation of a ﬁrst order Markov model (a) vs. a ﬁrst order Hidden Markov Model (b). E represents observed variables (evidence), X represents unobserved (hidden) variables.
Prediction can then performed using a Kalman ﬁlter. Future positions are predicted using the Kalman ﬁlter’s predict step repeatedly. Kalman ﬁlters seek to predict the hidden variables state by using a two-step process [133]. In the ﬁrst step a prediction is made based on the previous hidden state estimate and the transition model between the hidden states. In a Kalman ﬁlter this model is the transition probabilities between the hidden states combined with a normally distributed variable representing the process noise with a mean of zero and a model speciﬁc standard deviation. The prediction step represents the selection of the hidden variables next state with the highest probability given no additional knowledge and is shown in equation 2.1, where xi represents the state of the hidden variable at time i and ei represents the observed symbol (evidence) at time i.

23

p(xt|e1:t−1) = p(xt|xt−1)p(xt−1|e1:t−1)dxt−1

(2.1)

In the second step a measurement is taken/received and the probabilities adjusted, calculating the probability given the new piece of evidence. Known as the update step, the equation is shown in equation 2.2.

p(xt|e1:t) ∝ p(et|xt)p(xt|e1:t−1)

(2.2)

[128, 170, 180] all use such an approach to predict movement, although [128, 170] mainly focus on very short prediction horizons before utilizing the update step of the Kalman ﬁlter. As such the systems are aimed more at providing location estimation rather than prediction. Of note is [128] in which an extended Kalman ﬁlter2 is used and the prediction step utilized to provide predictions continuously until a measurement is available. The authors note the importance of receiving frequent position measurements noting that otherwise cumulative errors can result in an estimate with poor accuracy. Such an observation can be explained by examining the prediction equation on its own. In the absence of any update, the model reverts to a simple ﬁrst order Markov model, where the ﬁrst order Markov model is the hidden states and the transition matrix between these states. In this case the model returns to simply making the locally most likely decision at each timestep, irrespective of longer term goals [180, 205].

Hierarchical dynamic Bayesian networks
The most notable work done in the area of partially data-driven modelling for movement prediction is the series of papers [119–122, 149, 150] from researchers at the University of Washington, with the bulk of the work the PhD thesis of Lin Liao [118]. The work examines the use of both Hierarchical dynamic Bayesian networks and later the conceptually similar Conditional Random Fields3. Hierarchical dynamic Bayesian networks extend hidden Markov models by allowing multiple unobserved variables (model aspects such as velocity with a corresponding set of states) to be modelled in a hierarchical fashion with the bottom layer being the observed variable. The framework provides a natural, graphical, way for the modeller to show how the variables, both observed
2The extended Kalman ﬁlter ﬁrst linearizes the system using ﬁrst-order Taylor series expansion. The Kalman ﬁlter predict and update are then updated accordingly. See [23] for more details.
3Conditional random ﬁelds directly models conditional probability distributions rather than the joint probability distributions, for more information see [1].
24

and unobserved, being modelled interact. Figure 2.4 shows the model used by [118, ch 6, pg 81] to model movement. The model encodes the observed GPS measurements (observable movement patterns) as being dependent on the location, which in turn is dependent on the transportation mode, which in turn is dependent on the person’s goal. The model represents a two-slice hierarchical dynamic Bayesian network, making the simplifying assumption that all modelled aspects are only dependent on the previous time slice.
Figure 2.4: Movement model proposed in the PhD Thesis of Lin Liao, reproduced from [118, ch 6, pg 81] These Bayesian approaches to modelling start with some a priori knowledge about the model structure, the previously discussed graph supplied by the model designer including the variable domain (continuous or discrete and the enumeration of states in the case of the latter), and a set of model parameters (probability distributions of certain parts of the model). The model parameters can either remain ﬁxed, or be adjusted so that the models provides a better ﬁt to a set of training data using the Expectation-Maximization (EM) algorithm (for more details on learning Dynamic Bayesian Networks see [73]). In [149] the authors ﬁx all model parameters except the edge transition probabilities (which is conditioned on the previous edge transition and the previous mode of transport) and the probability of mode of transport (which is again conditioned on the previous edge transition and the previous mode of transport). The use of ﬁxed parameters greatly reduces the amount of training data required and reduces training complexity. Within Hierarchical Dynamic Bayesian Networks (and Conditional Random Fields) inference and learning complexity are of primary concern when designing the models, with arbitrary graph structures being NP-hard and considered intractable in general [83]. As
25

such simplifying assumptions, such as the ﬁrst order Markov property used in [122], are required. Additionally approximation algorithms are typically used for inference [118], particularly if variables are allowed to be continuous (as is the case of velocity in [118, 149]). Models utilizing conditional random ﬁelds have similar issues and additionally the use of continuous variables have not been explored in the context of movement prediction. This is potentially due to the recent nature of extensions to conditional random ﬁelds allowing the use of continuous variables [157] which introduce diﬀerent challenges to ensuring feasible models [159].
Importantly partially data-driven models have a structure based on the speciﬁcally modelled features which makes it possible to reason about these features. For instance in the model presented in [118] the system models the persons goals and can provide alerts if the person’s behaviour starts to become closer to a goal other than one speciﬁed.
Summary
In summary, partially data-driven models address many of the challenges speciﬁc to pedestrian movement prediction. In particular they enable a greater historic context to be learnt by the algorithms from the historic training data. However a number of drawbacks still exist:
1. The model structure is ﬁxed, requiring well-crafted models to be developed by hand using domain-speciﬁc knowledge.
2. Learning and inference can be computationally expensive with approximation algorithms required in the case of more complex models
3. The full historic context within the historic trails is potentially underused, by assuming independence beyond the previous timesteps to ensure tractability. While interactions between the hidden variables may completely capture the observed multi-time step dependencies they equally may not. For instance consider a hierarchical model with hidden variables of goal and location and a set of observed positions with the assumption that the next time step is then dependent only on the past one. This model assumes that given the current goal, the choice at a cross road (between locations) is always only dependent on the current goal and current location. Clearly this is more complex that a ﬁrst order Markov model over observed positions. The more additional hidden variables or an increase in
26

the number of states per variable (e.g. the number of goals the person could have relative to the locations) the more complex interactions can occur allowing diﬀering behaviour across multiple time steps.
4. Prediction beyond the short term may quickly degrade in performance, particularly with simple models. Typical inference algorithms such as Kalman and particle ﬁlters predict multiple future timesteps by projecting the prediction step. While performing well over short horizons the algorithms can degrade signiﬁcantly over large prediction horizons due to the lack of information to execute the update step. This is particularly true for Hidden Markov Models where the prediction step is then only using a ﬁrst order Markov model as the underlying prediction mechanism.
2.1.3 Fully data-driven models
Unlike the partially data-driven models presented in the previous section, fully datadriven models derive their structure from the training data. Speciﬁcally the data driven approaches address the prediction problem by learning probabilities for prediction routes for a learnt4 set of contexts. Contexts are continuous subsequences of observations that have appeared in the training data which are then matched against the input. Under the assumption that the use of arbitrary length contexts can provide accurate predictions these algorithms learn both the structure (the contexts) and the probabilities for these contexts.
At this point it is important to make a distinction between the model and the data structure used to implement the model. In basic ﬁrst order Markov models the data structure is typically just a matrix containing transition probabilities. In fully data-driven models the model is built from patterns contained within the data which is somewhat unknown. Data structures for data-driven models are varied, with the most common being tree-based implementations due to their computation eﬃciency at matching. Fully data-driven models and variants account for the majority of the proposed approaches within the literature, with proposed approaches varying in model subtleties and in their
4Here the term learnt is used in a very broad fashion. Note that this may refer to the learning of a set of contexts in advance or to the automatic selection of the best context from the training data at runtime via a fast matching procedure. Perhaps a better, but less succinct, way to describe this would be derived from the training data.
27

approach to dealing with noise. The models and their subtleties are dealt with in this section. Approaches to dealing with noise that are, in general, not speciﬁc to any one model are discussed separately in section 2.2. It is of note that fully data-driven models are still considered a modelling problem, with general variable length Markov model algorithms learning the variable structure using the data as a training set while trying to not overﬁt5 the model. This is in contrast to pattern matching although, as will be highlighted, in practice they can be very similar.
Universal Prediction & Variable length Markov models
A predictor for a process which is governed by an arbitrary unknown model that performs essentially as well as if the model was known in advance is called a universal predictor. Developed initially for compression, Universal Predictors address a slightly diﬀerent problem than that described in for pedestrian route prediction in section 1.1. Speciﬁcally Universal Predictors do not use a set of trails as the training data. Rather a single, long, stream of sequential observations is expected as the training data. Contexts of variable length are then automatically determined from the stream of observations and the probability over all possible states for each selected context is calculated by considering all subsequences of the training data stream. When utilizing Universal Predictors for movement prediction a number of approaches are possible. Considering the GPS data from a single person it is possible not segment the GPS data into trails and use it as a single stream of sequential observations directly. Note that this is not generally desirable since device errors or ﬂat batteries can result in movement occurring but not being recorded. If this movement is not noted and the observations simply concatenated movement between locations that did not occur is artiﬁcially introduced. Moreover, however, in general GPS data is acquired from multiple people simultaneously. In this case one could append the GPS streams from each person, however, this would lead to subsequences detailing movement that did not occur at the concatenation points. An example is shown in ﬁgure 2.5. The solution, therefore, is to only consider sub-sequences within the individual trails when identifying contexts and calculating the probability over all possible states for each identiﬁed context. This approach, for instance, is taken in [31].
5Overﬁtting is the development of a model which has learnt the training data and not the general patterns. In other words the probabilities match the training data, but not do not generalize to the unseen population. A simple example in the case of ﬁtting polynomials to a set of points is ﬁtting a polynomial of equal degree to the number of points.
28

Figure 2.5: An example highlighting an error that can be introduced by not considering individual trails (from a single person or from two separate people) separately.
In the rest of this section it is assumed that such an approach would be implemented and the algorithms discussed considering a single stream of observations.
The general problem considered by the standard by the universal prediction algorithms is:
• Given a sequence of state observations s1, s2, . . . , st−1 where each si is a state within a collection possible states S (e.g. the locations that can be occupied)
• Given an input E = e0, . . . , ek, ei ∈ S which is provided when requesting the prediction and varies across diﬀerent predictions with arbitrary length (k)
• Then either, (1) generate the conditional probability distribution P (S|E) over all possible states S, or (2) simply provide a prediction of a state st. Since the former is more informative it is generally preferred within the domain of movement prediction enabling predictions to be ranked.
At a generic level universal prediction assumes that the given sequence of observations6 was generated by either a known or an unknown7 source distribution. Within movement prediction universal predictors that assume the source distribution is from the family of Markov predictors have been used [52]. The basic assumption, as previously stated, is that the use of arbitrary length contexts (histories) can provide accurate predictions.
6in this case the movement sequecence or set of sequences once trails are used and only subsequences within trails are considered
7If the source is unknown the data is then considered to be generated in an arbitrary but deterministic fashion.
29

Universal predictors simultaneous identify a ﬁxed set of contexts and calculate the probability distribution over possible states for the next timestep. Considering all subsequences of the training data algorithms have been developed where the context length is assumed to be either bounded or unbounded. Under each assumption it can be shown that universal prediction algorithms can provide guarantees with respect to achieving known lower bounds on error rates8 as the number of observations tends to inﬁnity [134]. The case where the maximum length is bounded but determined automatically by the algorithm corresponds to the approach proposed under the name variable length Markov model [30] within the statistics literature, indicating the conceptual extension to the ﬁxed length Markov models discussed previously.
Many universal predictors can be seen to form the basis of many movement prediction methods as well as having been directly applied. Moving beyond the ﬁxed k-order Markov models, universal predictors were among the ﬁrst to be applied within the movement literature. For instance, [22] utilized a modiﬁed version of the compression algorithm LZ78. The application of a number of universal prediction variants followed. The variants diﬀer primarily in two ways. The ﬁrst is the way in which they handle the zero frequency problem, which refers to what probabilities should be assigned to symbols in the alphabet if no match can be made. The second is how the variable length modelling is done. In general this is the problem of how the set of contexts are determined from the continuous (or set of continuous streams when modiﬁed for movement prediction) input stream of observations.
LZ Family of compression algorithms: In 1999 [22] examined the use of ﬁxed length Markov models with respect to next step mobile cell tower prediction based on past movement logs between the towers. Noting the issues of ﬁxed length models they proposed the use of a modiﬁed version of the LZ78 compression algorithm. LZ78 creates variable length contexts by splitting the training sequence into non-overlapping blocks representing the shortest sequence not seen so far. Each block then forms a branch in a tree based data structure.
The basic approach to identifying the contexts within LZ78, however, has a number of issues for prediction with the inability of the algorithm to correctly maintain frequency counts of subsequences across blocks. This was addressed in [78] using a sliding window
8Assuming the source is of a certain class. As noted in the case of movement prediction the assumption is that the source distribution is from the family of Markov predictors.
30

approach to update all occurrences of subsequences of the current block in the tree. In all cases the maximum depth of the tree and hence maximum context length that can be matched is controlled by the longest, shortest subsequence not seen in the tree up to that point in parsing the input string. While this has proven to work well with data compression, with respect to prediction the approach has shown to be less attractive in general than other universal prediction approaches [14].
Probabilistic Suﬃx Tree: Like the LZ78 algorithm and variants the basic form of the probabilistic suﬃx tree (PST) is a tree structure that models the probability distribution for discrete events occurring in a sequence. The approach diﬀers signiﬁcantly in how it determines the maximum depth of the tree, and in that the tree constructed is a suﬃx tree. Formally, given a set of states S (for example representing all possible locations) the structure is a tree with:
1. degree equal to the number of states (|S|)
2. the nodes labelled by pairs (q, P (S|q)) where
• q represents a sequence of observations (s0, . . . , st) for which the nodes next step (t + 1) probability is conditional on.
• |q| = the current depth of the tree
3. Child nodes represents an expansion of the historical context q of the current node by a single, earlier observation. Conversely each parent nodes q represents the suﬃx (one less piece of context) of the current node.
For a given ﬁxed depth k the above structure represents a ﬁxed length Markov model of length k and the size of the tree is exponential in k. Probabilistic suﬃx trees [164, 165] (also known as prediction suﬃx trees) alleviate this problem by, given a predetermined maximum depth, pruning the tree keeping only the nodes that have probability distributions diﬀerent enough from their parents. In other words, only when the extra context would make a signiﬁcant diﬀerence to the prediction is the extra context modelled, for some deﬁnition of signiﬁcant. In practice a predetermined maximum depth is utilized to provide a principled notion of two probability distributions being diﬀerent enough [165]. Figure 2.6 shows an example of a PST. As such, PSTs learn contexts which are subsequences of the training sequence which provide distinct conditional probabilities on the symbol in the next time step.
Extensions to PSTs have been proposed which use statistical tests to determine which
31

s=





P (a| ) = 1/3





P (b| ) = 1/3





P (c| ) = 1/3

s=a





P (a|a) = 0





P (b|a) = 2/3





P (c|a) = 1/3

s=b





P (a|b) = 1/3





 P (b|b) = 0 





P (c|b) = 2/3

s=c





P (a|c) = 1/2





P (b|c) = 1/2





P (c|c) = 0

s = ba





P (a|ba) = 0





P (b|ba) = 0





P (c|ba) = 1

s = ca





P (a|ca) = 0





P (b|ca) = 1





P (c|ca) = 0

s = ab





P (a|ab) = 0





P (b|ab) = 0





P (c|ab) = 1

s = cb





P (a|cb) = 1





P (b|cb) = 0





P (c|cb) = 0

s = bc





P (a|bc) = 1/2





P (b|bc) = 1/2





P (c|bc) = 1

s = abc





P (a|abc) = 1/2





P (b|abc) = 1/2





P (c|abc) = 0

Figure 2.6: Example of a PST with a maximum length of 3 for the sequence abcabcbac. Nodes with all zero probabilities are omitted.
nodes should be merged that do not require a predeﬁned bound allowing the bound to grow based on the training data [30]. It is this work which was originally proposed under the name Variable Length Markov Chains in the statistics literature. Additionally work has since been done to consider the case where a bound is not assumed (i.e. it is considered that the data was generated by an unbounded process rather than a bounded process of some automatically determined bound) [53, 66].
Predictions based on PSTs involve matching the input against the tree in reverse order, i.e. starting by following the edge matching the most recent observation in the input. The node reached after matching the whole input then contains the probabilities of the next symbol and the symbol with the highest probability is selected as the next time step prediction.
Within the movement prediction domain [200] proposed the use of PSTs in order to prediction urban vehicle routes. A ﬁxed maximal length K was provided and two thresholds manually set to control when the context should be extended and when it should not, rather than using one of the various automatic methods from [30, 53, 66, 165]. In contrast, while also using a PST structure [147] use domain speciﬁc information to create

32

maximal contexts and the build the tree omitting (pruning) nodes with zero probability only. In the work the authors also forgo the normal storage of probabilities at each node, instead storing a set of metadata and determining the probabilities based on various criteria on the ﬂy. In their work the authors propose the use of standard probabilities based on counts as well as on a notion of time spent in the sequence. With respect to movement route prediction both [147, 200] only seek to predict the next time step. The model and prediction mechanism they utilize (the PST) however, could be recursively called appending the next time step result to the input before the next recursive prediction call.
The work of [147] shows a gradual transition from the universal prediction framework to the general notion of pattern matching. No longer are contexts identiﬁed based on the assumption of an underlying variable length Markov model, but rather they pre-specify them using domain knowledge and consequently they can not longer be considered universal prediction algorithms. However, substantial similarities exist as in both cases a variable length Markov model is being constructed in the form of a PST. With the contexts being well deﬁned and no longer just considered sequences of arbitrary data the pruning mechanisms can operate at the level of a trail [139, 147] rather than when adding each node. Extending this further the problem simply becomes one of matching the input to a set of historic trails via a partial matching algorithm [178] and selecting the most likely trail from those matched [137]. Within PSTs the matching is simply exact matching against the built tree although, considering the problem as a pattern matching one, there is clearly a much wider range of techniques available. This transition to pattern matching is further discussed later in this section.
Prediction by partial match (Preﬁx trees): Prediction by partial match (PPM) is a technique that utilizes variable length contexts to make predictions [45]. Traditional PPM has a similar requirement to PST requiring the maximal order, k to be deﬁned, although an unbounded version has been proposed [44]. A simple way of initially understanding PPM is to consider it to construct a table of probabilities P (S|C) for all the diﬀerent possible contexts (continuous subsequences) C of length 1 . . . k from the training data for all states in S. Predictions are then made by attempting to match a context of equal length to the input. If such a match can not be found the input is reduced by one (the least recent observation in the input) and another match attempted. This represents the fallback strategy previously discussed in section 2.1.1. At the lowest level there is assumed to be a set of ﬁxed probabilities for all states conditioned on the
33

empty set. This simple approach is suﬃcient for basic sequence, and hence movement, predictions as at some point it will match (or report a ﬁxed probability) and return the most likely prediction. This is suﬃcient if we are interested in the most likely prediction and not the probability of the prediction. If the probability of the prediction is required then each time a fallback was initiated a probability known as an escape probability is required. The general recursive mechanism of PPMs is, given σ ∈ S, ei ∈ S, then:

 Pˆ(σ|et:j...n) = Pˆ(σ|et:j...n)
Pˆ(escape|et:j...n) × Pˆ(σ|et:(j+1)...n)

if et:j...n appeared in the training sequence;

otherwise.

(2.3)

Unfortunately the selection of escape probabilities is non-trivial and [45, pg 397] note with respect to the selection of escape probabilities, “. . . in the absence of any a prior knowledge, there seems to be no theoretical basis for choosing one solution over another.”.

Since this work does not focus on the need to determine the actual probability and relative probabilities suﬃce, the selection of escape probabilities and the various proposals are not further discussed. A common and eﬃcient approach is to implement the PPM as a preﬁx tree [14]. As such PPMs construct contexts based on all preﬁxes of each symbol in the training data from of length 1 . . . k.

PPM is used within the movement prediction literature in [31] and, while not stated explicitly, in [202]. In both cases preﬁx trees were used and building the PPM preﬁx tree is done by considering all sub-sequences within the individual trails. In [31] escape probabilities were utilized since actual probabilities were required as part of the evaluation function used. In contrast in [202] escape probabilities were not used and the accuracy of the resulting prediction from the system compared to a known correct result. Of note is that if the movement data is already split into logical patterns of movement between goals (trails), the ﬁxed length of the PPM can be logically set at the length of the longest trail.

Sampled Pattern Matching: In a similar fashion to PSTs, Sample Pattern Matching (SPM) [92] automatically determines the contexts used that are used to predict a given sequence given an input training sequence. Context is determined in a conceptually straight forward manner by ﬁrst selecting the longest suﬃx from the input that is contained in the training data and then taking a fraction (α) of this longest suﬃx

34

as the context. As such if α = 1 the approach is the same as PPM. For any α < 1 the approach trades contextual information for potentially more observed samples from which to construct the next state probabilities thereby helping to prevent overﬁtting. SPM has not seen much use within recent literature on movement prediction. A possible reason for this is that the problem of overﬁtting is often dealt with as a preprocessing step (i.e. only frequent movement patterns are mined from a collection and presented to the prediction algorithms [95, 137, 139]). The approach, however, is evaluated with respect to predicting next location given Wi-Fi mobility data in [182] along with a number of diﬀerent approaches. They conclude by noting no signiﬁcant diﬀerence between the more complex algorithms such as SPM and a second order Markov model with a fallback mechanism. The result, however, is not relevant to pedestrian movement prediction as the data set utilized represented very coarse grain data in which the number of locations a user moves from is expected to be small and quite regular. As such it is not surprising that by using small contexts accurate predictions can be obtained. Within pedestrian movement data long and varied contexts are expected per person and so the results are not applicable in this context.
Context-Tree Weighting: In contrast to other approaches such as PST and PPM, context-tree weighting (CTW) does not seek to determine a set of previous contexts and then match an input to a context in the set. Rather the approach merges exponentially many Markov chains of bounded (user deﬁned maximum) order. Originally proposed for sequences of with binary states only [196], the approach has been extended in various ways to allow states [191]. CTW has shown varied results, named as one of the better universal predictors along with PPM in [14] but shown to perform not as well as PPM in [103]. No studies with real world movement data have been performed. Within the context of pedestrian route prediction it is unclear that merging probabilities from lower order Markov chains is advisable when higher order context is available, since many common corridors of movement exist and it can be the ﬁrst few movements before entering the common corridor that are the discriminating ones for the prediction.
Concluding remarks: While the use of universal predictors is appealing in theory, the direct application in some practical domains do not always result in accurate predictions. Developed in general for data compression, and then generalized to prediction (since the problems can be shown to be directly related [65]) the general algorithms do not account for noise which introduces additional randomness into the system reducing predictability of the overall data and hence the performance of the system. Addition-
35

ally the parametrization of the diﬀerent algorithms can have a large impact on their behaviour for the ﬁnite sequences observed in practice, despite not aﬀecting the asymptotic performance [196]. Finally, and most importantly, the application of universal predictors requires the pedestrian prediction problem to be suitably encoded. Even if a perfect universal predictor was realized the encoding of the problem into a set of states, in other words which features to use, is still a notable problem. For instance within movement prediction, consider the problem of a fork in a path, one which leads to an undercover route, the other outside. Assume that when it rains I always take the undercover route, and when it doesn’t I take the other and it rains 30% of the time. If only the locations are modelled then the optimal predictor would model my choice as of taking the undercover path as 30% each time. However, if the weather was also encoded the optimal predictor would be able to predict my choice accurately every time.
Context pattern matching
In contrast to Universal Predictors, which were originally developed assuming a single sequences of observed states as the training data, context pattern matching take advantage of the extra (potentially location/area speciﬁc) information provided by the segmentation of a GPS stream into a set of trails while still remaining generic prediction algorithms. The logical unit of a trail, while being very generic with respect to the problem of movement prediction, enables a number of reasonable domain-dependent assumptions to be made. One useful assumption that can be made is that trails are independent of each other. This enables the context (and hence context length) to be deﬁned as the preceding states within a trail for any state within a trail. Therefore the context and its length does not need to be estimated as required by the universal prediction problem. Following this approach the known contexts and their corresponding predictions can be used to build a preﬁx tree and the standard PPM algorithm used, utilizing the fallback mechanism for matching previously unseen contexts. Alternatively partial pattern matching can be applied for which a large body of research exists with respect to dealing with noisy data. In contrast to Universal Predictors partial pattern matching is able to utilize extra information encapsulated in the logical unit of a trial, namely how many states to predict into the future when predicting a route rather than the next step. This is achieved by using the remainder of a matched trail as the prediction. In contrast Universal Predictors predict routes by simply perform next step
36

predictions recursively until a user-deﬁned stopping condition.
Speciﬁcally partial pattern matching with respect to movement prediction can be considered to be a three stage process:
1. Computation of typical patterns: A set of typical trails K is created from the training data of historical trails (K). The result is a set of unique trails which are considered to be signiﬁcant in the data set, in other words, they have not occurred by chance. Optionally each pattern additionally contains meta-data indicating how typical the pattern was in the data set.
2. Computation of best partial match: Given the pattern set K , a (partial) input trail E and a similarity function δ(·, ·) determine the closest match as m = max(δ(E, K)), ∀K ∈ K
3. Prediction: Since all patterns in K were typical patterns the continuation of the matched pattern is used as the prediction. If multiple patterns were matched, and the patterns contained meta-data with respect to how typical that pattern was, the most typical pattern is selected.
Framing the PPM algorithm in this light we see stage (1) is achieved when the algorithm identiﬁes contexts. The historic patterns in this case are the collection of contexts and the state which occurred next in the sequence. The probability of the context and the state is the meta-data indicating how typical the pattern was. Stage (2) is then performed utilizing the preﬁx tree, an exact matching function and fallback, matching against the contexts. In stage (3) the state which occurred after the next sequence with the highest probability is selected.
The advantage of the context pattern matching is the ability to deal with noise in both identifying typical trails and in the matching phase. For the former a body of work based on frequent pattern mining within databases has emerged with respect to trajectories (e.g. [69, 76, 139]), extending traditionally data mining techniques. With respect to the latter a number of approaches have been proposed and are discussed individually below. In addition, as previously mentioned, a route rather than a next step prediction is readily available.
Matching via preﬁx trees using custom distance functions: The mining of frequent trails from a data set of movement logs was proposed in [139]. The process mines trails as sequences of edge transitions between locations. Each frequent pattern
37

was given frequency metadata in the form of a count of the number of times the pattern appeared in the database. For a prediction a preﬁx tree was constructed from the frequent patterns. Matching was then performed against the tree starting at the root and matching nodes down the tree by one of three matching (distance) functions. The ﬁrst required exact matching, returning no prediction if an exact match could not be found. The second simply reﬂects a ﬁrst order Markov model by reducing the input to the last seen state. The ﬁnal matching function weighted the conﬁdence of the match by the relative coverage of the input on the frequent pattern. After matching the matched section of the pattern was considered as the antecedent of a rule within the database and the remainder the consequence. Utilizing the frequency counts associated with the nodes of the tree the conﬁdence of the rule was calculated as the similarity function. Comparing the three methods the authors show the beneﬁt of using the full history over a simple ﬁrst order Markov model, but indicate poor results when using the reweighting strategy. Such a result lends itself to the conclusion that noise reduction strategies can negatively inﬂuence predictions. This is of course possible, however, it is of note that relative coverage is a somewhat ad-hoc strategy heavily reliant on the initial quantization. Of more interest are distance functions that operate in the twodimensional spatial domain, as later proposed in [137] although limited comparison was performed with other techniques in either case.
The proposal of [137] also mined frequent spatial patterns but additionally included transition times between locations. In order to perform matching, a preﬁx tree was constructed using the frequent patterns based on the location data. Encoding transition times, edges in the tree were given minimum and maximum values. These values encoded upper and lower bounds on the transition time between the states. Matching was again performed via a custom matching function. The proposed function ﬁrst attempts a direct match by starting at the root of the tree and selecting nodes exactly equalling the input and ﬁtting in the temporal range speciﬁed by the edges. If such an exact match is not possible the distance function ﬁrst relaxes the temporal conditions penalizing the similarity score. If a match is still not found the spatial constraint is relaxed and the weighted time and distance diﬀerences used along with the support of the matched trail to calculated a similarity measure.
Both proposals in [137] and [139] can be considered modiﬁed versions of PPM, with both approaches utilizing preﬁx tree data structures and using conditional probabili-
38

ties9 as the base measure when ranking prediction choices. Diﬀering from PPM both deﬁne contexts via frequent pattern mining aiming to reduce the eﬀect of noisy data and the problem of overﬁtting by presenting only relevant patterns to the pattern matching algorithm. Additionally both proposals include custom, domain based, similarity functions which enables fuzzy matching in a spatio-temporal fashion not possible under the universal prediction deﬁnition. This again helps address the problems associated with noisy input data. In contrast to [139] and other previously presented approaches the introduction of the temporal meta-data in [137] shows the encoding of previously unconsidered information. However, the extra information is added in a coarse-grained fashion (a range) and as a secondary matching condition with the in-sequence location data requiring to be matched ﬁrst before the temporal distance is considered. As such the approach is prone to noise in the form of gaps in both the historic data and the input sequence, although this was of negligible concern as originally proposed due to the coarse level of spatial quantization employed. In ﬁne grain pedestrian modelling from GPS logs such occurrences are highly probable with a symbol missing 7.1% of the time on average in the dataset examined in this thesis when quantized spatially by a 5 × 5 metre grid.
Matching via alternate data structures: Moving away from the PPM paradigm and preﬁx trees a pattern matching approach called alignment prediction is proposed in [178]. Successfully applied in computation biology to ﬁnd approximately matching patterns between RNA or DNA sequences, the approach aims to address problems of noisy data. The approach addresses the issue of noise in the form of gaps, computing a partial match score via an alignment approach for each typical trail. Using an arbitrary distance function, and allowing parts of both the input and the typical trail to be skipped based on a penalty function, the input is aligned optimally to the typical sequence. The unmatched remainder of the trail with the best partial match score is then used as the prediction. Computationally complex in naive implementations, the approach is made eﬃcient using integer programming and fast approximation algorithms.
Alignment prediction oﬀers a ﬂexible framework for modelling movement prediction and is the closest related work to the approach presented in chapter 6. However, while
9Conﬁdence is the name given to conditional probabilities within the data mining community. Due to the tree structure, support (pattern counts) and conﬁdence are proportional. Therefore, since the absolute values are never used, the utilization of support over conﬁdence is an arbitrary decision with support preferred for computational simplicity.
39

enabling a relaxed notion of sequence, control over the relaxation is via a somewhat non-intuitive skip function. The ﬁrst author discusses the choice of this function in [177], specifying a function by the assumption that a gap symbol is considered a missing symbol rather than one recorded in error. While such an assumption is highly likely, the alternative is also possible. In chapter 6 this is further discussed and a new approach proposed addressing this issue while maintaining lower worst case complexity bounds. Principal & Independent Component Analysis: Algorithms in previous sections have all addressed the matching using speciﬁc data structures to enable eﬃcient computation, although the basic form of pattern matching simply requires all typical patterns to be compared with the input using a similarity metric [69]. This, however, is computationally prohibitive for large datasets requiring at least O(|E|n) comparisons where |E| is the length of the input and n is the number of observations. An alternative approach that has been proposed recently [56] in the context of high level behaviours is principal component analysis (PCA) and the conceptually similar independent component analysis (ICA). Principal component analysis based approaches have sought to reduce the dimensionality of a dataset while still maintaining as much information as possible, in other words to identify the most discriminative features. The underlying notion is that the distance between a smaller set of the most discriminating features will provide more accurate matches, avoiding the curse of dimensionality. The curse of dimensionality refers to the problem of distance functions becoming less discriminative as the number of dimensions increase [19]. Techniques involving PCA or ICA start by encoding the states as binary feature vectors with each feature becoming a dimension thus resulting in a set of high-dimensional data. E.g. in a world with three discrete locations the feature vector a person located in the second location would be encoded as [010]. At a relative abstract level, and in the presence of multiple context features, good results have been reported [56]. However, at the lower level of pedestrian prediction where the binary encoding of the occupancy of the locations (e.g. 5m grid cells) would
40

result in a huge number of binary features the performance degrades rapidly10. This is primarily due to the PCA (and ICA) treating the dimensions as independent when they are clearly highly dependent. Since without throwing away the sequence information (the dependence information lost when using PCA or ICA) the problem is of low dimension anyway and such techniques are not required. As such the problem of pedestrian prediction is not amenable to such analysis, although GPS data could easily form one dimension of a larger set of contexts to which PCA or ICA was applied.
Summary
Context pattern matching addresses the obvious challenges of pedestrian route prediction from GPS logs. The approach provides an intuitive framework to incorporate the domain knowledge encapsulated inherently by the way trails are segmented. Additionally, noise is dealt with in a general way through the use of similarity functions allowing all the historic context present in each trail to be used while still remaining tractable. The beneﬁts, however, come at a price of increase complexity. This complexity is both in terms of computation which, while tractable, may exclude their use in certain situations and in the requirement to choose similarity functions and trajectory alignment heuristics. This has lead to a vast variety of approaches proposed. The most common approach has seen the use of a preﬁx tree for matching, similar to the PPM universal prediction algorithm, although alternatives in the form of basic nearest neighbour, alignment prediction and principal component analysis have been proposed.
Unfortunately limited comparisons have been made between proposed pattern matching approaches and in particular, no comparisons have been made speciﬁcally in the domain of ﬁne grained GPS trails utilized for pedestrian route prediction. A signiﬁcant number compare only variants of their own technique [69, 137, 139, 200? ], utilize either unique datasets or the Wi-Fi dataset also used in [182]. As previously discussed the Wi-Fi data is coarse-grained and has vastly diﬀerent qualities, leading to results in [182] and [31]
10Non-binary encodings are also problematic as they require a ﬁxed number of samples as PCA and ICA require ﬁxed length feature vectors. While such a requirement can be met by resampling techniques (e.g. those considered in [142]) the whole process still maintains little merit since the motivation to reduce the dimensionality is limited. Additionally the interpretation of the eigen vectors of such a nonbinary encoding has limited merit since principal components become averages of location data which results in the data being represented as a combination of spatially averaged trails. After reducing the dimensionality the resulting trails then provide minimal insight for prediction purposes.
41

indicating the superiority of low level Markov models with fallback (PPM) compared to higher order models. As perviously noted, this is not a result that is expected to generalize to the conditions of pedestrian movement prediction.
Raw GPS data is utilized in [178] but sampled at twenty minute intervals, reporting superiority of ﬁrst order Markov models over the alignment prediction in the short term and vice versa in the long term. Note that the use of twenty minute intervals is too sparse to enable pedestrian route prediction. In contrast GPS data quantized into regions of interest by clustering is used in [202] along with a pattern matching approach based on a custom heuristic. The authors compare their approach to a basic Markov model showing a vast improvement. However, the decision to use the average prediction length from their own algorithm as the number of future steps to predict using the Markov model potentially has a large negative eﬀect on the performance of the Markov model. Any missing or extra predicted points occurring due to this choice of length would result in a penalty under the evaluation metric used, artiﬁcially degrading the Markov model performance. This issue and alternative approaches to evaluation in the case of comparing ﬁx length and next step prediction algorithms is further discussed in chapter 3. Overall, limited comparative studies exist and none cover ﬁned-grain movement prediction for which it is unlikely that low order Markov models will perform well in general.
2.2 GPS Data: Encoding for route prediction
The route prediction algorithms typically do not work directly from the raw GPS logs. Indeed, the route prediction problem as deﬁned in this thesis in section 1.1 already assumed the segmentation of the GPS data into trails as a pre-processing step in order to separate the prediction problem from the deﬁnition of a route. Additionally it was noted that this pre-processing step enables the addition of detailed domain knowledge while directly applying generic prediction techniques (see section sec:patternMatching). Segmenting the GPS logs into trails is not the only form of pre-processing typically performed with many of the previously discussed prediction techniques encoding the GPS point observations in order to apply state-based techniques, reduce the computational complexity of matching and/or in an attempt to deal with noise present within the raw GPS data.
42

This section consists of three main subsections. The ﬁrst is focused on the requirement, motivations and methods used to encode the raw GPS data into various forms with respect to the problem of pedestrian route prediction. The second provides an empirical look at the noise present in the movement data set used in this thesis and considers the eﬀect the noise present has on the encodings. This is important as it has the potential to eﬀect a number of the previously discussed prediction algorithms, particularly those that assume accurate sequence information such as Markov models. Importantly this informs the development of new algorithms later in this thesis. The third section address the diﬀerent issue of segmenting trails.
2.2.1 Encoding location from raw GPS data
The motivations to encode raw GPS point observations are varied. They include the desire to address issues of noise to better encapsulate a notion of matching or similarity, the desire to utilize pre-existing algorithms which operate on discrete or symbolic data, the desire to reduce computational complexity and the desire to reason with locations understandable to the end user. The discussion on the possible encodings is split into two subsections. The ﬁrst uses quantization in order to address the desires for discrete locations and the second considers the direct use of continuous coordinates.
Encoding via quantization
Encoding to a discrete set of locations via quantization is intuitively appealing as it has the potential to address issues of noise, enable exact matching by symbol comparison (providing computational beneﬁts), enable the direct application of certain algorithms and provides the opportunity to use human identiﬁable locations. Quantization can be performed individually for each dimension of the data (e.g. location, time) resulting in a state being represented by a vector or globally resulting in a single symbol representing each state. Note that each unique combination of values the vector can take (since each element in the vector is discrete this is a ﬁnite number) can be thought of, and mapped to, a single symbol. Hence for the remainder of the discussion quantization of a location is considered to result in a symbol and the quantization of trails considered to be a set of symbols. In any case, in practice, exact comparison between vectors/symbols is cheap since the vector only contain discrete symbols which can simply checked for equality.
43

In all these cases the process of quantization must take into account the questions what constitutes a location? and when are two locations equal? or how close is this location to another?. Additionally the issues of noise must be considered, particularly if exact matching is going to be used. This is because even if it could be decided at what distance or otherwise two GPS points became diﬀerent there is the additional issue in the technology correctly identifying the device’s actual location. The level of noise caused by this GPS inaccuracy is aﬀected by a large range of factors including satellite positions, signal obstructions (such as buildings), quality of the device, atmospheric conditions and signal interference known as multipathing where signals from the satellites may reﬂect oﬀ an object and take an indirect route to the receiver [89]. Note that if this is not taken into account and very ﬁne-grained quantization used exact matching will generally fail and no predictions will be made. On the other hand if a suﬃciently coarse-grained quantization is used the eﬀect of small changes in the raw GPS readings due to error can be removed.
A ﬁnal point to note when encoding GPS data for use in algorithms using exact matching is how time is quantized. In general time is converted into sequence after location is quantized to a symbol representation (e.g. for use in Markov Models and Universal Predictors). This form of quantization leads to issues when comparing trails since missing points in the raw GPS data mean that the trails do not align as would be expected. Missing points can occur for a variety of reasons including diﬀering sample strategies (i.e. diﬀerent reporting requirement across diﬀerent devices requiring them to report at diﬀerent temporal or spatial intervals.), ﬁltering algorithms (i.e. those that remove impossible samples based on velocity and distance thresholds and the laws of physics), device error and/or temporary occlusion of satellites.
In the remainder of this section previous approaches to encoding GPS via quantization are discussed considering their applicability to the problem of pedestrian route prediction.
Logical location matching: Logical location matching has been employed in two main cases (1) in the transport domain segmenting roads at intersections to construct a complete set of possible locations, and (2) when predicting movement between known regions of interest (RoIs) such as train stations and speciﬁc tourist destinations. In both cases exactly the same techniques are applicable as the task is simply one of mapping points of continuous latitude and longitude (and potentially other features) to the sym-
44

bols in the domain of known locations. The techniques range from simply using ﬁxed regions based on domain knowledge (e.g. Wi-Fi zones [31, 147]) or employing heuristics such as correcting the point to the road with the smallest distance and angle deviation from the previous adjusted location [200] to utilizing a hidden Markov model and the Kalman ﬁlter [114, 116] or hierarchical Bayesian networks [149]. With respect to pedestrian route prediction, knowledge of a ﬁxed set of possible locations can not generally be considered to be available. Pedestrians are not constrained to roads and the quantization to regions of interest removes the route information. As such these approaches are not considered further.
Clustering: Clustering in this context can be considered a task of automatically identifying either a set of regions of interest or a complete set of possible logical locations. One of the ﬁrst proposals was the use of a modiﬁed, iterative k-means clustering algorithm [11] which required a radius speciﬁcation rather than the speciﬁcation of the number of clusters. In a similar fashion [95] propose the use of the DBSCAN clustering algorithm separately at each time point. This has the beneﬁt of not assuming that locations are represented by circular areas but requires observations to be time-aligned. Density based approaches were also proposed in [76] and variants utilized in [137, 202]. The ﬁrst proposed approach in [76] quantized the spatial area via a grid. Square regions that were visited (optionally including through interpolation) by at least a ﬁxed number of trails in the data set were then selected using a density threshold in a similar fashion to DBSCAN. Having extracted a set of popular ﬁne grain locations a heuristic was used to identify regions of interest. The second approach proposed selecting all crossroads where more than a certain number of the crossing trajectories change their direction, although this technique was not elaborated on or evaluated. While computing RoIs are not of much value in route prediction, the selection of a reduced set of dense grid squares can help computational tractability, although otherwise simply reﬂects a uniform grid quantization approach. The investigation of cross roads may be of worth, but may also result in throwing away useful information and has not been investigated in this thesis.
Uniform quantization: The simplest approach to reducing noise and creating discrete places for reasoning is via grid based quantization. In this approach the aim of identifying locations with speciﬁc semantic meanings is somewhat abandoned and the motivation mainly due to increased practicalities when performing matching (reduced computational eﬀort) and modelling (amenable to discrete prediction models). A further motivation is
45

that the approximate spatial error rate of the GPS devices is generally known providing a reasonable estimation of the grid size required to get continuous sequences in general reducing the issues of sequence alignment due to missing GPS point observations. Finally it is of note that after grid based quantization the grid locations can either then be used in distance functions (via their centroid or grid reference) to help address the problem of incorrectly sequenced symbols, or as a set of discrete symbols.
Direct use of continuous coordinates
In the approaches discussed noise has been dealt with as a pre-processing step, taking noisy continuous data as input and providing a set of sequences of discrete symbols generally treated as noise free as output. In contrast [69, 178] employ a similarity/distance function when comparing trajectories containing continuous valued sample points. Due to the noisy nature of the raw data both similarity functions provide a mechanism to deal with noise with [69] matching points to line segments with the drawback of high computational complexity. In contrast [178] propose the use of alignment prediction techniques which has a similar worse case complexity but for which eﬃcient implementations and fast approximations exist. The direct use of a distance function has the added advantage that it provides a higher level of discrimination than exact matching and can determine the closest trail in cases where exact matching cannot. The method proposed in chapter 6 extends the direct use of continuous coordinates to exploit this extra discrimination additionally enabling a direct approach to model sequence and time while providing improved worst case complexity.
2.2.2 Properties of noise within a data set from mobile phone GPS data
The previously discussed encodings and the corresponding prediction algorithms deal with noise to varying degrees. In this section movement data sets used within the literature are ﬁrst discussed where it is noted that limited information with respect to noise levels is available. Due to this the data set used in this thesis is examined to provide some insights into the levels of noise in this type of data in order to inform the development of the prediction algorithms presented later in this thesis.
Data sets used for movement prediction have generally recorded data at either the in-
46

dividual level or at a transportation level, e.g car or taxi. At the transportation level GPS is almost exclusively used [69, 116, 137, 200? ]. At the individual level data sets can be further categorised by the level of accuracy of the technology recording the location. The three levels are mobile cell tower traces [22, 55, 56], Wi-Fi access point logging [147, 178, 182] and mobile GPS data [11, 118, 178, 202, 203]. A ﬁnal category is synthetic data (i.e. as used in [95, 139]) which is not focused on in this thesis. Clearly each type of data has a variety of properties. For example data recorded at the cell tower level is unlikely to have missing data leading to sequencing problems, transportation data is likely to be more accurate depending if the GPS device performs some map matching automatically and Wi-Fi access can only be invoked where access points exist and a user chooses to connect. For pedestrian route prediction the datasets of interest are GPS based data sets at the individual level. Most data sets have been characterized by a small number of users (e.g. studies with 1 - 6 participants [11, 118, 178]) over varying time periods from six days to seven months. Recently, however, work on larger datasets have emerged with [202] involving 17 participants in one month resulting in 900 trips. Additionally work using a dataset from Microsoft research Asia has recently been published [203] although the work focused on location and activity recommendation and as such was focused on a much higher level use of the GPS data. In all cases only limited descriptions of the data is present and limited discussion about the levels of noise.
The dataset used throughout this thesis is a called the D-SCENT dataset. The DSCENT dataset was generated from an augmented reality simulation that was developed as part of the D-SCENT project funded by EPSRC at the University of Nottingham in the UK. Participants took on the role of workers constructing an Olympic site, performing a host of purchasing and building tasks. It is important to note that while tasks undertaken by participants were artiﬁcial, their movement across the real world playing area was completely unconstrained and reﬂects real spatial behaviour. The dataset was collated over a year, with the simulation featuring 12 locations and covering a 80, 000m2 spatial area. Sixty participants interacted with the game via G1 smart phones and their (assisted) GPS data was collated every 5 seconds by a central server resulting in over 30,000 position records. Therefore the data set represents a dense collection of relatively short term movement patterns, by a multitude of individuals, between a number of controlled locations, under real world noise and movement with goals unknown to the system as a result of the game rules and team dynamics.
In section 2.2 two main categories of noise were noted in the case of discrete data. These
47

were errors in the position reported and missing GPS point observations. The ﬁrst leads

to symbol corruption where incorrect symbols are recorded given the true position of a

person. The second leads to symbols not being included (missing symbols) resulting in

sequence alignment issues when comparing trails. Since symbol corruption is impossible

to measure in general and is indistinguishable from subtle movement deviations which

may contain discriminative information for the predictions, therefore this noise type is

not examined other than to note that the percentage of pairwise matches between trails

(segmented at known destinations) that exactly match is very small, only 0.0467% at ten

metre quantization and 0.0006% at ﬁve metre quantization. If the notion of equality is

relaxed to one trail merely containing all locations of the other in any order then for ten

metre quantization this improves drastically to 0.9963%, to over the number expected

if

there

was

only

one

possible

route

between

each

of

the

twelve

locations

(

1 12(12−1)

≈

0.7576% ). Under ﬁve metre quantization, however, the number is still comparatively

low at 0.0789%. In all cases trails were quantized using known locations.

The second category of errors is missing symbols (after location quantization). Missing symbols relates to uneven sampling. This often occurs in real world data logging presenting a problem to algorithms assuming correct sequence information. Assuming that under quantization a continuous path is expected, this can easily be examined providing an insight into the number of missing symbols. Again trails were created using a known set of destinations. The number of missing symbols in each trail was determined by counting the number of symbols that were not directly followed by another in the symbols direct spatial neighbourhood. This was then averaged over the length of the trail providing a measure representing the probability of a missing symbols, per symbol in that path. On average the probability of a missing symbol, per symbol was 1.9% with a standard deviation of 5% under ten metre quantization and 7.1% with a standard deviation of 7.9% under ﬁve metre quantization. In other words, under ten metre quantization approximately 2 in 100 symbols were missing. Under ﬁve metre quantization this jumped to around 7 in 100 symbols being missing. In the D-SCENT data, ten metre quantization produced trails with a median (mode) length of 8 (6) spatial regions with an interquartile range of 5. Five metre quantization produced trails with a median (mode) length of 13 (10) spatial regions with an interquartile range of 10. Histograms of the distributions are shown in ﬁgure 2.7.

48

Histogram of trail length (5m grid quantization)

Histogram of trail length (10m grid quantization)

Frequency 0 20 40 60 80 100 120
Frequency 0 20 40 60 80 100 120

0

10

20

30

40

Number of spatial regions per trail

0

10

20

30

40

Number of spatial regions per trail

Figure 2.7: Histograms showing the trail length distribution for trails from the D-SCENT data set quantized using a uniform grid of 5 and 10m. Trails were segmented based on a set of known locations.
2.2.3 Trail identiﬁcation
As previously motivated the route prediction problem considered in this thesis requires the identiﬁcation of trails from the raw GPS data. These trails simply represent logical movement patterns (routes) and provide a domain speciﬁed limit to the context and prediction horizons within prediction algorithms. This aids in reducing computational complexity and can help provide more accurate predictions. Most importantly it helps separate the deﬁnition and subsequent identiﬁcation of routes from the prediction mechanisms. This allows this thesis to focus on the latter.
The most common approach to trail identiﬁcation is to utilize a heuristic that segments trails based on a threshold of inactivity or signal loss [132]. This represents the notion that a location is one in which a user either enters a building (signal loss), pauses or puts down the device at destinations (such is the case if the GPS receiver is in a car [69]) and that these locations should be used for segmentation. The heuristic is therefore based on both a temporal threshold between observed datapoints (e.g a three minute threshold is used in [69], ten minutes in [11]). Trips are then typically ﬁltered heuristically to remove those where minimal movement occurred under the assumption that such movement was captured in error. Alternative approaches seek to learn the destinations [119? ]. The simplest, however, is a pre-known set of destinations, although this is generally not

49

available.
2.2.4 Summary
In this section both the challenges associated with using incidentally collected GPS location data and the challenge of then encoding this data such that matching/comparisons could be performed was examined. Prior work has utilized either discrete encoding and exact matching based on these discrete states or distance functions either on the continuous coordinates directly or after discrete encoding. In the work reviewed a number of diﬀerent approaches to quantization were discussed. Of these clustering is not considered at all in the thesis and logical locations are only used to consider the noise within the data set. In the case of logical location matching this is due to the unrealistic manual eﬀort required in general. In the case of clustering this is due to the approaches in the literature addressing the problem of identifying regions of interest which then prevents the generation of routes.
The remaining approaches to encoding GPS location data reviewed are uniform quantization and the direct use of continuous coordinates. It is of note that the utility of these encodings can be vastly diﬀerent, with uniform quantization allowing prediction algorithms based on discrete encodings. As previously discussed using discrete encodings can signiﬁcantly reduce the computational complexity of the predictors by only performing exact symbol matching. Such a decrease in computational complexity generally also comes at a cost with respect to the accuracy of the prediction algorithm, but in some cases such a trade-oﬀ is acceptable. Such an algorithm is motivated and developed in chapter 4 where two diﬀerent levels of uniform quantization are used and compared. In contrast, in chapter 6 the continuous coordinates are directly used in a novel prediction algorithm developed to achieve the highest prediction accuracy.
An interesting aspect not covered in this thesis is the empirical comparison of the eﬀect of quantization and the direct use of coordinates under identical algorithms. Such a comparison is left as future work.
Following the review of approaches to encoding the GPS location data the properties of noise within the GPS data set used in this thesis was examined providing insights into the amount of noise present, highlighting such a data source is noisy even when trail quantization is performed with a set of known locations. For instance in the data
50

set used in this thesis 98% of the time at least one symbol will be missing per trail if quantized with a ﬁve meter grid.
Finally methods for trail identiﬁcation were discussed brieﬂy for completeness, as while such methods are used in this thesis, the extension of such methods is beyond the scope of this thesis.
2.3 Conclusion
In the ﬁrst part of this chapter a large number of prediction algorithms applied in a variety of movement prediction contexts were examined considering the general challenge in developing prediction algorithms for pedestrian route prediction. Unfortunately, while many approaches to movement prediction exist, no comparative study using ﬁne-grained GPS data exist for pedestrian route prediction, with the closest study being that by [178], however, the sampling rate used was very coarse with samples taken at 20 minute intervals. Other studies either use data sets other than GPS data that is of a much higher granularity (e.g. the comparative study on Wi-Fi data in [182]) or ﬁrst detect stay points or regions of interest (e.g. [202]) or simply only compare variants of their own algorithms.
In this light the chapter provided a theoretical summary and evaluation of the potential of algorithms in the ﬁeld of movement prediction, providing a broad but detailed summary of related work in movement prediction approaches in general. Speciﬁcally the chapter provided a critique of the approaches in line with the challenges of utilizing the full historic context and the issue of noise which are particularly relevant to pedestrian route prediction from GPS logs. The general conclusions are summarized below, following which a number of algorithms are highlighted as representative baselines for the empirical evaluations of the proposed algorithms in chapters 4 and 6.
While a large number of speciﬁc algorithms were discussed here they are grouped into seven categories: ﬁxed order Markov models, Markov decision processes, hidden Markov models, Kalman Filter, hierarchical dynamic Bayesian networks, universal predictors, pattern matching approaches.
Of these ﬁxed order Markov models, Markov decision processes, hidden Markov models and the Kalman Filter are unlikely to adequately utilize the long context information
51

which is present in real world GPS logs. While hierarchical dynamic Bayesian networks have the potential to utilize longer historic context, allowing powerful representations, they are dependent on a speciﬁc hand-crafted model of suﬃcient complexity to encapsulate the observed multi-time step dependencies. Additionally such models are significantly more complex in both learning and inference, with approximation algorithms required in the case of more complex models. The remaining approaches are all able to explicitly model long context histories.
Of the remaining approaches, PCA and ICA fail to deliver good prediction performance due to the use of an encoding which discards sequencing information. The rest can be seen to have much greater potential.
Universal predictors have good theoretical potential, however, they do not account for noise inherently, relying on an external method of feature encoding. In addition they do not provide mechanisms to handle varying sample rates. Note, however, that by using data encoded to discrete state symbols and exact matching the computational complexity of the algorithm is signiﬁcantly reduced.
Adopting a diﬀerent paradigm, pattern matching approaches provide a way of both directly modelling higher order sequential data thereby utilizing all historic context while also addressing issues of noise though similarity functions. It is in this highly ﬂexible framework that many of the most recent approaches to movement prediction have been realized, due to the large number of diﬀerent heuristic matching and similarity functions that can be applied. A drawback to these approaches is the computational complexity which can vary signiﬁcantly once again due to the speciﬁc matching and similarity functions used.
Noting this large diﬀerence in computational complexity motivates the development of the two prediction algorithms presented in chapters 4 and 6. The former focuses on maximizing accuracy while still being able to run on very limited hardware while the latter is focused solely on performance and general tractability, additionally motivated by analysis in chapter 5. The lack of empirical comparisons with respect to pedestrian route prediction motivates chapter 3 which considers the evaluation of predictors and requires the selection of a number of baselines predictors. In all cases the prediction algorithm from [137] is selected. The algorithm can be classiﬁed as a pattern matching approach and represents a recent, state-of-the-art approach in the general ﬁeld of movement prediction. When used as a baseline the approach is modiﬁed slightly to enable ﬁne-grained
52

route prediction, by simply skipping the step where the GPS streams are quantized into regions of interest. In the evaluation of the performance focused algorithm proposed in chapter 4 two predictors of similar computational usage are included as baselines. These are a standard ﬁrst order Markov model and a variable length Markov model representing the class of simple models and universal predictors respectively. The second part of this chapter focused on challenges associated with using incidentally collected data, namely the issues of noise and missing symbols. Here an empirical investigation of the data set used in this thesis provided evidence conﬁrming the theoretical expectation that the GPS data used in pedestrian route prediction is both noisy and contains missing symbols. Good prediction algorithms in the context of prediction from GPS data should therefore take into account that missing symbols regularly occur in addition to considering errors in the reported location. Additionally discussed was challenge of encoding the data. Speciﬁcally the decision to convert the data into discrete symbols via quantization or not was discussed. Here it was noted that it is dependent on the aim of the predictor, which symbolic comparison signiﬁcantly cheaper than the comparison of continuous variables via a distance function. Of course the performance in either case is heavily dependent on the prediction technique used and the empirical evaluation of the performance of algorithms under both conditions was noted as interesting future work. Note that quantization via clustering was not considered applicable to the problem of route prediction and is not considered further in this thesis. The remaining previously proposed approaches of grid based quantization and no quantization are further considered later in this thesis in predictors evaluated in chapters 4 and 6. For the former pertains to an approach where the deﬁnition of a good prediction algorithm is more related to runtime performance while the latter approach is focused primarily on prediction accuracy. As expected the predictors using no quantization and a distance function to account for noise show an increased performance. Finally, while only utilized in this thesis and not examined, methods for the identiﬁcation of logical trails within recorded GPS data streams were discussed for completeness.
53

Chapter 3
Evaluating route prediction
algorithms
It is easy to lie with statistics. It is hard to tell the truth without statistics. - Andrejs Dunkels
Throughout this thesis numerous diﬀerent algorithms and their various forms of parameterization for predicting pedestrian routes are presented and compared. Clearly such an undertaking requires a robust framework for evaluation. While in many ﬁelds there is a standard performance metric, such as receiver operator curves (ROC) for evaluating binary classiﬁers [64], or precision recall curves in the information retrieval ﬁeld [163], a survey of recent literature with respect to movement prediction reveals no such standard practices. For instance out of the 16 papers directly relevant to low level movement prediction from logs reviewed in this thesis roughly only a third share a common evaluation metric with another paper. Furthermore only ﬁve compare their results to other techniques and of none report any levels of signiﬁcance associated with their ﬁndings, rather only presenting descriptive statistics. In light of this and to provide a robust foundation for the discussion and comparison of the algorithms presented in this thesis, this chapter provides a uniﬁed view of utilized evaluation methodologies within the literature. Based on that, a recommended set of evaluation metrics, visualizations, and statistical comparison procedures is outlined. This forms the basis of comparison used throughout this thesis and provides a set of simple but robust guidelines for others wishing to compare similar algorithms within the
54

ﬁeld.
Speciﬁcally this chapter examines the evaluation of algorithms addressing the route prediction problem as detailed in chapter 1, section 1.1. For clarity the deﬁnition of the route prediction problem is repeated below:
• Given a set of historic trails: D = {H1, . . . , H|D|} where a trail H = h1, . . . , h|H| and each point is equal to some feature vector (minimally encoding location), for instance: h = [longitude, latitude, time]
• And given an observed input trail E = e1, . . . , e|E| , where e is a point observation of the same type as h
• Provide a resulting prediction R = r|E|+1, . . . , r|R| , |R| ≥ 1 of arbitrary length indicating the route that is expected to be taken, where rk is a feature vector encoding only locational information (e.g. rk = [longitude, latitude]).
Recalling once more that H, E and R are all ordered sets as indicated by the angle brackets.
From the problem deﬁnition the high-level evaluation of prediction error for a individual prediction can be easily deﬁned as δ(R, F ) where R is the resulting prediction as deﬁned in the problem deﬁnition, F is what actually happened and δ(·, ·) is some distance function returning the dissimilarity of R to F . This notion of prediction quality is similar to that deﬁned in [177]. The goal of any prediction algorithm is then to minimize the distance measure across all predictions although it is of note that exactly what is being minimized, for example the average error, is open to debate. The deﬁnition of the distance function and the deﬁnition of what form of aggregation should be minimized is the ﬁrst challenge in the evaluation of route prediction algorithms and is addressed in section 3.1.
The second challenge, examined in section 3.2, relates to how the set of historic trails, D, is used to both train predictors and also evaluate them. In practice the whole of set D would be used to train the predictor. However, for evaluation some of the trails from D must be used to construct the input (E) continuation used as the known result, F , against which the prediction is evaluated. Of course when making predictions in the real world F is unknown. However, we can obtain F for testing by taking a full historic trail and cutting it into two segments. The ﬁrst segment then becomes E (the observed input trail from which the prediction algorithm makes its prediction) and F becomes what
55

Figure 3.1: Figure visually showing an the process of a basic evaluation procedure labelled with the corresponding symbols used throughout this thesis.
actually happened. Therefore two sets must be made from the data set, D. The ﬁrst is the training set. This is what the predictor is provided with ahead of time to be used for learning. The training set is denoted throughout this thesis as: K = {K1, . . . , K|K|}, where K ∈ D. The second is the test set, a set of historic observations from D which are each split to construct pairs of E and F . These pairs are then used to evaluate the predictors. In this thesis the test set is denoted as: T = {T1, . . . , T|T |}, where T ∈ D. The process is illustrated in ﬁgure 3.1. The challenge in this instance is the provision of a methodology to divide the a data set of historic trails into these training and test sets. How this is done is important as it can subtly change the type of error being evaluated between three main variants.
The ﬁrst possible type of prediction error is the prediction error of a trained model. In other words, the expected error of a predictor given a ﬁxed training set. Clearly this is not the desirable measure of performance with respect to movement prediction algorithms from GPS data for which the training data has the potential to change rapidly. The second type of prediction error is the prediction error over training sets. This assumes a speciﬁc data universe (e.g. a speciﬁc problem domain for which data has been gathered) but is a generalization of the prediction performance to an arbitrary training set from that domain. In reality, the calculated generalization error is speciﬁc to the size of the training set repeatedly used to calculate the error. The third measure of error is the predictor error over multiple diﬀerent data sets. This provides a further generalization of the error over a wider range of conditions and inputs and/or domains. Clearly this is the
56

desired goal in general. Additionally it is somewhat easier to perform statistical analysis with since the diﬀerent data sets are generally considered independent, a highly desired property which single data sets lack. The implications of this point form the majority of the discussion in section 3.2. Despite the beneﬁts of using multiple data sets it is common that a suﬃcient number of data sets do not not exist or are not accessible and so the second type of error must be used. This is the error that is focused on in this chapter. From a theoretical perspective this is still a correct performance metric when evaluating an algorithm for real world use in a speciﬁc domain. From a pragmatic perspective it is the best possible choice when limited data sets exist, which is the case with GPS logs at this point in time. It is of note that this has generally not been considered in the literature, with the majority of the reviewed papers simply splitting their data resulting in only evaluating the prediction error of the trained model. Of the remaining papers, two used leave-one-out cross validation to obtain the generalization error, one used an unspeciﬁed form of cross validation, three did not report their methodology and one resampled from the training data, a technique which is known to produce artiﬁcially good results. A third challenge, directly coupled with the second challenge of how to choose the test and training sets, is the choice of which statistical tests should be used to compare the algorithms and provide evidence as to the validity of the results. Clearly an important issue, this is also examined in section 3.2 arriving at a set of recommended procedures in the case of route prediction evaluation. Finally it is important to provide descriptive statistics and visualizations. This is addressed in section 3.3 where a concrete set of visual and descriptive statistics are motivated in order to provide a good overview of diﬀerent aspects of the performance of route prediction algorithms. In this chapter each section concludes with a recommended approach with respect to the aspect of the evaluation problem they address. These recommendations are then summarized in section 3.4 providing a complete framework for evaluating route prediction algorithms.
57

3.1 Metrics for evaluating prediction quality
The ﬁrst hurdle in evaluating predictions is deﬁning the distance function, δ(·, ·), which returns a measure of how diﬀerent an individual prediction is from what actually occurred. From the problem deﬁnition presented in section 1.1 the distance function is of the form δ(R, F ) where R = r|E|+1, . . . , r|R| , |R| ≥ 1, F = f|E|+1, . . . , f|F | , |F | ≥ 1 and |E| is the length of the observed trail from which the prediction was made. This encapsulates the stipulation that route predictions contain order, i.e. that both the prediction and observed routes are a set of data points with a known order. Furthermore it is assumed that this order refers to a timing order with successive points occurring after preceding points. Additionally the subscript |E| + 1 denotes that the prediction and corresponding truth follow on from the observed trail. In the remainder of this chapter this notation will be dropped for conciseness, with the start of the prediction/known truth indicated with an index of zero.
Importantly this deﬁnition makes no stipulations that the data points are equally spaced in time or correspond to any point in time. Therefore both the prediction and any observed routes can be considered time series of arbitrary lengths over varying time periods with an arbitrary sample rate. It is important to note that in the evaluation it is desired to measure the diﬀerence between the route and not the route and timing information. This is implicit in the problem deﬁnition which speciﬁcally only returns a feature of spatial location omitting time. As motivated in section 1.1 this is done so as to not conﬂate the accuracy of algorithms with respect to the prediction of a route with the prediction of the timings of the route. In practice what this means is that timing information can not, and should not, be used to interpolate and align the sampled points that are part of the prediction (R) and the known trail (F ).
The above discussion highlights the information that is available in order to evaluate a prediction through the deﬁnition of a distance function. Of course this does not provide an indication what such a distance measure might be with many approaches possible. Considering this sections 3.1.1 - 3.1.6 provide a survey of diﬀerent measures used within recent literature for varying prediction tasks. It is of note that in some cases numerous methods have been used within individual papers. Following the survey an alternate distance measure is proposed in section 3.1.7.
58

3.1.1 Average log loss

A common method for measuring prediction accuracy within the class of universal predictors (see 2.1.3) is the average log loss [134] which is directly related to the likelihood of the predictor predicting the correct sequence. In other words the predictor with the minimal average log loss score predicts the sequence and all sequential sub-sequences with the maximal average probability. It is of note that the log loss does not split a test historic trail into an input and an evaluation section but rather considers the probability of the sequence being modelled by the predictor. Formally the average log loss, with respect to a test historic trail, H = h1, . . . , h|H| and Pˆ(·|·) the conditional probability as learnt by the predictor, is deﬁned as (from [14]):

l(Pˆ,

H)

=

1 − |H |

|H |

log Pˆ(hi|h1, . . . , hi−1)

i=1

(3.1)

The approach evaluates a predictor’s ability/likelihood to exactly predict the correct

sequence. As presented above this incorporates the predictors ability to model how

likely the observed trail was as well as how probable the unobserved portion was. This

can be seen by recalling that in the context of prediction problem addressed in this

thesis it is assumed that H has been split in two representing an observed portion of the

trail (E) and and an unobserved portion (F ). While potentially undesirable this is less

of a concern as one could imagine a reformulation that only averaged the probabilities

relating to the symbols contained in F . In either case, however, a strict evaluation of

only the exact solution is being made. This is not optimal for identifying predictors

with good practical performance in the domain of pedestrian movement where close

matches can be of practical use. This is in contrast to other domains such as lossless

text compression where close alternatives would lead to data corruption and hence are

of no use. Additionally the continuous and noisy nature of the movement prediction

domain make exact matches unlikely in the general. Therefore the average log loss is

not as applicable as other evaluation functions since, within the domain of movement

prediction, the best predictors are considered those that make the closest predictions

to the correct route, rather than those that make the exact prediction with the highest

probability.

59

3.1.2 Accuracy as a proportion
The majority of studies within the movement prediction domain consider the prediction task of predicting only the next location (data point) [31, 55, 182, 200, 202] or the ﬁnal destination [147? ]. In such situations the most prevalent approach has been to evaluate individual predictions in a binary fashion.
This represents the simplest strategy based on the prevalent use of prediction mechanisms which work with symbolic data. As such distance functions are not required and the prediction is considered correct if the prediction and the known resulting symbol are the same and incorrect otherwise. The approach makes the assumption that predictions beyond the exact match are not useful, and that the boundary at which a prediction is no longer close enough to be considered correct is equal to the boundaries in place from the symbolic encoding. In the case of region of interest encoding this seems highly likely. In the case of symbol encoding via grid quantization, however, the correspondence is less clear. In either case, explicit consideration is potentially more desirable.
Generalizing this to route accuracies, [116] deﬁne a binary similarity function over complete routes with a predicted route correct if there was at least 95% overlap between the prediction and the correct route in the sections common to both routes. In contrast [137] deﬁne accuracy per individual prediction as the proportion of correctly predicted locations over the individual predicted route. While intuitively more appealing as it does not rely on a somewhat arbitrary level of overlap, the approach requires the alignment of data points in the prediction and the correct route.
3.1.3 Other measures without pointwise distance functions
The Levenshtein distance is proposed to evaluate the quality of a prediction in [202], citing its tradition as a metric for comparing sequences. The Levenshtein distance measures the distance between two sequences as the number of edit operations it takes to transform one sequence into another. Allowable operations are insertions, deletions or substitutions of a single symbol. Unless some form of normalization is performed the resulting score is non-intuitive when aggregated due to the varying lengths of trails. Finally as with accuracy by proportion approaches this metric does not take advantage of the fact that the symbols represent (at least) locations which have an associated distance metric and hence are unable to provide an as-speciﬁc and discriminative interpretation
60

of close and therefore a more discriminative quality of the prediction.
In [139] a custom quality measure is proposed. Considering next step prediction the approach ﬁrst mines a set of possible predictions to which probabilities are assigned. After ranking the probabilities, the probability exactly matching the correct result is used, diminished by the diﬀerence between the probabilities of higher ranking possible predictions. Again the approach does not use a pointwise distance metric, rather relying on symbolic matching and generalizing such an approach to route prediction would have similar issues with respect to movement prediction as average log loss.

3.1.4 Accuracy as average error, RMSE & MAE

With location data there is almost always a distance function available over the individual location measurements. In the case of destination or next step prediction by using a distance function the accuracy can be measured as the distance between the prediction location and the reality.

In the case of route prediction distances between aligned point pairs along the prediction

and the correct route can be taken. Two common metrics used to measure the diﬀerence

in time series are Root of the Mean Square Error (RMSE) and the Mean Absolute Error

(MAE). Given a time series of n time steps, the prediction R = r1 . . . rn and correct

answer F = f1 . . . fn the RMSE and MAE are deﬁned respectively as:

RMSE =

n i=1

(ri

−

fi)2

n

(3.2)

M AE =

n i=1

|ri

−

fi|

n

(3.3)

Both the RMSE and the MAE metrics adjust the score to take into account the varying length of the predictions by dividing by the length of the time series. Additionally both provide a mechanism to remove the sign of the error so that the magnitude of the errors are what is being measured. In the case of RMSE this is achieved though squaring the diﬀerences. In the case of MAE this is achieved by taking the modulus. Of the two the MAE has the simplest interpretation, being the average of the error between each set of aligned points in the prediction and answer. In terms of trails with only location information it is the average distance each point in the prediction was from the correct point in the answer for the same position in the sequence. In contrast the RMSE has a more complex interpretation. Of note is that, compared to

61

MAE, large errors between individual points in the sequence have a relatively greater

inﬂuence on the total than smaller errors. Considering only the total square error part

of the RMSE (the

n i=1

(ri

− fi)2)

it

is

clear

that

this

will

grow

as

the

total

error

is

concentrated within a decreasing number of increasing large individual errors. The

division and square root then only scale the value and hence this is a more complex

notion of error which is additionally based on the distribution of the error. As an

example consider two sets of prediction/truth pairs a : Ra = [10 100 100], F a = [0 0 0] and

b : Rb = [10 10 190], F b = [0 0 0]. In both data sets the average error (MAE) is the same.

In contrast, since in data set b the diﬀerence is more concentrated in a single observation,

a higher relative score is reported by the RMSE for data set b compared to data set a.

Speciﬁcally RM SE(a) =

102+1002+1002 3

≈

81.85

vs.

RM SE(b) =

102+102+1902 3

=

110.

A more in-depth comparison of the MAE and the RMSE can be found in [197].

While both the RMSE and MAE provide a distance measure for series of length n it is of note that the prediction may be longer or shorter than the correct prediction. Additionally the metrics require that the time series be aligned. Since only sequence information is known, and in any case it is desired to measure the spatial similarity only and not the spatial and temporal similarity, resampling uniformly to align the sequences based on time is not possible. An alternative would be to resample based on distance. However, the two-dimensional nature of locations mean that this quickly leads to unintuitive results. This is discussed in more detail in section 3.1.6 and highlighted through the MAE example in ﬁgure 3.3.

3.1.5 Hausdorﬀ distance based metrics
Rather than re-sampling and using a standard metric such as the MAE or RMSE the authors in [69] and [185] deﬁne measures that takes two trails as the base level of input rather than a set of points. The similarity between the two trails is then calculated via Hausdorﬀ Distance based algorithms. Broadly speaking these measures report the average minimal distance between the prediction (R) and the known trail (F ) for each point in each trail.
[185] propose that two trails should be considered the same if the maximum distance between each point in one trail and the closest point in the other trail is less than a pre-speciﬁed amount. The approach is asymmetric in that, given two trails, e.g. a prediction (R) and corresponding truth (F ), the distance between R and F does not
62

equal the distance between F and R. Accounting for this two pre-deﬁned maximum distance values are required, one for each case. The authors propose that these values should be the largest sampling intervals in the trails. The use of such an approach as a full measure to adequately calculate the relative distance between trails, however, is not discussed. In this case pre-deﬁned thresholds can not be used but rather an average could be taken over the two results. This approach still suﬀers slightly from the eﬀect of sample rates. It is of note that, while not discussed, the approach is corresponds to calculating the Hausdorﬀ distance (which is not symmetric) for R, F and F, R and applying a threshold to each result. The Hausdorﬀ distance between to (ﬁnite) sets of points is deﬁned as:

Recall: R = r1, . . . , r|R| , |R| ≥ 1 and F = f1, . . . , f|F | , t ≥ 1, Then: hausdorﬀ(R, F ) = max min r − f
r∈R f ∈F

(3.4)

An important weakness of the Hausdorﬀ distance in this setting is that it does not take the order of the points within the curves into account. Intuitively the problem is that points in one trail can match in an arbitrary order against the other. This can lead to small Hausdorﬀ distances when much larger ones would be expected when comparing trails. This is highlighted in an example in ﬁgure 3.2 (a).
Aiming to directly address both the sample rate and issue of ordered points [69] deﬁne a new distance measure. To prevent confusion this distance measure is referred to as the Froehlich-Krumm distance in the remainder of this chapter. The Froehlich-Krumm distance computes the average of the shortest distance between points in one trail and line segments in the other. Additionally the condition that the segment matched to must be either the same as the previous match or occur after the one selected by the previous match. This acknowledges that the trails contain ordered points. However, as shown in the example in ﬁgure 3.2 (b) it does so in a way that may not be intuitive. Speciﬁcally the Froehlich-Krumm distance calculates the average of the distance between each point in the ﬁrst trail and the closest line segment in the second trail and then vice-versa averaging the results. In each asymmetric distance calculation not all points are required and as such measurements between points are not made as would be expected from a global perspective comparative to either a good alignment via good sampling (as shown
63

in ﬁgure 3.2, as used by the MAE) or via global compuationally determined alignment (as done by the Fr´echet distance discussed later in section 3.1.7). The requirement that not all points are required in each distance calculation coupled with the nature of the minimum function tends to lead to underestimating the distance compared to what would be expected. A formal deﬁnition of the Froehlich-Krumm is provided in equation 3.5.

Deﬁnition: Froehlich-Krumm distance (from [69])

Let R = r1, . . . , r|R| Let F = f1, . . . , f|F | where R, F are ordered sets of location point observations. Without loss of generality here they are considered as the prediction and the

corresponding known truth respectively.

Let LS(X) = {s1, . . . , s(|X|−1)} be the set of line segments formed between each pair of consecutive points in the ordered set X, X ∈ {R, F }.

Let Ind(sr) = r Let δ(p, s) be the shortest Euclidean distance between point p and line segment l.

Then:

Froechlich-Krumm(R, F )

=

1 2

|F | i=1

mins LS(R) |F |

δ(fi,

s)

+

|R| j=1

minv

LS(F )

δ(rj ,

v)

|R|

,

s.t.

∀i>1Ind arg min δ(fi−1, s) ≤ Ind arg min δ(fi, s)

s LS(R)

s LS(R)

∀j>1Ind arg min δ(ri−1, v) ≤ Ind arg min δ(ri, v) (3.5)

v LS(F )

v LS(F )

In contrast to pointwise distance measures, the Froehlich-Krumm and Hausdorﬀ distances seek to make a more relaxed comparison between trails automatically determining an the points/segments between which the measurements will be taken. Considering ﬁgure 3.2 it is clear that they are not perfect and that with good alignment in the samples the MAE can still perform in a way that is closer to what might be expected. In the next section the MAE and the Froehlich-Krumm distance is compared showing by example that, despite performing well when the trail observation points are well sampled, the MAE is inferior to the Froechlich-Krumm distance. Following this in section 3.1.7 a diﬀerent distance measure is proposed which performs in a more intuitive fashion than

64

the Froechlich-Krumm distance.
3.1.6 Froehlich-Krumm distance vs MAE
The Froehlich-Krumm distance and the MAE both seek to compute the average distance between a predicted route and the route that actually happened, but in slightly diﬀerent ways. Importantly the Froehlich-Krumm distance automatically determines which parts in each trail to calculate distances between. In contrast the MAE uses the ﬁxed order of the observation samples within each trail in a pairwise fashion. Speciﬁcally in this section a number of examples are considered showing that, in very common cases, performing pairwise comparisons is suboptimal. It also will be shown that this is true under uniform sampling through interpolation based on spatial distances. Recall that, as discussed at the beginning of section 3.1, it is not possible to interpolate based on time.
When considering the MAE and Froehlich-Krumm distance in reality it important to note that the trails may not contain the same number of point observations. In the case of the Froehlich-Krumm distance the default behaviour is somewhat intuitive in that the penalization in the case of extra length in one trail is based on the distance to the closest segment in the other trail. In the case of the MAE no default behaviour is present and so two cases are considered in all examples. The ﬁrst approach is to simply compute sequential pairwise correspondences between points in the two trails, omitting any extra length in either trail. The second is to pair any extra points from one trail with the last point in the other trail.
An example where the diﬀerences in the distance measure discussed above matter is the the case of discretely sampled curves, where the prediction is made parallel, but on the inside of the curve. This is shown in ﬁgure 3.3. A common occurrence in real world scenarios, the problem with the MAE in this instance is that the alignment of the spatially uniform alignment (potentially provided by the recording device) of the points does not ﬁt with basic intuition. Considering the example in more depth, however, it is possible to think of a ﬁxed resampling technique that would provide a good alignment and that could be used as a pre-processing step to calculating the MAE. The resampling technique that would provide this is resampling the same number of points from each trail so that, in each trail, the points in each individual trail are evenly spaced spatially.
Considering the solution oﬀered for the previous example a second example is shown in
65

1 Unit
2 Units
2 Units
1 Unit (a) Distances considered by the Hausdorﬀ distance.
hausdorﬀ ( , ) 1 Unit 2 Units
2 Units
1 Unit (b) Distances considered by the Froehlich-Krumm distance.
Froechlich-Krumm( , ) 1 Unit
2 Units
2 Units
1 Unit (c) Distances considered by the discrete Fr´echet distance and MAE.
Fr´echet( , ) MAE ( , )
Figure 3.2: Exampling showing how diﬀerent distance functions deal with the presence of ordered point observations within trails. (a) Hausdorﬀ distance: Observation order is not taken into account (b) Froehlich-Krumm distance: Observation order is taken account when computing and then merging two asymmetrical distances (c) Fr´echet distance and MAE: Observation order is taken into account at a global level. Note that MAE and Fr´echet distances consider the same distances due to the speciﬁc sampling of the points.
66

ﬁgure 3.4. This time the two diﬀerent ways of dealing with extra length are evaluated for three diﬀerent sets of points. The ﬁrst set are those provided originally, which includes a missing point and uneven sampling between the two trails. The second is the result of resampling uniformly in space. The third involves the aforementioned technique of sampling identical numbers of points. Despite these range of options, however, none provides an intuitive distance measure compared to the Froehlich-Krumm distance. This clearly highlights that, considering the two-dimensional nature of location within the route prediction problem, ﬁxed pairwise alignment though sampling is not suﬃcient. Therefore distance measures such as MAE can not be recommended. Taking a closer look at the example in ﬁgure 3.4 it is also interesting that the problem noted in section 3.1.5 is again present, with the Froehlich-Krumm once more artiﬁcially under-weighting the distance incurred by the loop. A third example is provided in ﬁgure 3.5 which involves trails that cross over. The example once again highlights the inability of the ﬁxed resampling schemes followed by pairwise point matching to successfully capture the diﬀerences between the curves in 2-D space.
In conclusion it is possible to see that (1) the MAE is not appropriate and (2) while performing closer to the general intuition, there are simple cases where the FroehlichKrumm distance also does not perform correctly. In light of this an alternative metric is proposed and examined in the following section.
3.1.7 A solution: The average Fr´echet Distance
In the previous sections the MAE and the Froehlich-Krumm distance measures have been examined. It was noted that the MAE was unable to adequately select the pairs of points to measure between trails, instead potentially relying on a pre-processing step of resampling. No resampling technique considered resulted in a satisfying result on simple examples. In contrast the Froehlich-Krumm distance suﬀered from a diﬀerent problem. Based on the Hausdorﬀ distance the measure suﬀers from the use of two averaged asymmetric distance calculations where, within each sub-calculation, not all points are required to match. As shown in ﬁgure 3.2 this can lead to a selection of point-to-point comparisons which underestimate the distance compared to what would be expected.
Due to the shortcomings of previously proposed/utilized distance measures for movement prediction, a modiﬁed version of the average discrete Fr´echet distance is proposed here.
67

Figure 3.3: An example of the Froehlich-Krumm distance and the MAE when comparing between two paths represented by points diamonds and circles respectively. In this case the points in each trail have been recorded uniformly based on the distance travelled. For the MAE the distance is reported when the extra length is taken into account and when it is not. In the Froehlich-Krumm distance at each point in a, the closest point on the segments between all points in b is taken and the distance calculated and vice versa. In both cases the average distance is reported. Note that the MAE score is approximately 1.7-1.9 times larger than the Froehlich-Krumm distance, and that if the paths continued as indicated by the arrows, this diﬀerence would continue to increase.
68

Figure 3.4: An example of the MAE and the Froehlich-Krumm distance when comparing between two trails represented by points shaped as diamonds and circles respectively. Evaluation of the MAE under three diﬀerent resampling procedures and the two diﬀerent approaches to dealing with extra length are shown. In the Froehlich-Krumm distance at each point in one trail, the closest point on the segments between all points in the other trail is taken and the distance calculated and vice versa. In all cases the average distance is reported. Note that the Froehlich-Krumm distance counts the bump relatively less, compared to the other sections which are counted twice before the weighted average is taken. As such in this case the Froehlich-Krumm distance underestimates the distance compared to what would typically be expected.
69

Figure 3.5: An example of the MAE and the Froehlich-Krumm distance when comparing between two trails that cross over. Each trail is represented by points shaped as diamonds and circles respectively. Evaluation of the MAE under three diﬀerent resampling procedures and the two diﬀerent approaches to dealing with extra length are shown. In the Froehlich-Krumm distance at each point in one trail, the closest point on the segments between all points in the other trail is taken and the distance calculated and vice versa.
70

Figure 3.6: An example highlighting a case where the Froehlich-Krumm distance underestimates the distance compared to what would generally be expected, compared to the average discrete Fr´echet distance where a more intuitive solution is achieved. Note, however, the eﬀect of the discrete sampling at irregular intervals means that both measures do not give a perfect answer.
This modiﬁed version of the average discrete Fr´echet distance is a variant of the discrete Fr´echet distance [7] which has seen the application in numerous domains, including related domains such as map matching [28, 36, 183, 195]. It is of note that it is similar in spirit to the Froehlich-Krumm distance proposed by [69] and to the Hausdorﬀ distance which the Froehlich-Krumm distance is based. In contrast to the Hausdorﬀ distance the average discrete Fr´echet distance takes into account the point observation order within the trails. With respect to the Froehlich-Krumm distance the average discrete Fr´echet distance selects the point-to-point comparisons while simulationsly considering both trails and therefore does not suﬀer from the same problems observed earlier. As shown in ﬁgure 3.2, in contrast to the Froehlich-Krumm distance, the average discrete Fr´echet distance automatically (in contrast to the MAE where this behaviour is ﬁxed) considers the same set of point-to-point intuitive comparisons as the MAE. A further two examples are shown in ﬁgure 3.6 and 3.7 with the second the same example as in ﬁgure 3.5 showing crossed trails. Combined, ﬁgures 3.5 and 3.7 show an example where the automatic selection of the measured point-to-point comparisons is superior to the MAE under various resampling schemes.
It is of note that there also exists a continuous version of the Fr´echet distance. This version is not utilized as it makes assumptions about the accuracy of all points sampled for each trail. Speciﬁcally the continuous version of the Fr´echet distance integrates to get
71

Figure 3.7: An example of the Froehlich-Krumm distance vs. the average discrete Fr´echet distance for two trails that cross. An extension of the example in ﬁgure 3.5 the ﬁgure shows the more intuitive point-to-point comparisons selected by the average discrete Fr´echet distance.
72

the area between the trails. Intuitively this can be seen as resampling via interpolation with an inﬁnite resample rate. Since it is often the case that points jump, in other words a point is signiﬁcantly in error, it is undesirable to interpolate all the way out to, and back from, that point adding point-to-point comparisons at each new interpolated point. Rather it is desirable that this point only contributes one sample to the overall error calculation. As such no resampling is preferred and the discrete version of the algorithm used. Further discussion will focus solely on the discrete version, with any discussion of the Fr´echet distance referring to the discrete version. When each trail is represented by a set of points the average discrete Fr´echet distance can be interpreted as the average of the minimum total lengths connecting points across such that all points on the trails are occupied and the lengths do not cross over. Conceptually points from one path are matched to the closest point in the other under an ordering constraint. The ordering constraint used ensures that no backtracking along either trail occurs between inter-trail point pairs while allowing pauses of arbitrary length. Formally the average Fr´echet distance is deﬁned as follows: Let P, Q be polygonal curves made up of sequentially ordered points R = (r1, . . . , r|R|) and F = (f1, .., f|F |) respectively. Once again, without loss of generality these curves will be considered the prediction result and its corresponding truth respectively. Deﬁne a coupling L between R and F as a sequence of point pairs where each pair comprises of one point from curve R and one point from curve F such that all points from R and F are used at least once:
L = (ra1, fb1), (ra2, fb2), . . . , (ram, fbm) In each pair a and b refer to indexes into P and Q respectively. Note that in each pair a and b may take diﬀerent values. The additional subscript on the indexes denote the order of the point represented by the index in the coupling L. A sequence is only a
73

coupling if it satisﬁes the constraints:

a1 = 1 b1 = 1 am = |R| bm = |F | ∀i=1,...,q(ai+1 = ai ∨ ai+1 = ai + 1) ∧ (bi+1 = bi ∨ bi+1 = bi + 1)

The constraints a1 = 1, b1 = 1, am = |R|, bm = |F | ensure that the ﬁrst pair in the sequence is the pair of ﬁrst points in the curves and that the last pair is the end points from both curves. The remainder of the constraints force a coupling to respect the sequential order of the points in R and F . A coupling denotes a potential set of pairwise points between R and F . There are a large number of such couplings. Let L be the set of all possible couplings.

The discrete sum Fr´echet distance [60] is then the coupling that minimizes the following

function, where d(·, ·) is an arbitrary1 distance function between two point observa-

tions:

m

Fr´echetSum = min L∈L

d(rai , fbi )

i=1

(3.6)

The average discrete Fr´echet distance is then deﬁned as:

min Fr´echetAvg = L∈L

m i=1

d(rai

,

fbi

)

m

(3.7)

An example showing two potential couplings (including the selected minimal coupling) as considered by the average discrete Fr´echet distance is shown in ﬁgure 3.8.

The average discrete Fr´echet distance can be calculated in the R statistical computing software [158] with some small modiﬁcations to the pathFrechet function in the package longitudinalData [72]2. The modiﬁcations are provided as a new function in appendix A.

While the average discrete Fr´echet distance has been motivated as being able to ﬁnd
good point-to-point comparisons to measure it is still restricted to choosing between the
1In this work the Euclidean distance function is used. 2Which in turn is based on the algorithm from [60]

74

F:

f1

f2

(ra1 , fb1 )

R:

r1

(ra2 , fb2 )

f3

f4

(ra3 , fb3 ) (ra4 , fb4 )

(ra5 , fb5 )

r2

r3

L1 = (ra1 , fb1 ), . . . , (ra5 , fb5 )

F:

f1

f2

f3

f4

(ra1 , fb1 )

R:

r1

(ra2 , fb2 )

(ra3 , fb3 )

(ra5 , fb5 )

(ra4 , fb4 )

r2

r3

L2 = (ra1 , fb1 ), . . . , (ra5 , fb5 )

Figure 3.8: A example of two trails, R and F of diﬀerent lengths, showing two potential couplings (L1 and L2). In this case L2 is the coupling with the minimal score by equation 3.7 and its value by this equation is the average Fr´echet distance. Coupling L1 represents another valid coupling, but not the minimal coupling.

sample points available. As previously discussed it is undesired to introduce interpolated sample points due to the nature of GPS and the noise it contains. Unfortunately, however, the other form of error within GPS, missing symbols means that it would be unwise to not perform any interpolation. This is easily illustrated in ﬁgure 3.9 as an extreme case where a large number of points from the second trail as missing. As is easily visible, this forces the discrete Fr´echet distance to measure undesirable pointto-point distances. Therefore, while undesirable in general, some resampling needs to be performed. In this thesis linear interpolation is used to resample and the smallest segment length in the two paths being compared used as the segment length. The ability to perform this resampling automatically is also provided as part of the new function in appendix A.
A ﬁnal concern is diﬀering lengths between the prediction and ground truth. Here, two cases must be considered, (1) the prediction is shorter than the ground truth and (2) the prediction is longer than the ground truth. In the former it would generally be expected that the shorter prediction would get penalized for making a shorter prediction, in the latter the case is not immediately as clear. On one hand it may be desirable to predict the end point and hence longer predictions should be penalized. On the other hand it could be argued that as long as the prediction followed the ground truth then it was correct,

75

Figure 3.9: An example highlighting the need for resampling in the presence of missing samples.

regardless of what it did afterwards. The discrete average Fr´echet distance addresses the

case of short predictions by penalizing the prediction at a cost of the distance between

the ﬁnal point on the prediction and the point further along in the ground truth sequence

before the average is taken. This seems reasonable. In the case of long predictions the

same penalization is applied, although in this case the penalty is occurred for each

additional point in the prediction. Unfortunately this can lead to the unexpected result

of reducing the overall average score if, for instance, the extra length stays very close to

the ﬁnal point in the ground truth. As such either a strict penalty must be applied or this

possibility considered acceptable. Either case is suboptimal for measuring performance

since the exact value of the penalty that should be applied is unclear. Due to this, in

this thesis any extra prediction length is not considered. Both cases can be implemented

as a modiﬁcation of the average Fr´echet distance. This is achieved by examining the

selected minimal coupling Lmin (see equation 3.8) based on the summed Fr´echet distance penalty from which average Fr´echet distance is calculated

m
Lmin = argmin d(rai, fbi)
L∈L i=1

(3.8)

The truncated average discrete Fr´echet distance which takes into account the desire not to penalize extra prediction length is therefore deﬁned here as:

76

Let R = the prediction curve, with a unaltered deﬁnition otherwise

Let F = the curve of what actually happened, with an unaltered deﬁnition otherwise

Recall: R = r1, . . . , r|R| and F = f1, .., f|F |

Recall: L is an arbitrary coupling of (R, F ), L = (ra1, fb1), (ra2, fb2), . . . , (ram, fbm) Recall: d(·, ·) is a arbitrary distance function between two point observations

Recall: L is the set of all possible couplings of (R, F )

Let F I(L) return the ﬁrst index i of a pair (ra, fb) in L in which fb = f|F |

m

Let Lmin = argmin d(rai, fbi)
L∈L i=1

Fr´echetAvgTrunc(R,F) =

F I(Lmin) i=1

d(rai ,

fbi

)

F I(Lmin)

(3.9)

In other words, the summation of Lmin is stopped once the last point in the ground truth trail has been accounted for. At this point it is possible to either use the sum as is (as per equation 3.9) or to add a custom penalty for the additional length in the prediction, in both cases adjusting the divisor accordingly. The truncated average discrete Fr´echet distance is implemented with an ﬂag (default to FALSE, as used in this thesis) to indicate whether extra distance is to be penalized. The code additionally resamples based on the smallest segment by default as previously discussed with a ﬂag to disable this behaviour. Custom penalties for extra length are currently not supported although this modiﬁcation is quite straightforward. The R code is provided in appendix A.

3.1.8 Aggregating prediction scores
In the previous sections distance measures used to measure the prediction quality of a single prediction were discussed. The goal, however, is to measure the prediction quality of individual predictors not just the prediction quality for a single prediction by a speciﬁc predictor. In order to achieve this goal it is important to consider the performance of the predictors across multiple predictions. This requires a method for aggregating a set of prediction results. A decision on this method is required before discussing the end goal of a more general measure between two predictors. In line with later use the aggregation considered here is the overall prediction quality calculated across a number
77

of predictions given a trained predictor (in other words for a ﬁxed training set K).

This aggregation is calculated by creating a test set of full historic trails T , that are

(generally) not part of K, and segmenting them to create a set of (input, observed re-

sult) pairs as required for individual trail prediction. Speciﬁcally ∀T ∈ T , T = E, F

where E = e1, . . . , e|E| , F = f|E|+1, . . . , f|T | and e and f represent point observations (feature vectors) including location (e.g. [longitude, latitude]). For convenience a pre-

dictor is formalized as a function P(K, ·) that takes as arguments a training set K and

a arbitrary observation (indicated by the ·) from which a prediction is made. Given

a set of test cases T a naive approach would be to simply take the mean, a common

measure of central tendency, over all predictions. Taking this approach the aggregation

of prediction error (deﬁned as P E) of predictor P, given training set K and test set T ,

is:

P E( P(K, ·), T ) =

|T | i=1

Fr´echetAvgTrunc(P

(K,

Ei),

Fi)

|T |

(3.10)

Unfortunately this does not turn out to be an aggregation that reﬂects the overall quality

of the predictor in practice. Speciﬁcally it does not provide a good measure of central

tendency because the distribution of individual prediction errors are not normally dis-

tributed. To illustrate this prediction error histograms from predictors evaluated in

Chapter 6 are shown in ﬁgure 3.1.8. In each case the histogram shows results for a

predictor (for more details on the predictor see chapter 6) which has been trained on a

ﬁxed

historic

set

(

9 10

of

the

whole

data

set)

and

evaluated

on

a

held

out

test

set

(

1 10

of

the whole data set). The result is that the mean is taken in the direction of the skew.

Since the error can never be less than zero, this always increases the mean compared to

what might be expected. In addition the mean can be quite susceptible to outliers.

For example consider two arbitrary predictors, A and B, making twenty diﬀerent predictions. Consider the case where the MAE is used to evaluate the predictors and that the MAE value can range from zero (best) and an arbitrary large upper bound (worst). If method A makes 10 exactly correct predictions (zero MAE) but also 10 completely wrong predictions (MAE value of 50) then the method would average 25. In contrast say method B makes 20 predictions with a MAE value of 20, resulting in an average of 20. In this case, via averages, method B would be considered superior. However, this is an undesirable conclusion if we consider that the MAE values could have been referring to the average distance at each time step between the prediction and correct result in metres. As such method B always makes wildly incorrect predictions (on average out

78

by 20m at each point) whereas method A makes correct predictions 50% of the time and wildly incorrect predictions the other 50% of the time, making it a more useful prediction method that B.
An alternative measure of central tendency that is generally recommended for skewed data is the median. The median represents the middle value of the set of prediction errors and is the value that separates the upper half from the lower half. With regard to route prediction the median indicates, for a given predictor for that test set, that 50% of the predictions were better than that value.
Here, however, a third option is proposed. This is the use of a binary decision function based on a user-deﬁned level of an acceptable prediction at the level of an individual prediction. The aggregation (via the mean) can then be interpreted as the proportion of predictions which were acceptable. This approach makes use of the fact that in many applications predictions are only useful to a certain threshold. This results in a proportion similar to evaluation approaches that rely on exact matching. However, the approach allows an arbitrary speciﬁcation of the accuracy for which a prediction is deemed correct, decoupling this from the level of symbolic quantization. In addition the approach does not exclude the examination of the predictions distance values before applying a threshold.
When deciding between the median and the proportion approach it is important to realise that just because predictor A has a lower median than predictor B does not mean that predictor A makes more predictions below a certain threshold than predictor B. Real world examples where the median for predictor A is smaller than the median of predictor B while predictor B has predicted more below a relevance threshold of 5m are shown in ﬁgure 3.1.8. In the ﬁgure the ﬁrst column represents predictor A and the second predictor B. Each row then represents a comparison of interest. Considering pairwise comparisons between all test/training sets over all predictors used in chapter 6 these cases occur in 17.29% of the pairs.
While either could be used they therefore provide diﬀerent views on predictors performance and this may lead to diﬀerent conclusions as to which predictor is better. Here the proportion based approach is considered superior as, after selecting a application speciﬁc threshold, the results clearly deﬁne how ﬁt for purpose the various prediction algorithms are. Speciﬁcally the proportion of acceptable predictions on average the predictor will make is measured. In addition it is important to note that a general understanding of
79

the performance of the predictors over varying thresholds can still be obtained via descriptive statistics and visualizations. In fact this is recommended later in this chapter with a number of visualizations provided for this purpose in section 3.3

In contrast the median only provides a notion that at least 50% will be better than the value reported in the evaluation. While the median will provide a (potentially diﬀerent) indication of which predictor is better it does not provide a concrete indication of whether an algorithm is ﬁt for the purpose of the target application. Of course the answer could be to always select the best performing predictor, as deﬁned by the lowest median. However, as noted, this still may not make the most correct predictions to the level deemed acceptable by the application. Additionaly, as discussed in more depth in chapter 4 the best predictor may not be the one desired, rather the desired predictor may be one that is computationally eﬃcent while still maintaing good prediction quailty within the application speciﬁc threshold.

Based on the above discussion the following deﬁnition of prediction quality of an individual predictor (P) for a given training set (K), test set (T ) and user-deﬁned level of an acceptable prediction (α) is advocated and used throughout this thesis:

P E( P(K, ·), T ) =

|T | i=1

γ

(

Fr´echetAvgTrunc(P

(K,

Ei),

Fi)

)

|T |

where:
 1 γ(x) = 0

if x < α otherwise

(3.11)

3.1.9 Summary: A distance metric for movement prediction
Measuring the quality of a prediction in a way that is meaningful in practice is an important issue. Much previous research has focused on the use of proportions through binary similarity functions based on the underlying symbolic encoding. Rarely, however, is the eﬀect of the previously chosen level of symbol quantization on evaluation considered. One solution is to use a distance function which then provides a more detailed understanding of the diﬀerence between the prediction and the correct answer that can be achieved. With a straightforward interpretation the mean average error (MAE) provides an intuitive and widely used measure of the diﬀerence between two time series.
80

However, even if resampling is used to align the samples, the results do not reﬂect the general intuition. To this end [69] proposed a Hausdorﬀ based distance metric. While performing better, it still has shortcomings and so the truncated average discrete Fr´echet distance was proposed. Having deﬁned the prediction error for an individual prediction the problem of aggregating the prediction error of a trained predictor (PE) was discussed. It was noted that the error distribution of the predictions is not normal but rather positively skewed. In addition it was noted that outliers have the potential to cause serious issues. To address this an approach that results in proportions derived from a user-deﬁned threshold deﬁning a useful prediction is advocated. The approach addresses the issue by classifying each individual prediction as either useful or not based on user-deﬁned level of an acceptable prediction. The threshold is based on the truncated average discrete Fr´echet distance providing a threshold with a well deﬁned and intuitive interpretation. Finally graphs of the proportions over varying thresholds is advocated providing a simple yet informative view of comparative performance. These are detailed in section 3.3. Statistical testing, however, must be carried out at the speciﬁc levels of interest. This is discussed further in section 3.2 along with how to calculate the actual prediction error of interest, the prediction error of a predictor independent of the speciﬁc training set.
3.2 Testing methodology: Comparing predictors
Evaluation of the performance of diﬀerent prediction algorithms is clearly an important task. Here the goal is to evaluate the question is there a diﬀerence in prediction performance between two predictors?. Performance here is deﬁned as the prediction quality of a predictor independent of the training set and prediction instance. This can be deﬁned as the expected3 performance of predictors trained with an arbitrary training set. Calculating such a statistic involves varying not only the test data as done to calculate the P E but also varying the training set used. Additionally it is important to note that the predictor error is also dependent on the size of the historic training set used. This parameter is generally not varied, however, as it is expected that learning algorithms will not decrease their performance as the number of training instances is increased. Additionally when used in practice the amount of training data is expected to increase with time. Therefore it does not make sense to evaluate the performance of predictors
3The expected performance is the mean performance.
81

with less than the maximum possible training set size. In the case of route prediction these arguments certainly seems reasonable since historic observations are continuously being generated by the system.
Formally the expected prediction performance, EP E, for a predictor P trained on an arbitrary training set K of ﬁxed size |K| can be deﬁned as:

EP E|K|(P) = E[P E(K, ·), T )]

(3.12)

Where E[·] denotes the expectation (mean) and T denotes arbitrary test sets. The function P E(·, ·) is as deﬁned in equation 3.11.

Having deﬁned the measure of interest it is important to recognize that each predictor,

P, has a true EP E|K|(P) value. However, without enumerating the whole population of trails it is not possible to calculate this value. Therefore this must be estimated. Since

it is an estimated quantity it also important to assess the uncertainty surrounding this

estimation. In assessing the uncertainty the distribution of the sampled P E values is

typically assumed to be normal. By making such an assumption and calculating the

variance of the EP E (denoted as V ar[EP E]) standard statistical tests such as paired

t-test or paired diﬀerence test can be applied to determine if statistically signiﬁcant

diﬀerences exist between predictors. Formally these tests answer the question of is there

a diﬀerence in prediction performance between two predictors A and B? through the null

hypothesis:

H0 : EP E(A) = EP E(B)

(3.13)

82

