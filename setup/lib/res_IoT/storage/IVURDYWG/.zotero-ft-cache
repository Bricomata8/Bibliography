The first person to invent a car that runs on water…
… may be sitting right in your classroom! Every one of your students has the potential to make a difference. And realizing that potential starts right here, in your course.
When students succeed in your course—when they stay on-task and make the breakthrough that turns confusion into confidence—they are empowered to realize the possibilities for greatness that lie within each of them. We know your goal is to create an environment where students reach their full potential and experience the exhilaration of academic success that will last them a lifetime. WileyPLUS can help you reach that goal.
WileyPLUS is an online suite of resources—including the complete text—that will help your students:
• come to class better prepared for your lectures • get immediate feedback and context-sensitive help on assignments
and quizzes • track their progress throughout the course
“I just wanted to say how much this program helped me in studying… I was able to actually see my mistakes and correct them. … I really think that other students should have the chance to use WileyPLUS.”
Ashlee Krisko, Oakland University
www.wiley.com/college/wileyplus
80% of students surveyed said it improved their understanding of the material. *

FOR INSTRUCTORS

WileyPLUS is built around the activities you perform in your class each day. With WileyPLUS you can:

Prepare & Present
Create outstanding class presentations using a wealth of resources such as PowerPoint™ slides, image galleries, interactive simulations, and more. You can even add materials you have created yourself.

Create Assignments

Track Student Progress

Automate the assigning and grading of

Keep track of your students' progress

homework or quizzes by using the pro- and analyze individual and overall class

vided question banks, or by writing your results.

own.
Now Available with WebCT and Blackboard!

“It has been a great help, and I believe it has helped me to achieve a better grade.”
Michael Morris, Columbia Basin College

FOR STUDENTS

You have the potential to make a difference!
WileyPLUS is a powerful online system packed with features to help you make the most of your potential and get the best grade you can!

With WileyPLUS you get:

• A complete online version of your text and other study resources.

• Problem-solving help, instant grading, and feedback on your homework and quizzes.

• The ability to track your progress and grades throughout the term.

For more information on what WileyPLUS can do to help you and your students reach their potential, please visit www.wiley.com/college/wileyplus.
76% of students surveyed said it made them better prepared for tests. *
*Based on a survey of 972 student users of WileyPLUS

Introduction to Analog and Digital Communications

This page intentionally left blank

Introduction to Analog and Digital Communications
Second Edition Simon Haykin
McMaster University, Hamilton, Ontario, Canada
Michael Moher
Space-Time DSP, Ottawa, Ontario, Canada
JOHN WILEY & SONS, INC.

ASSOCIATE PUBLISHER Dan Sayre SENIOR ACQUISITIONS EDITOR AND PROJECT MANAGER PROJECT EDITOR Gladys Soto MARKETING MANAGER Phyllis Diaz Cerys EDITORIAL ASSISTANT Dana Kellog SENIOR PRODUCTION EDITOR Lisa Wojcik MEDIA EDITOR Stefanie Liebman DESIGNER Hope Miller SENIOR ILLUSTRATION EDITOR Sigmund Malinowski COVER IMAGE © Photodisc/Getty Images

Catherine Shultz

This book was set in Quark by Prepare Inc. and printed and bound by Hamilton Printing. The cover was printed by Phoenix Color Corp.
This book is printed on acid free paper. ϱ
Copyright © 2007 John Wiley & Sons, Inc. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978)750-8400, fax (978)646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030-5774, (201)7486011, fax (201)748-6008, or online at http://www.wiley.com/go/permissions.
To order books or for customer service please, call 1-800-CALL WILEY (225-5945).
ISBN-13 978-0-471-43222-7 ISBN-10 0-471-43222-9
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

To the 20th Century pioneers in communications who, through their mathematical theories and ingenious devices,
have changed our planet into a global village

This page intentionally left blank

PREFACE
An introductory course on analog and digital communications is fundamental to the undergraduate program in electrical engineering. This course is usually offered at the junior level. Typically, it is assumed that the student has a background in calculus, electronics, signals and systems, and possibly probability theory.
Bearing in mind the introductory nature of this course, a textbook recommended for the course must be easy to read, accurate, and contain an abundance of insightful examples, problems, and computer experiments. These objectives of the book are needed to expedite learning the fundamentals of communication systems at an introductory level and in an effective manner. This book has been written with all of these objectives in mind.
Given the mathematical nature of communication theory, it is rather easy for the reader to lose sight of the practical side of communication systems. Throughout the book, we have made a special effort not to fall into this trap. We have done this by moving through the treatment of the subject in an orderly manner, always trying to keep the mathematical treatment at an easy-to-grasp level and also pointing out practical relevance of the theory wherever it is appropriate to do so.
Structural Philosophy of the Book
To facilitate and reinforce learning, the layout and format of the book have been structured to do the following:
• Provide motivation to read the book and learn from it. • Emphasize basic concepts from a “systems” perspective and do so in an orderly manner. • Wherever appropriate, include examples and computer experiments in each chapter to illus-
trate application of the pertinent theory. • Provide drill problems following the discussion of fundamental concepts to help the user
of the book verify and master the concepts under discussion. • Provide additional end-of-chapter problems, some of an advanced nature, to extend the
theory covered in the text.
Organization of the book
1. Motivation Before getting deeply involved in the study of analog and digital communications, it is imperative that the user of the book be motivated to use the book and learn from it. To this end, Chapter 1 begins with a historical background of communication systems and important applications of the subject.
2. Modulation Theory Digital communication has overtaken analog communications as the dominant form of communications. Although, indeed, these two forms of communications work in different ways, modulation theory is basic to them both. Moreover, it is easiest to understand this important subject by first covering its fundamental concepts applied to analog communications and then moving on to digital communications. Moreover, amplitude modulation is simpler than angle modulation to present. One other highly relevant point is the fact that to understand modulation theory, it is important that Fourier theory be mastered first. With these points in mind, Chapters 2 through 7 are organized as follows:
ix

x

APPENDIX 1 ᭿ POWER RATIOS AND DECIBEL

• Chapter 2 is devoted to reviewing the Fourier representation of signals and systems. • Chapters 3 and 4 are devoted to analog communications, with Chapter 3 covering ampli-
tude modulation and Chapter 4 covering angle modulation.
• Chapter 5 on pulse modulation covers the concepts pertaining to the transition from analog to digital communications.
• Chapters 6 and 7 are devoted to digital communications, with Chapter 6 covering baseband data transmission and Chapter 7 covering band-pass data transmission.
3. Probability Theory and Signal Detection Just as Fourier analysis is fundamental to modulation theory, probability theory is fundamental to signal detection and receiver performance evaluation in the presence of additive noise. Since probability theory is not critical to the understanding of modulation, we have purposely delayed the review of probability theory, random signals, and noise until Chapter 8. Then, with a good understanding of modulation theory applied to analog and digital communications and relevant concepts of probability theory and probabilistic models at hand, the stage is set to revisit analog and digital communication receivers, as summarized here:
• Chapter 9 discusses noise in analog communications. • Chapter 10 discusses noise in digital communications. Because analog and digital com-
munications operate in different ways, it is natural to see some fundamental differences in treating the effects of noise in these two chapters.
4. Noise The introductory study of analog and digital communications is completed in Chapter 11. This chapter illustrates the roles of modulation and noise in communication systems by doing four things:
• First, the physical sources of noise, principally, thermal noise and shot noise, are described. • Second, the metrics of noise figure and noise temperature are introduced.
• Third, how propagation affects the signal strength in satellite and terrestrial wireless communications is explained.
• Finally, we show how the signal strength and noise calculations may be combined to provide an estimate of the signal-to-noise ratio, the fundamental figure of merit for communication systems.
5. Theme Examples In order to highlight important practical applications of communication theory, theme examples are included wherever appropriate. The examples are drawn from the worlds of both analog and digital communications.
6. Appendices To provide back-up material for the text, eight appendices are included at the end of the book, which cover the following material in the order presented here:
• Power ratios and the decibel • Fourier series
• Bessel functions
• The Q-function and its relationship to the error function
• Schwarz’s inequality
• Mathematical tables

Preface

xi

• Matlab scripts for computer experiments to problems in Chapters 7–10
• Answers to drill problems
7. Footnotes, included throughout the book, are provided to help the interested reader to pursue selected references for learning advanced material.
8. Auxiliary Material The book is essentially self-contained. A glossary of symbols and a bibliography are provided at the end of the book. As an aid to the teacher of the course using the book, a detailed Solutions Manual for all the problems, those within the text and those included at the end of chapters, will be made available through the publisher: John Wiley and Sons.

How to Use the Book
The book can be used for an introductory course on analog and digital communications in different ways, depending on the background of the students and the teaching interests and responsibilities of the professors concerned. Here are two course models of how this may be done:
COURSE MODEL A: FULL TWO-SEMESTER COURSE
(A.1) The first semester course on modulation theory consists of Chapters 2 through 7, inclusive.
(A.2) The second semester course on noise in communication systems consists of Chapters 8 through 11, inclusive.
COURSE MODEL B: TWO SEMESTER COURSES, ONE ON ANALOG AND THE OTHER ON DIGITAL
(B.1) The first course on analog communications begins with review material from Chapter 2 on Fourier analysis, followed by Chapter 3 on amplitude modulation and Chapter 4 on angle modulation, then proceeds with a review of relevant parts of Chapter 8 on noise, and finally finishes with Chapter 9 on noise in analog communications.
(B.2) The second course on digital communications starts with Chapter 5 on pulse modulation, followed by Chapter 6 on baseband data transmission and Chapter 7 on digital modulation techniques, then proceeds with review of relevant aspects of probability theory in Chapter 8, and finally finishes with Chapter 10 on noise in digital communications.
Simon Haykin Ancaster, Ontario, Canada
Michael Moher Ottawa, Ontario, Canada

This page intentionally left blank

ACKNOWLEDGEMENTS
The authors would like to express their deep gratitude to • Lily Jiang, formerly of McMaster University for her help in performing many of the computer experiments included in the text. • Wei Zhang, for all the help, corrections, and improvements she has made to the text.
They also wish to thank Dr. Stewart Crozier and Dr. Paul Guinand, both of the Communications Research Centre, Ottawa, for their inputs on different parts of the book.
They are also indebted to Catherine Fields Shultz, Senior Acquisitions Editor and Product Manager (Engineering and Computer Science) at John Wiley and Sons, Bill Zobrist formerly of Wiley, and Lisa Wojcik, Senior Production Editor at Wiley, for their guidance and dedication to the production of this book.
Last but by no means least, they are grateful to Lola Brooks, McMaster University, for her hard work on the preparation of the manuscript and related issues to the book.
xiii

This page intentionally left blank

CONTENTS

Chapter 1 Introduction

1

1.1 Historical Background 1 1.2 Applications 4 1.3 Primary Resources and Operational Requirements 13 1.4 Underpinning Theories of Communication Systems 14 1.5 Concluding Remarks 16

Chapter 2 Fourier Representation of Signals and Systems

18

2.1 The Fourier Transform 19 2.2 Properties of the Fourier Transform 25 2.3 The Inverse Relationship Between Time and Frequency 39 2.4 Dirac Delta Function 42 2.5 Fourier Transforms of Periodic Signals 50 2.6 Transmission of Signals Through Linear Systems: Convolution
Revisited 52 2.7 Ideal Low-pass Filters 60 2.8 Correlation and Spectral Density: Energy Signals 70 2.9 Power Spectral Density 79 2.10 Numerical Computation of the Fourier Transform 81 2.11 Theme Example: Twisted Pairs for Telephony 89 2.12 Summary and Discussion 90

Additional Problems 91 Advanced Problems 98

Chapter 3 Amplitude Modulation

100

3.1 Amplitude Modulation 101 3.2 Virtues, Limitations, and Modifications of Amplitude Modulation 113 3.3 Double Sideband-Suppressed Carrier Modulation 114 3.4 Costas Receiver 120

xv

xvi

APPENDIX 1 ᭿ POWER RATIOS AND DECIBEL

3.5 Quadrature-Carrier Multiplexing 121 3.6 Single-Sideband Modulation 123 3.7 Vestigial Sideband Modulation 130 3.8 Baseband Representation of Modulated Waves and Band-Pass
Filters 137 3.9 Theme Examples 142 3.10 Summary and Discussion 147

Additional Problems 148 Advanced Problems 150

Chapter 4 Angle Modulation
4.1 Basic Definitions 153 4.2 Properties of Angle-Modulated Waves 154 4.3 Relationship between PM and FM Waves 159 4.4 Narrow-Band Frequency Modulation 160 4.5 Wide-Band Frequency Modulation 164 4.6 Transmission Bandwidth of FM Waves 170 4.7 Generation of FM Waves 172 4.8 Demodulation of FM Signals 174 4.9 Theme Example: FM Stereo Multiplexing 182 4.10 Summary and Discussion 184
Additional Problems 185 Advanced Problems 187

152

Chapter 5 Pulse Modulation: Transition from Analog to Digital

Communications

190

5.1 Sampling Process 191 5.2 Pulse-Amplitude Modulation 198 5.3 Pulse-Position Modulation 202 5.4 Completing the Transition from Analog to Digital 203 5.5 Quantization Process 205 5.6 Pulse-Code Modulation 206

Contents

xvii

5.7 Delta Modulation 211 5.8 Differential Pulse-Code Modulation 216 5.9 Line Codes 219 5.10 Theme Examples 220 5.11 Summary and Discussion 225

Additional Problems 226 Advanced Problems 228

Chapter 6 Baseband Data Transmission
6.1 Baseband Transmission of Digital Data 232 6.2 The Intersymbol Interference Problem 233 6.3 The Nyquist Channel 235 6.4 Raised-Cosine Pulse Spectrum 238 6.5 Baseband Transmission of M-ary Data 245 6.6 The Eye Pattern 246 6.7 Computer Experiment: Eye Diagrams for Binary and Quaternary
Systems 249 6.8 Theme Example: Equalization 251 6.9 Summary and Discussion 256
Additional Problems 257 Advanced Problems 259

231

Chapter 7 Digital Band-Pass Modulation Techniques
7.1 Some Preliminaries 262 7.2 Binary Amplitude-Shift Keying 265 7.3 Phase-Shift Keying 270 7.4 Frequency-Shift Keying 281 7.5 Summary of Three Binary Signaling Schemes 289 7.6 Noncoherent Digital Modulation Schemes 291 7.7 M-ary Digital Modulation Schemes 295 7.8 Mapping of Digitally Modulated Waveforms onto Constellations
of Signal Points 299

262

xviii

APPENDIX 1 ᭿ POWER RATIOS AND DECIBEL
7.9 Theme Examples 302 7.10 Summary and Discussion 307 Additional Problems 309 Advanced Problems 310 Computer Experiments 312

Chapter 8 Random Signals and Noise
8.1 Probability and Random Variables 314 8.2 Expectation 326 8.3 Transformation of Random Variables 329 8.4 Gaussian Random Variables 330 8.5 The Central Limit Theorem 333 8.6 Random Processes 335 8.7 Correlation of Random Processes 338 8.8 Spectra of Random Signals 343 8.9 Gaussian Processes 347 8.10 White Noise 348 8.11 Narrowband Noise 352 8.12 Summary and Discussion 356
Additional Problems 357 Advanced Problems 361 Computer Experiments 363

Chapter 9 Noise in Analog Communications
9.1 Noise in Communication Systems 365 9.2 Signal-to-Noise Ratios 366 9.3 Band-Pass Receiver Structures 369 9.4 Noise in Linear Receivers Using Coherent Detection 370 9.5 Noise in AM Receivers Using Envelope Detection 373 9.6 Noise in SSB Receivers 377 9.7 Detection of Frequency Modulation (FM) 380

313 364

Contents
9.8 FM Pre-emphasis and De-emphasis 387 9.9 Summary and Discussion 390
Additional Problems 391 Advanced Problems 392 Computer Experiments 393
Chapter 10 Noise in Digital Communications
10.1 Bit Error Rate 395 10.2 Detection of a Single Pulse in Noise 396 10.3 Optimum Detection of Binary PAM in Noise 399 10.4 Optimum Detection of BPSK 405 10.5 Detection of QPSK and QAM in Noise 408 10.6 Optimum Detection of Binary FSK 414 10.7 Differential Detection in Noise 416 10.8 Summary of Digital Performance 418 10.9 Error Detection and Correction 422 10.10 Summary and Discussion 433
Additional Problems 434 Advanced Problems 435 Computer Experiments 436
Chapter 11 System and Noise Calculations
11.1 Electrical Noise 438 11.2 Noise Figure 442 11.3 Equivalent Noise Temperature 443 11.4 Cascade Connection of Two-Port Networks 445 11.5 Free-Space Link Calculations 446 11.6 Terrestrial Mobile Radio 451 11.7 Summary and Discussion 456
Additional Problems 457 Advanced Problems 458

xix
394
437

xx

APPENDIX 1 ᭿ POWER RATIOS AND DECIBEL

APPENDIX 1 POWER RATIOS AND DECIBEL 459

APPENDIX 2 FOURIER SERIES 460

APPENDIX 3 BESSEL FUNCTIONS 467

APPENDIX 4 THE Q-FUNCTION AND ITS RELATIONSHIP TO THE ERROR FUNCTION 470

APPENDIX 5 SCHWARZ’S INEQUALITY 473

APPENDIX 6 MATHEMATICAL TABLES 475

APPENDIX 7 MATLAB SCRIPTS FOR COMPUTER EXPERIMENTS TO PROBLEMS IN CHAPTERS 7-10 480

APPENDIX 8 ANSWERS TO DRILL PROBLEMS 488

GLOSSARY 495

BIBLIOGRAPHY 498

INDEX 501

CHAPTER 1
INTRODUCTION
“To understand a science it is necessary to know its history”
—Auguste Comte (1798–1857)
1.1 Historical Background
With this quotation from Auguste Comte in mind, we begin this introductory study of communication systems with a historical account of this discipline that touches our daily lives in one way or another.1 Each subsection in this section focuses on some important and related events in the historical evolution of communication.
Telegraph The telegraph was perfected by Samuel Morse, a painter. With the words “What hath God wrought,” transmitted by Morse’s electric telegraph between Washington, D.C., and Baltimore, Maryland, in 1844, a completely revolutionary means of real-time, long-distance communications was triggered. The telegraph, ideally suited for manual keying, is the forerunner of digital communications. Specifically, the Morse code is a variable-length code using an alphabet of four symbols: a dot, a dash, a letter space, and a word space; short sequences represent frequent letters, whereas long sequences represent infrequent letters.
Radio In 1864, James Clerk Maxwell formulated the electromagnetic theory of light and predicted the existence of radio waves; the underlying set of equations bears his name. The existence of radio waves was confirmed experimentally by Heinrich Hertz in 1887. In 1894, Oliver Lodge demonstrated wireless communication over a relatively short distance (150 yards). Then, on December 12, 1901, Guglielmo Marconi received a radio signal at Signal Hill in Newfoundland; the radio signal had originated in Cornwall, England, 1700 miles away across the Atlantic. The way was thereby opened toward a tremendous broadening of the scope of communications. In 1906, Reginald Fessenden, a self-educated academic, made history by conducting the first radio broadcast. In 1918, Edwin H. Armstrong invented the superheterodyne radio receiver; to this day, almost all radio receivers are of this type. In 1933, Armstrong demonstrated another revolutionary concept—namely, a modulation scheme that he called frequency modulation (FM). Armstrong’s paper making the case for FM radio was published in 1936.
1 This historical background is adapted from Haykin’s book (2001).
1

2

CHAPTER 1 ᭿ INTRODUCTION

Telephone
In 1875, the telephone was invented by Alexander Graham Bell, a teacher of the deaf. The telephone made real-time transmission of speech by electrical encoding and replication of sound a practical reality. The first version of the telephone was crude and weak, enabling people to talk over short distances only. When telephone service was only a few years old, interest developed in automating it. Notably, in 1897, A. B. Strowger, an undertaker from Kansas City, Missouri, devised the automatic step-by-step switch that bears his name. Of all the electromechanical switches devised over the years, the Strowger switch was the most popular and widely used.

Electronics
In 1904, John Ambrose Fleming invented the vacuum-tube diode, which paved the way for the invention of the vacuum-tube triode by Lee de Forest in 1906. The discovery of the triode was instrumental in the development of transcontinental telephony in 1913 and signaled the dawn of wireless voice communications. Indeed, until the invention and perfection of the transistor, the triode was the supreme device for the design of electronic amplifiers.
The transistor was invented in 1948 by Walter H. Brattain, John Bardeen, and William Shockley at Bell Laboratories. The first silicon integrated circuit (IC) was produced by Robert Noyce in 1958. These landmark innovations in solid-state devices and integrated circuits led to the development of very-large-scale integrated (VLSI) circuits and singlechip microprocessors, and with them the nature of signal processing and the telecommunications industry changed forever.

Television
The first all-electronic television system was demonstrated by Philo T. Farnsworth in 1928, and then by Vladimir K. Zworykin in 1929. By 1939, the British Broadcasting Corporation (BBC) was broadcasting television on a commercial basis.

Digital Communications
In 1928, Harry Nyquist published a classic paper on the theory of signal transmission in telegraphy. In particular, Nyquist developed criteria for the correct reception of telegraph signals transmitted over dispersive channels in the absence of noise. Much of Nyquist’s early work was applied later to the transmission of digital data over dispersive channels.
In 1937, Alex Reeves invented pulse-code modulation (PCM) for the digital encoding of speech signals. The technique was developed during World War II to enable the encryption of speech signals; indeed, a full-scale, 24-channel system was used in the field by the United States military at the end of the war. However, PCM had to await the discovery of the transistor and the subsequent development of large-scale integration of circuits for its commercial exploitation.
The invention of the transistor in 1948 spurred the application of electronics to switching and digital communications. The motivation was to improve reliability, increase capacity, and reduce cost. The first call through a stored-program system was placed in March 1958 at Bell Laboratories, and the first commercial telephone service with digital switching began in Morris, Illinois, in June 1960. The first T-1 carrier system transmission was installed in 1962 by Bell Laboratories.

1.1 Historical Background

3

In 1943, D. O. North devised the matched filter for the optimum detection of a known signal in additive white noise. A similar result was obtained in 1946 independently by J. H. Van Vleck and D. Middleton, who coined the term matched filter.
In 1948, the theoretical foundations of digital communications were laid by Claude Shannon in a paper entitled “A Mathematical Theory of Communication.” Shannon’s paper was received with immediate and enthusiastic acclaim. It was perhaps this response that emboldened Shannon to amend the title of his paper to “The Mathematical Theory of Communications” when it was reprinted a year later in a book co-authored with Warren Weaver. It is noteworthy that prior to the publication of Shannon’s 1948 classic paper, it was believed that increasing the rate of information transmission over a channel would increase the probability of error. The communication theory community was taken by surprise when Shannon proved that this was not true, provided the transmission rate was below the channel capacity.
Computer Networks
During the period 1943 to 1946, the first electronic digital computer, called the ENIAC, was built at the Moore School of Electrical Engineering of the University of Pennsylvania under the technical direction of J. Presper Eckert, Jr., and John W. Mauchly. However, John von Neumann’s contributions were among the earliest and most fundamental to the theory, design, and application of digital computers, which go back to the first draft of a report written in 1945. Computers and terminals started communicating with each other over long distances in the early 1950s. The links used were initially voice-grade telephone channels operating at low speeds (300 to 1200 b/s). Various factors have contributed to a dramatic increase in data transmission rates; notable among them are the idea of adaptive equalization, pioneered by Robert Lucky in 1965, and efficient modulation techniques, pioneered by G. Ungerboeck in 1982. Another idea widely employed in computer communications is that of automatic repeat-request (ARQ). The ARQ method was originally devised by H. C. A. van Duuren during World War II and published in 1946. It was used to improve radio-telephony for telex transmission over long distances.
From 1950 to 1970, various studies were made on computer networks. However, the most significant of them in terms of impact on computer communications was the Advanced Research Projects Agency Network (ARPANET), first put into service in 1971. The development of ARPANET was sponsored by the Advanced Research Projects Agency of the U. S. Department of Defense. The pioneering work in packet switching was done on ARPANET. In 1985, ARPANET was renamed the Internet. The turning point in the evolution of the Internet occurred in 1990 when Tim Berners-Lee proposed a hypermedia software interface to the Internet, which he named the World Wide Web. In the space of only about two years, the Web went from nonexistence to worldwide popularity, culminating in its commercialization in 1994. We may explain the explosive growth of the Internet by offering these reasons:
᭤ Before the Web exploded into existence, the ingredients for its creation were already in place. In particular, thanks to VLSI, personal computers (PCs) had already become ubiquitous in homes throughout the world, and they were increasingly equipped with modems for interconnectivity to the outside world.
᭤ For about two decades, the Internet had grown steadily (albeit within a confined community of users), reaching a critical threshold of electronic mail and file transfer.
᭤ Standards for document description and transfer, hypertext markup language (HTML), and hypertext transfer protocol (HTTP) had been adopted.

4

CHAPTER 1 ᭿ INTRODUCTION

Thus, everything needed for creating the Web was already in place except for two critical ingredients: a simple user interface and a brilliant service concept.

Satellite Communications
In 1955, John R. Pierce proposed the use of satellites for communications. This proposal was preceded, however, by an earlier paper by Arthur C. Clark that was published in 1945, also proposing the idea of using an Earth-orbiting satellite as a relay point for communication between two Earth stations. In 1957, the Soviet Union launched Sputnik I, which transmitted telemetry signals for 21 days. This was followed shortly by the launching of Explorer I by the United States in 1958, which transmitted telemetry signals for about five months. A major experimental step in communications satellite technology was taken with the launching of Telstar I from Cape Canaveral on July 10, 1962. The Telstar satellite was built by Bell Laboratories, which had acquired considerable knowledge from pioneering work by Pierce. The satellite was capable of relaying TV programs across the Atlantic; this was made possible only through the use of maser receivers and large antennas.

Optical Communications
The use of optical means (e.g., smoke and fire signals) for the transmission of information dates back to prehistoric times. However, no major breakthrough in optical communications was made until 1966, when K. C. Kao and G. A. Hockham of Standard Telephone Laboratories, U. K., proposed the use of a clad glass fiber as a dielectric waveguide. The laser (an acronym for light amplification by stimulated emission of radiation) had been invented and developed in 1959 and 1960. Kao and Hockham pointed out that (1) the attenuation in an optical fiber was due to impurities in the glass, and (2) the intrinsic loss, determined by Rayleigh scattering, is very low. Indeed, they predicted that a loss of 20 dB/km should be attainable. This remarkable prediction, made at a time when the power loss in a glass fiber was about 1000 dB/km, was to be demonstrated later. Nowadays, transmission losses as low as 0.1 dB/km are achievable.
The spectacular advances in microelectronics, digital computers, and lightwave systems that we have witnessed to date, and that will continue into the future, are all responsible for dramatic changes in the telecommunications environment. Many of these changes are already in place, and more changes will occur over time.

1.2 Applications
The historical background of Section 1.1 touches many of the applications of communication systems, some of which are exemplified by the telegraph that has come and gone, while others exemplified by the Internet are of recent origin. In what follows, we will focus on radio, communication networks exemplified by the telephone, and the Internet, which dominate the means by which we communicate in one of two basic ways or both, as summarized here:
᭤ Broadcasting, which involves the use of a single powerful transmitter and numerous receivers that are relatively inexpensive to build. In this class of communication systems, information-bearing signals flow only in one direction, from the transmitter to each of the receivers out there in the field.
᭤ Point-to-point communications, in which the communication process takes place over a link between a single transmitter and a single receiver. In this second class of communication systems, there is usually a bidirectional flow of information-bearing

1.2 Applications

5

Source of information

Information-bearing (message) signal

Communication System

Transmitter
Transmitted signal

Channel

Receiver

Estimate of message signal

Received signal

User of information

FIGURE 1.1 Elements of a communication system.

signals, which, in effect, requires the use of a transmitter and receiver (i.e., transceiver) at each end of the link.
The block diagram of Fig. 1.1 highlights the basic composition of a communication system. The transmitter, at some location in space, converts the message signal produced by a source of information into a form suitable for transmission over the channel. The channel, in turn, transports the message signal and delivers it to the receiver at some other location in space. However, in the course of transmission over the channel, the signal is distorted due to channel imperfections. Moreover, noise and interfering signals (originating from other sources) are added to the channel output, with the result that the received signal is a corrupted version of the transmitted signal. The receiver has the task of operating on the received signal so as to produce an estimate of the original message signal for the user of information. We say an “estimate” here because of the unavoidable deviation, however small, of the receiver output compared to the transmitter input, the deviation being attributed to channel imperfections, noise, and interference.
᭿ RADIO
Speaking in a generic sense, the radio embodies the means for broadcasting as well as pointto-point communications, depending on how it is used.
The AM radio and FM radio are both so familiar to all of us. (AM stands for amplitude modulation, and FM stands for frequency modulation.) The two of them are built in an integrated form inside a single unit, and we find them in every household and installed in every car. Via radio we listen to news about local, national, and international events, commentaries, music, and weather forecasts, which are transmitted from broadcasting stations that operate in our neighborhood. Traditionally, AM radio and FM radio have been built using analog electronics. However, thanks to the ever-increasing improvements and costeffectiveness of digital electronics, digital radio (in both AM and FM forms) is already in current use.
Radio transmits voice by electrical signals. Television, which operates on similar electromagnetic and communication-theoretic principles, also transmits visual images by electrical signals. A voice signal is naturally defined as a one-dimensional function of time, which therefore lends itself readily to signal-processing operations. In contrast, an image with motion is a two-dimensional function of time, and therefore requires more detailed attention. Specifically, each image at a particular instant of time is viewed as a frame subdivided into a number of small squares called picture elements or pixels; the larger the number of pixels used to represent an image, the better the resolution of that image will be. By scanning the pixels in an orderly sequence, the information contained in the image is converted into an electrical signal whose magnitude is proportional to the brightness level of the individual pixels. The electrical signal generated at the output of the scanner is

6

CHAPTER 1 ᭿ INTRODUCTION

Earth transmitting
station

Earth

Uplink

Downlink
Earth receiving
station
FIGURE 1.2 Satellite communication system.

Satellite (in geostationary orbit)

the video signal that is transmitted. Generation of the video signal is the result of a welldefined mapping process known to the receiver. Hence, given the video signal, the receiver is able to reconstruct the original image. As with digital radio, television is also the beneficiary of spectacular advances in digital electronics. These advances, coupled with the application of advanced digital signal processing techniques and the demands of consumers, have motivated the development of high-definition television (HDTV), which provides a significant improvement in the quality of reconstructed images at the receiver output.
We turn next to the point-to-point communication scene. The radio has also touched our daily lives in highly significant ways through two avenues: satellite communications and wireless communications. Satellite communications, built around a satellite in geostationary orbit, relies on line-of-sight radio propagation for the operation of an uplink and a downlink. The uplink connects an Earth terminal to a transponder (i.e., electronic circuitry) on board the satellite, while the downlink connects the transponder to another Earth terminal. Thus, an information-bearing signal is transmitted from the Earth terminal to the satellite via the uplink, amplified in the transponder, and then retransmitted from the satellite via the downlink to the other Earth terminal, as illustrated in Fig. 1.2. In so doing, a satellite communication system offers a unique capability: global coverage.
In a loose sense, wireless communications operates in a manner similar to satellite communications in that it also involves a downlink and an uplink. The downlink is responsible for forward-link radio transmission from a base station to its mobile users. The uplink is responsible for reverse-link radio transmission from the mobile users to their base stations. Unlike satellite communications, the operation of wireless communications is dominated by the multipath phenomenon due to reflections of the transmitted signal from objects (e.g., buildings, trees, etc.) that lie in the propagation path. This phenomenon tends to degrade the receiver performance, which makes the design of the receiver a challenging task. In any event, wireless communications offers a unique capability of its own: mobility. Moreover, through the use of the cellular concept, the wireless communication system is enabled to reuse the radio spectrum over a large area as many times as possible. Within a cell, the available communication resources can be shared by the mobile users operating within that cell.
᭿ COMMUNICATION NETWORKS
The computer was originally conceived as a machine working by itself to perform numerical calculations. However, given the natural ability of a computer to perform logical functions, it was soon recognized that the computer is ideally suited to the design of

1.2 Applications

7

Routers

Boundary of subnet

Hosts

FIGURE 1.3 Communication network.

communication networks. As illustrated in Fig. 1.3, a communication network consists of the interconnection of a number of routers that are made up of intelligent processors (e.g., microprocessors). The primary purpose of these processors is to route voice or data through the network, hence the name “routers.” Each router has one or more hosts attached to it; hosts refer to devices that communicate with one another. The purpose of a network is to provide for the delivery or exchange of voice, video, or data among its hosts, which is made possible through the use of digital switching. There are two principal forms of switching: circuit switching and packet switching.
In circuit switching, dedicated communication paths are established for the transmission of messages between two or more terminals, called stations. The communication path or circuit consists of a connected sequence of links from source to destination. For example, the links may consist of time slots (as in time-division multiplexed systems), for which a common channel is available for multiple users. The important point to note is that once it is in place, the circuit remains uninterrupted for the entire duration of transmission. Circuit switching is usually controlled by a centralized hierarchical control mechanism with knowledge of the network’s entire organization. To establish a circuit-switched connection, an available path through the telephone network is seized and then dedicated to the exclusive use of the two users wishing to communicate. In particular, a call-request signal propagates all the way to the destination, whereupon it is acknowledged before communication can begin. Then, the network is effectively transparent to the users, which means that during the entire connection time the resources allocated to the circuit are essentially “owned” by the two users. This state of affairs continues until the circuit is disconnected.
Circuit switching is well suited for telephone networks, where the transmission of voice constitutes the bulk of the network’s traffic. We say so because voice gives rise to a stream traffic, and voice conversations tend to be of long duration (about 2 minutes on the average) compared to the time required for setting up the circuit (about 0.1 to 0.5 seconds).
In packet switching,2 on the other hand, the sharing of network resources is done on a demand basis. Hence, packet switching has an advantage over circuit switching in that

2 Packet switching was invented by P. Baran in 1964 to satisfy a national defense need of the United States. The original need was to build a distributed network with different levels of redundant connections, which is robust in the sense that the network can withstand the destruction of many nodes due to a concerted attack, yet the surviving nodes are able to maintain intercommunication for carrying common and control information; see Baran (1990).

8

CHAPTER 1 ᭿ INTRODUCTION

when a link has traffic to send, the link tends to be more fully utilized. Unlike voice signals, data tend to occur in the form of bursts on an occasional basis.
The network principle of packet switching is store and forward. Specifically, in a packet-switched network, any message longer than a specified size is subdivided prior to transmission into segments not exceeding the specified size. The segments so formed are called packets. After transporting the packets across different parts of the network, the original message is reassembled at the destination on a packet-by-packet basis. The network may thus be viewed as a pool of network resources (i.e., channel bandwidth, buffers, and switching processors), with the resources being dynamically shared by a community of competing hosts that wish to communicate. This dynamic sharing of network resources is in direct contrast to the circuit-switched network, where the resources are dedicated to a pair of hosts for the entire period they are in communication.

᭿ DATA NETWORKS
A communication network in which the hosts are all made up of computers and terminals is commonly referred to as a data network. The design of such a network proceeds in an orderly way by looking at the network in terms of a layered architecture, which is regarded as a hierarchy of nested layers. A layer refers to a process or device inside a computer system that is designed to perform a specific function. Naturally, the designers of a layer will be familiar with its internal details and operation. At the system level, however, a user views the layer in question merely as a “black box,” which is described in terms of inputs, outputs, and the functional relation between the outputs and inputs. In the layered architecture, each layer regards the next lower layer as one or more black boxes with some given functional specification to be used by the given higher layer. In this way, the highly complex communication problem in data networks is resolved as a manageable set of welldefined interlocking functions. It is this line of reasoning that has led to the development of the open systems interconnection (OSI) reference model.3 The term “open” refers to the ability of any two systems to interconnect, provided they conform to the reference model and its associated standards.
In the OSI reference model, the communications and related-connection functions are organized as a series of layers with well-defined interfaces. Each layer is built on its predecessor. In particular, each layer performs a related subset of primitive functions, and it relies on the next lower layer to perform additional primitive functions. Moreover, each layer offers certain services to the next higher layer and shields that layer from the implementation details of those services. Between each pair of layers there is an interface, which defines the services offered by the lower layer to the upper layer.
As illustrated in Fig. 1.4, the OSI model is composed of seven layers. The figure also includes a description of the functions of the individual layers of the model. Layer k on system A, say, communicates with a layer R on some other system B in accordance with a set of rules and conventions, which collectively constitute layer k protocol, where k ϭ 1, 2, . . . , 7. (The term “protocol” has been borrowed from common usage that describes conventional social behavior between human beings.) The entities that comprise the corresponding layers on different systems are referred to as peer processes. In other words, communication between system A and system B is achieved by having the peer processes in the two systems communicate via protocol. Physical connection between peer processes

3 The OSI reference model was developed by a subcommittee of the International Organization for Standardization (ISO) in 1977. For a discussion of the principles involved in arriving at the original seven layers of the OSI model and a description of the layers themselves, see Tannenbaum (1996).

1.2 Applications

9

Layer End-user X

7

Application

Layer 7 protocol

End-user Y Application

Function
Provision of access to the OSI environment for end-users.

6

Presentation

5

Session

Layer 6 protocol Layer 5 protocol

Presentation Session

Transformation of the input data to provide services selected by the application layer; an example of data transformation is encryption to provide security.
Provision of the control structure for communication between two cooperating users, and the orderly management of the dialogue between them.

4

Transport

Layer 4 protocol

Layer 3

protocol

3

Network

Network

Layer 3 protocol

Layer 2

2

Data link control

protocol

DLC

DLC

Layer 2 protocol

1

Physical

Physical

Physical

Transport
Network Data link control Physical

End-to-end (i.e., source-to-destination) control of the messages exchanged between users.
Routing of packets through the network and flow control designed to guarantee good performance over a communication link found by the routing procedure.
Error control for the reliable transfer of information across the channel.
Transmission of raw bits of data over a physical channel; this layer deals with the mechanical, electrical, functional, and procedural requirements to access the channel.

Physical link

Physical link

System A

Subnet node

System B

FIGURE 1.4 OSI model; the acronym DLC in the middle of the figure stands for data link control.

exists only at layer 1—namely, the physical layer. The remaining layers, 2 through 7, are in virtual communication with their distant peers. Each of these latter six layers exchanges data and control information with its neighboring layers (lower and above) through layerto-layer interfaces. In Fig. 1.4, physical communication is shown by solid lines, and virtual communications are shown by dashed lines.

᭿ INTERNET4
The discussion of data networks just presented leads to the Internet. In the Internet paradigm, the underlying network technology is decoupled from the applications at hand by adopting an abstract definition of network service. In more specific terms, we may say the following:
᭤ The applications are carried out independently of the technology employed to construct the network.
᭤ By the same token, the network technology is capable of evolving without affecting the applications.

4 For a fascinating account of the Internet, its historical evolution from the ARPANET, and international standards, see Abbate (2000). For easy-to-read essays on the Internet, see Special Issue, IEEE Communications Magazine (2002); the articles presented therein are written by pioneering contributors to the development of the Internet.

10

CHAPTER 1 ᭿ INTRODUCTION

Subnet 2

Router

Router

Subnet 1

Subnet 3

...

...

Hosts

Hosts

FIGURE 1.5 An interconnected network of subnets.

The Internet application depicted in Fig. 1.5 has three functional blocks: hosts, subnets, and routers. The hosts constitute nodes of the network, where data originate or where they are delivered. The routers constitute intermediate nodes that are used to cross subnet boundaries. Within a subnet, all the hosts belonging to that subnet exchange data directly; see, for example, subnets 1 and 3 in Fig. 1.5. In basic terms, the internal operation of a subnet is organized in two different ways (Tanenbaum, 1996):
1. Connected manner, where the connections are called virtual circuits, in analogy with physical circuits set up in a telephone system.
2. Connectionless manner, where the independent packets are called datagrams, in analogy with telegrams.
Like other data networks, the Internet has a layered set of protocols. In particular, the exchange of data between the hosts and routers is accomplished by means of the Internet protocol (IP), as illustrated in Fig. 1.6. The IP is a universal protocol that resides in the network layer (i.e., layer 3 of the OSI reference model). It is simple, defining an addressing plan with a built-in capability to transport data in the form of packets from node to node. In crossing a subnetwork boundary, the routers make the decisions as to how the packets addressed for a specified destination should be routed. This is done on the basis of routing tables that are developed through the use of custom protocols for exchanging pertinent information with other routers. The net result of using the layered set of protocols is the provision of best effort service. That is, the Internet offers to deliver each packet of data,

AP TCP/UDP
IP

AP TCP/UDP
IP

AP TCP/UDP
IP

AP TCP/UDP
IP

Subnet 1

Subnet 1

Subnet 1

AP: Application protocol TCP: Transmission control protocol

UDP: User datagram protocol IP: Internet protocol

FIGURE 1.6 Illustrating the network architecture of the Internet.

1.2 Applications

11

but there are no guarantees on the transit time experienced in delivery or even whether the packets will be delivered to the intended recipient.
The Internet has evolved into a worldwide system, placing computers at the heart of a communication medium that is changing our daily lives in the home and workplace in profound ways. We can send an e-mail message from a host in North America to another host in Australia at the other end of the globe, with the message arriving at its destination in a matter of seconds. This is all the more remarkable because the packets constituting the message are quite likely to have taken entirely different paths as they are transported across the network.
Another application that demonstrates the remarkable power of the Internet is our use of it to surf the Web. For example, we may use a search engine to identify the references pertaining to a particular subject of interest. A task that used to take hours and sometimes days searching through books and journals in the library now occupies a matter of seconds!
To fully utilize the computing power of the Internet from a host located at a remote site, we need a wideband modem (i.e., modulator-demodulator) to provide a fast communication link between that host and its subnet. When we say “fast,” we mean operating speeds on the order of megabits per second and higher. A device that satisfies this requirement is the so-called digital subscriber line (DSL). What makes the DSL all the more remarkable is the fact that it can operate over a linear wideband channel with an arbitrary frequency response. Such a channel is exemplified by an ordinary telephone channel built using twisted pairs for signal transmission. A twisted pair consists of two solid copper conductors, each of which is encased in a polyvinyl chloride (PVC) sheath. Twisted pairs are usually made up into cables, with each cable consisting of many twisted pairs in close proximity to each other. From a signal-transmission viewpoint, the DSL satisfies the challenging requirement described herein by following the well-known engineering principle of divide and conquer. Specifically, the given wideband channel is approximated by a set of narrowband channels, each of which can then be accommodated in a relatively straightforward manner.
One last comment is in order. Typically, access to the Internet is established via hosts in the form of computer terminals (i.e., servers). The access is expanded by using hand-held devices that act as hosts, which communicate with subnets of the Internet via wireless links. Thus, by adding mobility through the use of wireless communications to the computing power of the Internet to communicate, we have a new communication medium with enormous practical possibilities.

᭿ INTEGRATION OF TELEPHONE AND INTERNET
One of the important challenges facing the telecommunications industry is the transmission of Voice over Internet Protocol (VoIP), which would make it possible to integrate telephony services with the rapidly growing Internet-based applications. The challenge is all the more profound because the IP is designed to accommodate the exchange of data between the hosts and the routers, which makes it difficult to support quality of service for VoIP. Quality of service (QoS) is measured in terms of two parameters:
᭤ Packet loss ratio, defined as the number of packets lost in transport across the network to the total number of packets pumped into the network.
᭤ Connection delay, defined as the time taken for a packet of a particular host-to-host connection to transmit across the network.
Subjective tests performed on VoIP show that in order to provide voice-grade telephone service, the packet loss ratio must be held below 1 percent, and one-way connection delay

12

CHAPTER 1 ᭿ INTRODUCTION

can accumulate up to 160 ms without significant degradation of quality. Well-designed and managed VoIP networks, satisfying these provisions, are being deployed. However, the issue of initial-echo control remains a challenge.5 Initial echo refers to the echo experienced at the beginning of a call on the first word or couple of words out of a user’s mouth. The echo arises due to an impedance mismatch somewhere in the network, whereupon the incident signal is reflected back to the source.
Looking into the future, we may make the following remarks on internet telephony:
1. VoIP will replace private branch exchanges (PBXs) and other office switches; PBXs are remote switching units that have their own independent controls.6
2. VoIP is also currently having success with longer distance calls, but this is mainly due to the excess capacity that is now available on long-haul networks. If the loading on these long-haul networks increases, the delays will increase and a real-time service such as VoIP will be degraded. Accordingly, if long-service providers keep adding capacity so that loading is always low and response time is fast, thereby ensuring quality of service, then VoIP telephony may become mainstream and widespread.

᭿ DATA STORAGE
When considering important applications of digital communication principles, it is natural to think in terms of broadcasting and point-to-point communication systems. Nevertheless, the very same principles are also applied to the digital storage of audio and video signals, exemplified by compact disc (CD) and digital versatile disc (DVD) players. DVDs are refinements of CDs in that their storage capacity (in the order of tens of gigabytes) are orders of magnitude higher than that of CDs, and they can also deliver data at a much higher rate.
The digital domain is preferred over the analog domain for the storage of audio and video signals for the following compelling reasons:
(i) The quality of a digitized audio/video signal, measured in terms of frequency response, linearity, and noise, is determined by the digital-to-analog conversion (DAC) process, the parameterization of which is under the designer’s control.
(ii) Once the audio/video signal is digitized, we can make use of well-developed and powerful encoding techniques for data compression to reduce bandwidth, and error-control coding to provide protection against the possibility of making errors in the course of storage.
(iii) For most practical applications, the digital storage of audio and video signals does not degrade with time.
(iv) Continued improvements in the fabrication of integrated circuits used to build CDs and DVDs ensure the ever-increasing cost-effectiveness of these digital storage devices.
With the help of the powerful encoding techniques built into their design, DVDs can hold hours of high-quality audio-visual contents, which, in turn, makes them ideally suited for interactive multimedia applications.

5 The limits on QoS measures mentioned herein are taken from the overview article by James, Chen, and Garrison (2004), which appears in a Special Issue of the IEEE Communications Magazine devoted to voice VoIP and quality of service. 6 PBXs are discussed in McDonald (1990).

1.3 Primary Resources and Operational Requirements

13

1.3 Primary Resources
and Operational Requirements

The communication systems described in Section 1.2 cover many diverse fields. Nevertheless, in their own individual ways, the systems are designed to provide for the efficient utilization of two primary communication resources:
᭤ Transmitted power, which is defined as the average power of the transmitted signal.
᭤ Channel bandwidth, which is defined by the width of the passband of the channel.
Depending on which of these two resources is considered to be the limiting factor, we may classify communication channels as follows:
(i) Power-limited channels, where transmitted power is at a premium. Examples of such channels include the following:
᭤ Wireless channels, where it is desirable to keep the transmitted power low so as to prolong battery life.
᭤ Satellite channels, where the available power on board the satellite transponder is limited, which, in turn, necessitates keeping the transmitted power on the downlink at a low level.
᭤ Deep-space links, where the available power on board a probe exploring outer space is extremely limited, which again requires that the average power of information-bearing signals sent by the probe to an Earth station be maintained as low as possible.
(ii) Band-limited channels, where channel bandwidth is at a premium. Examples of this second category of communication channels include the following:
᭤ Telephone channels, where, in a multi-user environment, the requirement is to minimize the frequency band allocated to the transmission of each voice signal while making sure that the quality of service for each user is maintained.
᭤ Television channels, where the available channel bandwidth is limited by regulatory agencies and the quality of reception is assured by using a high enough transmitted power.
Another important point to keep in mind is the unavoidable presence of noise at the receiver input of a communication system. In a generic sense, noise refers to unwanted signals that tend to disturb the quality of the received signal in a communication system. The sources of noise may be internal or external to the system. An example of internal noise is the ubiquitous channel noise produced by thermal agitation of electrons in the front-end amplifier of the receiver. Examples of external noise include atmospheric noise and interference due to transmitted signals pertaining to other users.
A quantitative way to account for the beneficial effect of the transmitted power in relation to the degrading effect of noise (i.e., assess the quality of the received signal) is to think in terms of the signal-to-noise ratio (SNR), which is a dimensionless parameter. In particular, the SNR at the receiver input is formally defined as the ratio of the average power of the received signal (i.e., channel output) to the average power of noise measured at the receiver input. The customary practice is to express the SNR in decibels (dBs), which is defined as 10 times the logarithm (to base 10) of the power ratio.7 For example, signal-tonoise ratios of 10, 100, and 1000 are 10, 20, and 30 dBs, respectively.

7 For a discussion of the decibel, see Appendix 1.

14

CHAPTER 1 ᭿ INTRODUCTION

In light of this discussion, it is now apparent that as far as performance evaluation is concerned, there are only two system-design parameters: signal-to-noise ratio and channel bandwidth. Stated in more concrete terms:
The design of a communication system boils down to a tradeoff between signal-tonoise ratio and channel bandwidth.

Thus, we may improve system performance by following one of two alternative design strategies, depending on system constraints:
1. Signal-to-noise ratio is increased to accommodate a limitation imposed on channel bandwidth.
2. Channel bandwidth is increased to accommodate a limitation imposed on signal-tonoise ratio.
Of these two possible design approaches, we ordinarily find that strategy 1 is simpler to implement than strategy 2, because increasing signal-to-noise ratio can be accomplished simply by raising the transmitted power. On the other hand, in order to exploit increased channel bandwidth, we need to increase the bandwidth of the transmitted signal, which, in turn, requires increasing the complexity of both the transmitter and receiver.

1.4 Underpinning Theories
of Communication Systems

The study of communication systems is challenging not only in technical terms but also in
theoretical terms. In this section, we highlight four theories, each of which is essential for understanding a specific aspect of communication systems.8

᭿ MODULATION THEORY
Modulation is a signal-processing operation that is basic to the transmission of an information-bearing signal over a communication channel, whether in the context of digital or analog communications. This operation is accomplished by changing some parameter of a carrier wave in accordance with the information-bearing (message) signal. The carrier wave may take one of two basic forms, depending on the application of interest:
᭤ Sinusoidal carrier wave, whose amplitude, phase, or frequency is the parameter chosen for modification by the information-bearing signal.
᭤ Periodic sequence of pulses, whose amplitude, width, or position is the parameter chosen for modification by the information-bearing signal.
Regardless of which particular approach is used to perform the modulation process, the issues in modulation theory that need to be addressed are:
᭤ Time-domain description of the modulated signal. ᭤ Frequency-domain description of the modulated signal. ᭤ Detection of the original information-bearing signal and evaluation of the effect of
noise on the receiver.
8 One other theory—namely, Information Theory—is basic to the study of communication systems. We have not included this theory here because of its highly mathematical and therefore advanced nature, which makes it inappropriate for an introductory book.

1.4 Underpinning Theories of Communication Systems

15

᭿ FOURIER ANALYSIS
The Fourier transform is a linear mathematical operation that transforms the time-domain description of a signal into a frequency-domain description without loss of information, which means that the original signal can be recovered exactly from the frequency-domain description. However, for the signal to be Fourier transformable, certain conditions have to be satisfied. Fortunately, these conditions are satisfied by the kind of signals encountered in the study of communication systems.
Fourier analysis provides the mathematical basis for evaluating the following issues:
᭤ Frequency-domain description of a modulated signal, including its transmission bandwidth.
᭤ Transmission of a signal through a linear system exemplified by a communication channel or (frequency-selective) filter.
᭤ Correlation (i.e., similarity) between a pair of signals.
These evaluations take on even greater importance by virtue of an algorithm known as the fast Fourier transform, which provides an efficient method for computing the Fourier transform.

᭿ DETECTION THEORY
Given a received signal, which is perturbed by additive channel noise, one of the tasks that the receiver has to tackle is how to detect the original information-bearing signal in a reliable manner. The signal-detection problem is complicated by two issues:
᭤ The presence of noise. ᭤ Factors such as the unknown phase-shift introduced into the carrier wave due to
transmission of the sinusoidally modulated signal over the channel.
Dealing with these issues in analog communications is radically different from dealing with them in digital communications. In analog communications, the usual approach focuses on output signal-to-noise ratio and related calculations. In digital communications, on the other hand, the signal-detection problem is viewed as one of hypothesis testing. For example, in the specific case of binary data transmission, given that binary symbol 1 is transmitted, what is the probability that the symbol is correctly detected, and how is that probability affected by a change in the received signal-to-noise ratio at the receiver input?
Thus, in dealing with detection theory, we address the following issues in analog communications:
᭤ The figure of merit for assessing the noise performance of a specific modulation strategy.
᭤ The threshold phenomenon that arises when the transmitted signal-to-noise ratio drops below a critical value.
᭤ Performance comparison of one modulation strategy against another.
In digital communications, on the other hand, we look at:
᭤ The average probability of symbol error at the receiver output.
᭤ The issue of dealing with uncontrollable factors.
᭤ Comparison of one digital modulation scheme against another.

16

CHAPTER 1 ᭿ INTRODUCTION

᭿ PROBABILITY THEORY AND RANDOM PROCESSES
From the brief discussion just presented on the role of detection theory in the study of communication systems, it is apparent that we need to develop a good understanding of the following:
᭤ Probability theory for describing the behavior of randomly occurring events in mathematical terms.
᭤ Statistical characterization of random signals and noise.
Unlike a deterministic signal, a random signal is a signal about which there is uncertainty before it occurs. Because of the uncertainty, a random signal may be viewed as belonging to an ensemble, or a group, of signals, with each signal in the ensemble having a different waveform from that of the others in the ensemble. Moreover, each signal within the ensemble has a certain probability of occurrence. The ensemble of signals is referred to as a random process or stochastic process. Examples of a random process include:
᭤ Electrical noise generated in the front-end amplifier of a radio or television receiver. ᭤ Speech signal produced by a male or female speaker. ᭤ Video signal transmitted by the antenna of a TV broadcasting station.
In dealing with probability theory, random signals, and noise, we address the following issues:
᭤ Basic concepts of probability theory and probabilistic models. ᭤ Statistical description of a random process in terms of ensemble as well as temporal
averages. ᭤ Mathematical analysis and processing of random signals.

1.5 Concluding Remarks
In this chapter, we have given a historical account and applications of communications and a brief survey of underlying theories of communication systems. In addition, we presented the following points to support our view that the study of this discipline is both highly challenging and truly exciting:
(i) Communication systems encompass many and highly diverse applications: radio, television, wireless communications, satellite communications, deep-space communications, telephony, data networks, Internet, and quite a few others.
(ii) Digital communication has established itself as the dominant form of communication. Much of the progress that we have witnessed in the advancement of digital communication systems can be traced to certain enabling theories and technologies, as summarized here: ᭤ Abstract mathematical ideas that are highly relevant to a deep understanding of the processing of information-bearing signals and their transmission over physical media. ᭤ Digital signal-processing algorithms for the efficient computation of spectra, correlation, and filtering of signals. ᭤ Software development and novel architectures for designing microprocessors. ᭤ Spectacular advances in the physics of solid-state devices and the fabrication of verylarge-scale integrated (VLSI) chips.

1.5 Concluding Remarks

17

(iii) The study of communication systems is a dynamic discipline, continually evolving by exploiting new technological innovations in other disciplines and responding to new societal needs.
(iv) Last but by no means least, communication systems touch our daily lives both at home and in the workplace, and our lives would be much poorer without the wide availability of communication devices that we take for granted.
The remainder of the book, encompassing ten chapters, provides an introductory treatment of both analog and digital kinds of communication systems. The book should prepare the reader for going on to deepen his or her knowledge of a discipline that is best described as almost limitless in scope. This is especially the case given the trend toward the unification of wireline and wireless networks to accommodate the integrated transmission of voice, video, and data.

CHAPTER 2
FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS
In mathematical terms, a signal is ordinarily described as a function of time, which is how we usually see the signal when its waveform is displayed on an oscilloscope. However, as pointed out in Chapter 1, from the perspective of a communication system it is important that we know the frequency content of the signal in question. The mathematical tool that relates the frequency-domain description of the signal to its time-domain description is the Fourier transform. There are in fact several versions of the Fourier transform available. In this chapter, we confine the discussion primarily to two specific versions:
᭤ The continuous Fourier transform, or the Fourier transform (FT) for short, which works with continuous functions in both the time and frequency domains.
᭤ The discrete Fourier transform, or DFT for short, which works with discrete data in both the time and frequency domains.
Much of the material presented in this chapter focuses on the Fourier transform, since the primary motivation of the chapter is to determine the frequency content of a continuous-time signal or to evaluate what happens to this frequency content when the signal is passed through a linear time-invariant (LTI) system. In contrast, the discrete Fourier transform, discussed toward the end of the chapter, comes into its own when the requirement is to evaluate the frequency content of the signal on a digital computer or to evaluate what happens to the signal when it is processed by a digital device as in digital communications.
The extensive material presented in this chapter teaches the following lessons:
᭤ Lesson 1: The Fourier transform of a signal specifies the complex amplitudes of the components that constitute the frequency-domain description or spectral content of the signal. The inverse Fourier transform uniquely recovers the signal, given its frequency-domain description.
᭤ Lesson 2: The Fourier transform is endowed with several important properties, which, individually and collectively, provide invaluable insight into the relationship between a signal defined in the time domain and its frequency domain description.
᭤ Lesson 3: A signal can only be strictly limited in the time domain or the frequency domain, but not both.
᭤ Lesson 4: Bandwidth is an important parameter in describing the spectral content of a signal and the frequency response of a linear time-invariant filter.
18

2.1 The Fourier Transform

19

᭤ Lesson 5: A widely used algorithm called the fast Fourier transform algorithm provides a powerful tool for computing the discrete Fourier transform; it is the mathematical tool for digital computations involving Fourier transformation.

2.1 The Fourier Transform1

᭿ DEFINITIONS

Let g1t2 denote a nonperiodic deterministic signal, expressed as some function of time t. By definition, the Fourier transform of the signal g1t2 is given by the integral

q
G1f 2 ϭ g1t2 exp1Ϫj2pft2 dt LϪq

(2.1)

where j ϭ 2Ϫ1, and the variable f denotes frequency; the exponential function exp1Ϫj2pft2 is referred to as the kernel of the formula defining the Fourier transform. Given the Fourier transform G1f2, the original signal g1t2 is recovered exactly using the for-

mula for the inverse Fourier transform:

q
g1t2 ϭ G1f2 exp1j2pft2 df LϪq

(2.2)

where the exponential exp1j2pft2 is the kernel of the formula defining the inverse Fourier transform. The two kernels of Eqs. (2.1) and (2.2) are therefore the complex conjugate of each other.
Note also that in Eqs. (2.1) and (2.2) we have used a lowercase letter to denote the time function and an uppercase letter to denote the corresponding frequency function. The functions g1t2 and G1f2 are said to constitute a Fourier-transform pair. In Appendix 2, we derive the definitions of the Fourier transform and its inverse, starting from the Fourier series of a periodic waveform.
We refer to Eq. (2.1) as the analysis equation. Given the time-domain behavior of a system, we are enabled to analyze the frequency-domain behavior of a system. The basic advantage of transforming the time-domain behavior into the frequency domain is that resolution into eternal sinusoids presents the behavior as the superposition of steady-state effects. For systems whose time-domain behavior is described by linear differential equations, the separate steady-state solutions are usually simple to understand in theoretical as well as experimental terms.
Conversely, we refer to Eq. (2.2) as the synthesis equation. Given the superposition of steady-state effects in the frequency-domain, we can reconstruct the original time-domain behavior of the system without any loss of information. The analysis and synthesis equations, working side by side as depicted in Fig. 2.1, enrich the representation of signals and

1Joseph Fourier studied the flow of heat in the early 19th century. Understanding heat flow was a problem of both practical and scientific significance at that time and required solving a partial-differential equation called the heat equation. Fourier developed a technique for solving partial-differential equations that was based on the assumption that the solution was a weighted sum of harmonically related sinusoids with unknown coefficients, which we now term the Fourier series. Fourier’s initial work on heat conduction was submitted as a paper to the Academy of Sciences of Paris in 1807 and rejected after review by Lagrange, Laplace, and Legendre. Fourier persisted in developing his ideas in spite of being criticized for a lack of rigor by his contemporaries. Eventually, in 1822, he published a book containing much of his work, Theorie analytique de la chaleur, which is now regarded as one of the classics of mathematics.

20

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Analysis equation:
͐∞
G(f ) = g(t) exp (–j 2␲ft)dt
–∞

Time-domain description:
g(t)

Frequency-domain description:
G(f )

Synthesis equation:
͐∞
g(t) = G(f ) exp (j 2␲ft)df
–∞

FIGURE 2.1 Sketch of the interplay between the synthesis and analysis equations embodied in Fourier transformation.

systems by making it possible to view the representation in two interactive domains: the time domain and the frequency domain.
For the Fourier transform of a signal g1t2 to exist, it is sufficient, but not necessary, that g1t2 satisfies three conditions known collectively as Dirichlet’s conditions:

1. The function g1t2 is single-valued, with a finite number of maxima and minima in any finite time interval.
2. The function g1t2 has a finite number of discontinuities in any finite time interval. 3. The function g1t2 is absolutely integrable—that is,
q
ƒg1t2ƒ dt Ͻ ϱ
LϪq

We may safely ignore the question of the existence of the Fourier transform of a time func-

tion g1t2 when it is an accurately specified description of a physically realizable signal (e.g.,

voice signal, video signal). In other words, physical realizability is a sufficient condition for

the existence of a Fourier transform. For physical realizability of a signal g1t2, the energy
q

of the signal defined by

ƒg1t2ƒ2 dt must satisfy the condition

LϪq

q

ƒg1t2ƒ2 dt Ͻ ϱ
LϪq

Such a signal is referred to as an energy-like signal or simply an energy signal. What we are therefore saying is that all energy signals are Fourier transformable.

᭿ NOTATIONS
The formulas for the Fourier transform and the inverse Fourier transform presented in Eqs. (2.1) and (2.2) are written in terms of two variables: time t measured in seconds (s) and frequency f measured in hertz (Hz). The frequency f is related to the angular frequency v as
v ϭ 2pf
which is measured in radians per second (rad/s). We may simplify the expressions for the exponents in the integrands of Eqs. (2.1) and (2.2) by using v instead of f. However, the use of f is preferred over v for two reasons. First, the use of frequency results in mathematical symmetry of Eqs. (2.1) and (2.2) with respect to each other in a natural way. Second, the spectral contents of communication signals (i.e., voice and video signals) are usually expressed in hertz.

2.1 The Fourier Transform

21

A convenient shorthand notation for the transform relations of Eqs. (2.1) and (2.2) is to write

G1f2 ϭ F3g1t24

(2.3)

and

g1t2 ϭ FϪ13G1f 24

(2.4)

where F34 and FϪ134 play the roles of linear operators. Another convenient shorthand notation for the Fourier-transform pair, represented by g1t2 and G1f2, is

g1t2 Δ G1f2

(2.5)

The shorthand notations described in Eqs. (2.3) through (2.5) are used in the text where appropriate.

᭿ CONTINUOUS SPECTRUM

By using the Fourier transform operation, a pulse signal g1t2 of finite energy is expressed as a continuous sum of exponential functions with frequencies in the interval Ϫϱ to ϱ. The amplitude of a component of frequency f is proportional to G1f2, where G1f2 is the Fourier transform of g1t2. Specifically, at any frequency f, the exponential function exp1j2pft2 is weighted by the factor G1f2 df, which is the contribution of G1f2 in an infinitesimal interval df centered on the frequency f. Thus we may express the function g1t2 in terms of the continuous sum of such infinitesimal components, as shown by the integral
q
g1t2 ϭ G1f2 exp1j2pft2 df LϪq

Restating what was mentioned previously, the Fourier transformation provides us with a tool to resolve a given signal g1t2 into its complex exponential components occupying the entire frequency interval from Ϫϱ to ϱ. In particular, the Fourier transform G1f2 of the signal defines the frequency-domain representation of the signal in that it specifies complex amplitudes of the various frequency components of the signal. We may equivalently define the signal in terms of its time-domain representation by specifying the function g1t2 at each instant of time t. The signal is uniquely defined by either representation.
In general, the Fourier transform G1f2 is a complex function of frequency f, so that we may express it in the form

G1f2 ϭ ƒG1f2ƒ exp3ju1f24

(2.6)

where ƒG1f2ƒ is called the continuous amplitude spectrum of g1t2, and u1f2 is called the con-
tinuous phase spectrum of g1t2. Here, the spectrum is referred to as a continuous spectrum because both the amplitude and phase of G1f2 are uniquely defined for all frequencies.
For the special case of a real-valued function g1t2, we have

G1Ϫf2 ϭ G*1f2

where the asterisk denotes complex conjugation. Therefore, it follows that if g1t2 is a realvalued function of time t, then

ƒG1Ϫf2ƒ ϭ ƒG1f2ƒ

and

u1Ϫf2 ϭ Ϫu1f2

22

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Accordingly, we may make the following statements on the spectrum of a real-valued signal:
1. The amplitude spectrum of the signal is an even function of the frequency; that is, the amplitude spectrum is symmetric with respect to the origin f ϭ 0.
2. The phase spectrum of the signal is an odd function of the frequency; that is, the phase spectrum is antisymmetric with respect to the origin f ϭ 0.
These two statements are summed up by saying that the spectrum of a real-valued signal exhibits conjugate symmetry.

EXAMPLE 2.1 Rectangular Pulse
Consider a box function or rectangular pulse of duration T and amplitude A, as shown in Fig. 2.2(a). To define this pulse mathematically in a convenient form, we use the notation

1, rect1t2 ϭ d
0,

Ϫ1 Յ t Յ 1

2

2

t Ͻ Ϫ 1 or t Ͼ 1

2

2

(2.7)

which stands for a rectangular function of unit amplitude and unit duration centered at t ϭ 0. Then, in terms of this “standard” function, we may express the rectangular pulse of Fig. 2.2(a) simply as

g1t2 ϭ A recta t b T

The Fourier transform of the rectangular pulse g1t2 is given by

T>2

G1f 2 ϭ

A exp1Ϫj2pft2 dt

LϪT>2

sin1pfT2

ϭ AT a

b

pfT

(2.8)

To simplify the notation in the preceding and subsequent results, we introduce another standard function—namely, the sinc function—defined by

sin1pl2 sinc1l2 ϭ
pl

(2.9)

|G(f )|

g (t)

AT

A

t

– —T2

0

—T2

(a)

f – —T4 – —T3 – —T2 – —T1 0 —T1 —T2 —T3 —T4
(b)

FIGURE 2.2 (a) Rectangular pulse. (b) Amplitude spectrum.

2.1 The Fourier Transform

23

sinc (␭) 1.0

0.5

␭ –2.5 –2.0 –1.5 –1.0 –0.5 0 0.5 1.0 1.5 2.0 2.5 3

–0.5

FIGURE 2.3 The sinc function.

where l is the independent variable. The sinc function plays an important role in communication theory. As shown in Fig. 2.3, it has its maximum value of unity at l ϭ 0, and approaches zero as l approaches infinity, oscillating through positive and negative values. It goes through zero at l ϭ Ϯ1, Ϯ2, Á , and so on.
Thus, in terms of the sinc function, we may rewrite Eq. (2.8) as

A recta t b Δ AT sinc1fT2 T

(2.10)

The amplitude spectrum ƒG1f 2ƒ is shown plotted in Fig. 2.2(b). The first zero-crossing of the
spectrum occurs at f ϭ Ϯ1>T. As the pulse duration T is decreased, this first zero-crossing moves up in frequency. Conversely, as the pulse duration T is increased, the first zero-crossing
moves toward the origin.

This example shows that the relationship between the time-domain and frequencydomain descriptions of a signal is an inverse one. That is, a pulse narrow in time has a significant frequency description over a wide range of frequencies, and vice versa. We shall have more to say on the inverse relationship between time and frequency in Section 2.3.
Note also that in this example, the Fourier transform G1f2 is a real-valued and symmetric function of frequency f. This is a direct consequence of the fact that the rectangular pulse g1t2 shown in Fig. 2.2(a) is a symmetric function of time t.

EXAMPLE 2.2 Exponential Pulse
A truncated decaying exponential pulse is shown in Fig. 2.4(a). We define this pulse mathematically in a convenient form using the unit step function:

1,

u1t2

ϭ

d

1 ,

2

0,

tϾ0 tϭ0 tϽ0

(2.11)

24

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

g (t)

g (t)

1.0

1.0

0.366

0.366

0

1/a

t

t

1/a

0

(a)

(b)

FIGURE 2.4 (a) Decaying exponential pulse. (b) Rising exponential pulse.

We may then express the decaying exponential pulse of Fig. 2.4(a) as

g1t2 ϭ exp1Ϫat2u1t2

Recognizing that g1t2 is zero for t Ͻ 0, the Fourier transform of this pulse is

q

G1f 2 ϭ exp1Ϫat2 exp1Ϫj2pft2 dt L0
q

ϭ exp3Ϫt1a ϩ j2pf 24 dt L0

ϭ

a

1 ϩ j2pf

The Fourier-transform pair for the decaying exponential pulse of Fig. 2.4(a) is therefore

exp1Ϫat2u1t2

Δ

1 a ϩ j2pf

(2.12)

A truncated rising exponential pulse is shown in Fig. 2.4(b), which is defined by

g1t2 ϭ exp1at2u1Ϫt2

Note that u1Ϫt2 is equal to unity for t Ͻ 0, one-half at t ϭ 0, and zero for t Ͼ 0. With g1t2 equal to zero for t Ͼ 0, the Fourier transform of this pulse is

0

G1f 2 ϭ

exp1at2 exp1Ϫj2pft2 dt

LϪq

Replacing t with Ϫt, we may next write

q

G1f 2 ϭ

exp3Ϫt1a Ϫ j2pf 24 dt

L0

ϭ

a

1 Ϫ j2pf

2.2 Properties of the Fourier Transform

25

|G(f )|

arg[G(f )]

1.0

f

–W

0

W

–π2–

W

f

–W

0

– –π2–

FIGURE 2.5 Frequency function G1f 2 for Problem 2.2.

The Fourier-transform pair for the rising exponential pulse of Fig. 2.4(b) is therefore

exp1Ϫat2u1Ϫt2

Δ

1 a Ϫ j2pf

(2.13)

The decaying and rising exponential pulses of Fig. 2.4 are both asymmetric functions of time t. Their Fourier transforms are therefore complex valued, as shown in Eqs. (2.12) and (2.13). Moreover, from these Fourier-transform pairs, we readily see that truncated decaying and rising exponential pulses have the same amplitude spectrum, but the phase spectrum of the one is the negative of the phase spectrum of the other.

᭤ Drill Problem 2.1 Evaluate the Fourier transform of the damped sinusoidal wave

g1t2 ϭ exp1Ϫt2 sin12pfc t2u1t2, where u1t2 is the unit step function.

᭣

᭤ Drill Problem 2.2 Determine the inverse Fourier transform of the frequency function

G1f 2 defined by the amplitude and phase spectra shown in Fig. 2.5.

᭣

2.2 Properties of the Fourier Transform

It is useful to have insight into the relationship between a time function g1t2 and its Fourier transform G1f2, and also into the effects that various operations on the function g1t2 have on the transform G1f2. This may be achieved by examining certain properties of the Fourier transform. In this section, we describe fourteen properties, which we will prove, one by one. These properties are summarized in Table A8.1 of Appendix 8 at the end of the book.

PROPERTY 1 Linearity (Superposition) Let g11t2 Δ G11f 2 and g21t2 Δ G21f 2. Then for all constants c1 and c2 , we have

c1g11t2 ϩ c2g21t2 Δ c1G11f 2 ϩ c2G21f 2

(2.14)

The proof of this property follows simply from the linearity of the integrals defining G1f2 and g1t2.
Property 1 permits us to find the Fourier transform G1f2 of a function g1t2 that is a linear combination of two other functions g11t2 and g21t2 whose Fourier transforms G11f 2 and G21f 2 are known, as illustrated in the following example.

26

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

EXAMPLE 2.3 Combinations of Exponential Pulses Consider a double exponential pulse (defined by (see Fig. 2.6(a))

exp1Ϫat2, g1t2 ϭ c 1,
exp1at2,
ϭ exp1Ϫaƒtƒ2

tϾ0 tϭ0 tϽ0

(2.15)

This pulse may be viewed as the sum of a truncated decaying exponential pulse and a truncated

rising exponential pulse. Therefore, using the linearity property and the Fourier-transform pairs of Eqs. (2.12) and (2.13), we find that the Fourier transform of the double exponential

pulse of Fig. 2.6(a) is

G1f 2

ϭ

a

1 ϩ j2pf

ϩ

a

1 Ϫ j2pf

ϭ

a2

2a ϩ 12pf 22

We thus have the following Fourier-transform pair for the double exponential pulse of Fig. 2.6(a):

exp1Ϫa ƒ t ƒ 2

Δ

a2

2a ϩ 12pf 22

(2.16)

g(t)

1.0

0.366

t

– —1a

(a)

—1a

g(t)

1.0

t 0
–1.0
(b) FIGURE 2.6 (a) Double-exponential pulse (symmetric). (b) Another double-exponential pulse (odd-symmetric).

2.2 Properties of the Fourier Transform

27

sgn(t) 1.0

t 0

–1.0

FIGURE 2.7 Signum function.

Note that because of the symmetry in the time domain, as in Fig. 2.6(a), the spectrum is real and symmetric; this is a general property of such Fourier-transform pairs.
Another interesting combination is the difference between a truncated decaying exponential pulse and a truncated rising exponential pulse, as shown in Fig. 2.6(b). Here we have

exp1Ϫat2, g1t2 ϭ c 0,
Ϫexp1at2,

tϾ0 tϭ0 tϽ0

(2.17)

We may formulate a compact notation for this composite signal by using the signum function that equals ϩ1 for positive time and Ϫ1 for negative time, as shown by

ϩ1, sgn1t2 ϭ c 0,
Ϫ1,

tϾ0 tϭ0 tϽ0

(2.18)

The signum function is shown in Fig. 2.7. Accordingly, we may reformulate the composite signal g1t2 defined in Eq. (2.17) simply as

g1t2 ϭ exp1Ϫaƒtƒ2 sgn1t2

Hence, applying the linearity property of the Fourier transform, we readily find that in light of Eqs. (2.12) and (2.13), the Fourier transform of the signal g1t2 is given by

F3exp1Ϫa ƒ t ƒ 2

sgn1t24

ϭ

a

1 ϩ j2pf

Ϫ

a

1 Ϫ j2pf

Ϫj4pf ϭ a2 ϩ 12pf22

We thus have the Fourier-transform pair Ϫj4pf
exp1Ϫa ƒ t ƒ 2 sgn1t2 Δ a2 ϩ 12pf22

(2.19)

In contrast to the Fourier-transform pair of Eq. (2.16), the Fourier transform in Eq. (2.19) is odd and purely imaginary. It is a general property of Fourier-transform pairs that apply to an odd-symmetric time function, which satisfies the condition g1Ϫt2 ϭ Ϫg1t2, as in Fig. 2.6(b); such a time function has an odd and purely imaginary function as its Fourier transform.

28

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

PROPERTY 2 Dilation property states that

Let g1t2 Δ G1f2. Then, the dilation property or similarity

g1at2

Δ

ƒ

1
aƒ

G

a

f a

b

(2.20)

where the dilation factor—namely, a—is a real number.

To prove this property, we note that
q
F3g1at24 ϭ g1at2 exp1Ϫj2pft2 dt LϪq

Set t ϭ at. There are two cases that can arise, depending on whether the dilation factor a is positive or negative. If a Ͼ 0, we get

F3g1at24 ϭ 1

q

f

g1t2 expcϪj2pa bt d dt

a LϪq

a

ϭ

1 a

G

a

f a

b

On the other hand, if a Ͻ 0, the limits of integration are interchanged so that we have the
multiplying factor Ϫ11>a2 or, equivalently, 1> ƒaƒ. This completes the proof of Eq. (2.20).
Note that the dilation factors a and 1>a used in the time and frequency functions in Eq. (2.20) are reciprocals. In particular, the function g1at2 represents g1t2 compressed in time by the factor a, whereas the function G1f>a2 represents G1f2 expanded in frequency by the same factor a, assuming that 0 Ͻ a Ͻ 1. Thus, the dilation rule states that the compression of a function g1t2 in the time domain is equivalent to the expansion of its Fourier transform G1f2 in the frequency domain by the same factor, or vice versa.
For the special case when a ϭ Ϫ1, the dilation rule of Eq. (2.20) reduces to the reflection property, which states that if g1t2 Δ G1f2, then

g1Ϫt2 Δ G1Ϫf2

(2.21)

Referring to Fig. 2.4, we see that the rising exponential pulse shown in part (b) of the figure is the reflection of the decaying exponential pulse shown in part (a) with respect to the vertical axis. Hence, applying the reflection rule to Eq. (2.12) that pertains to the decaying exponential pulse, we readily see that the Fourier transform of the rising exponential pulse is 1>1a Ϫ j2pf2, which is exactly what we have in Eq. (2.13).

PROPERTY 3 Conjugation Rule Let g1t2 Δ G1f 2. Then for a complex-valued time function g1t2, we have

g*1t2 Δ G*1Ϫf2

(2.22)

where the asterisk denotes the complex-conjugate operation.

To prove this property, we know from the inverse Fourier transform that
q
g1t2 ϭ G1f2 exp1j2pft2 df LϪq

2.2 Properties of the Fourier Transform

29

Taking the complex conjugates of both sides yields
q
g*1t2 ϭ G*1f2 exp1Ϫj2pft2 df LϪq

Next, replacing f with Ϫf gives

Ϫq

g*1t2 ϭ Ϫ

G*1Ϫf2 exp1j2pft2 df

Lq

q

ϭ G*1Ϫf2 exp1j2pft2 df LϪq

That is, g*1t2 is the inverse Fourier transform of G*1Ϫf2, which is the desired result. As a corollary to the conjugation rule of Eq. (2.22), we may state that if
g1t2 Δ G1f2, then

g*1Ϫt2 Δ G*1f2

(2.23)

This result follows directly from Eq. (2.22) by applying the reflection rule described in Eq. (2.21).

PROPERTY 4 Duality If g1t2 Δ G1f 2, then G1t2 Δ g1Ϫf2

(2.24)

This property follows from the relation defining the inverse Fourier transform of Eq. (2.21) by first replacing t with Ϫt, thereby writing it in the form
q
g1Ϫt2 ϭ G1f2 exp1Ϫj2pft2 df LϪq
Finally, interchanging t and f (i.e., replacing t with f in the left-hand side of the equation and f with t in the right-hand side), we get
q
g1Ϫf2 ϭ G1t2 exp1Ϫj2pft2 dt LϪq
which is the expanded part of Eq. (2.24) in going from the time domain to the frequency domain.

EXAMPLE 2.4 Sinc Pulse Consider a signal g1t2 in the form of a sinc function, as shown by
g1t2 ϭ A sinc12Wt2

To evaluate the Fourier transform of this function, we apply the duality and dilation properties to the Fourier-transform pair of Eq. (2.10). Then, recognizing that the rectangular function is an even function of time, we obtain the result

A sinc12Wt2 Δ A recta f b

2W

2W

(2.25)

which is illustrated in Fig. 2.8. We thus see that the Fourier transform of a sinc pulse is zero
for ƒf ƒ Ͼ W. Note also that the sinc pulse itself is only asymptotically limited in time in the sense
that it approaches zero as time t approaches infinity; it is this asymptotic characteristic that
makes the sinc function into an energy signal and therefore Fourier transformable.

30

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

g(t) A

G(f ) — 2AW

– — 2W3 – — W1

– — 2W1 0 — 2W1

t — W1 — 2W3

f

–W

0

W

(a)

(b)

FIGURE 2.8 (a) Sinc pulse g1t2. (b) Fourier transform G1f 2.

PROPERTY 5 Time Shifting If g1t2 Δ G1f 2, then g1t Ϫ t02 Δ G1f 2 exp1Ϫj2pft02
where t0 is a real constant time shift.

(2.26)

To prove this property, we take the Fourier transform of g1t Ϫ t02 and then set t ϭ 1t Ϫ t02 or, equivalently, t ϭ t ϩ t0 . We thus obtain
q
F3g1t Ϫ t024 ϭ exp1Ϫj2pft02 LϪq g1t2 exp1Ϫj2pt2 dt ϭ exp1Ϫj2pft02G1f 2
The time-shifting property states that if a function g1t2 is shifted along the time axis by an amount t0 , the effect is equivalent to multiplying its Fourier transform G1f 2 by the factor exp1Ϫj2pft02. This means that the amplitude of G1f 2 is unaffected by the time shift, but its phase is changed by the linear factor Ϫ2pft0 , which varies linearly with frequency f.

PROPERTY 6 Frequency Shifting If g1t2 Δ G1f 2, then exp1j2pfct2g1t2 Δ G1f Ϫ fc2
where fc is a real constant frequency.

(2.27)

This property follows from the fact that

q

F3exp1j2pfc t2g1t24

ϭ

g1t2 LϪq

exp3Ϫj2pt1f

Ϫ

fc24

dt

ϭ G1f Ϫ fc2

That is, multiplication of a function g1t2 by the factor exp1j2pfct2 is equivalent to shifting its Fourier transform G1f 2 along the frequency axis by the amount fc . This property is a special case of the modulation theorem discussed later under Property 11; basically, a shift
of the range of frequencies in a signal is accomplished by using the process of modulation.
Note the duality between the time-shifting and frequency-shifting operations described in
Eqs. (2.26) and (2.27).

2.2 Properties of the Fourier Transform

31

EXAMPLE 2.5 Radio Frequency (RF) Pulse
Consider the pulse signal g1t2 shown in Fig. 2.9(a), which consists of a sinusoidal wave of unit amplitude and frequency fc , extending in duration from t ϭ ϪT>2 to t ϭ T>2. This signal is sometimes referred to as an RF pulse when the frequency fc falls in the radio-frequency band. The signal g1t2 of Fig. 2.9(a) may be expressed mathematically as follows:

g1t2

ϭ

rect

a

t T

b

cos12pfc t2

(2.28)

To find the Fourier transform of the RF signal, we first use Euler’s formula to write

cos12pfc t2

ϭ

1 2

3exp1j2pfc t2

ϩ

exp1Ϫj2pfc t24

Therefore, applying the frequency-shifting property to the Fourier-transform pair of Eq. (2.10), and then invoking the linearity property of the Fourier transform, we get the desired result

recta t b T

cos12pfc t2

Δ

T 5sinc3T1f 2

Ϫ

fc24

ϩ

sinc3T1f

ϩ

fc246

(2.29)

g(t)

+1

—f1c

t

–1 T (a) |G(f )|
—T2

–fc

0

—T2 (b)

f fc
—T2

FIGURE 2.9 (a) RF pulse of unit amplitude and duration T. (b) Amplitude spectrum.

32

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

In the special case of fcT W 1—that is, the frequency fc is large compared to the reciprocal of the pulse duration T—we may use the approximate result

T 2

sinc3T1f

Ϫ

fc24,

G1f 2 ഠ e 0,

T 2

sinc3T1f

ϩ

fc24,

fϾ0 fϭ0 fϽ0

(2.30)

Under the condition fcT W 1, the amplitude spectrum of the RF pulse is shown in Fig. 2.9(b). This diagram, in relation to Fig. 2.2(b), clearly illustrates the frequency-shifting property of the
Fourier transform.

PROPERTY 7 Area Under g1t2 If g1t2 Δ G1f 2, then

q
g1t2 dt ϭ G102 LϪq

(2.31)

That is, the area under a function g1t2 is equal to the value of its Fourier transform G1f2 at f ϭ 0.

This result is obtained simply by putting f ϭ 0 in Eq. (2.1) defining the Fourier transform of the function g1t2.

᭤ Drill Problem 2.3 Suppose g1t2 is real valued with a complex-valued Fourier transform

G1f 2. Explain how the rule of Eq. (2.31) can be satisfied by such a signal.

᭣

PROPERTY 8 Area Under G1f 2 If g1t2 Δ G1f 2, then
q
g102 ϭ G1f2 df LϪq

(2.32)

That is, the value of a function g1t2 at t ϭ 0 is equal to the area under its Fourier transform G1f2.

The result is obtained simply by putting t ϭ 0 in Eq. (2.2) defining the inverse Fourier transform of G1f2.

᭤ Drill Problem 2.4 Continuing with Problem 2.3, explain how the rule of Eq. (2.32) can

be satisfied by the signal g1t2 described therein.

᭣

PROPERTY 9 Differentiation in the Time Domain Let g1t2 Δ G1f 2 and assume that the first derivative of g1t2 with respect to time t is Fourier transformable. Then

d g1t2 Δ j2pfG1f2 dt

(2.33)

That is, differentiation of a time function g1t2 has the effect of multiplying its Fourier transform G1f2 by the purely imaginary factor j2pf.

This result is obtained simply in two steps. In step 1, we take the first derivative of both sides of the integral in Eq. (2.2) defining the inverse Fourier transform of G1f2. In step 2, we interchange the operations of integration and differentiation.

2.2 Properties of the Fourier Transform

33

We may generalize Eq. (2.33) for higher order derivatives of the time function g1t2 as follows:

dn dt n

g1t2

Δ

1j2pf2nG1f 2

(2.34)

which includes Eq. (2.33) as a special case. Equation (2.34) assumes that the Fourier transform of the higher order derivative of g1t2 exists.

EXAMPLE 2.6 Unit Gaussian Pulse

Typically, a pulse signal g1t2 and its Fourier transform G1f 2 have different mathematical forms.

This observation is illustrated by the Fourier-transform pairs studied in Examples 2.1 through 2.5.

In this example, we consider an exception to this observation. In particular, we use the differen-

tiation property of the Fourier transform to derive the particular form of a pulse signal that has

the same mathematical form as its own Fourier transform.

Let g1t2 denote the pulse signal expressed as a function of time t, and G1f 2 denote its

Fourier transform. Differentiating the Fourier transform formula of Eq. (2.1) with respect to

frequency f, we may write

Ϫj2ptg1t2

Δ

d df

G1f

2

or, equivalently,

2ptg1t2 Δ j d G1f 2 df

(2.35)

Suppose we now impose the following condition on the left-hand sides of Eqs. (2.33) and (2.35):

d g1t2 ϭ Ϫ2ptg1t2 dt

(2.36)

Then in a corresponding way, it follows that the right-hand sides of these two equations must (after cancelling the common multiplying factor j) satisfy the condition

d G1f 2 ϭ Ϫ2pfG1f 2 df

(2.37)

Equations (2.36) and (2.37) show that the pulse signal g1t2 and its Fourier transform G1f 2 have exactly the same mathematical form. In other words, provided that the pulse signal g1t2 satisfies the differential equation (2.36), then G1f 2 ϭ g1f 2, where g1f 2 is obtained from g1t2 by substituting f for t. Solving Eq. (2.36) for g1t2, we obtain

g1t2 ϭ exp1Ϫpt22

(2.38)

The pulse defined by Eq. (2.38) is called a Gaussian pulse, the name being derived from the similarity of the function to the Gaussian probability density function of probability theory (see Chapter 8). It is shown plotted in Fig. 2.10. By applying Eq. (2.31), we find that the area under

g(t) 1.0

0.5 –0.47 0 0.47

t

FIGURE 2.10

Gaussian pulse.

34

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

this Gaussian pulse is unity, as shown by
q
exp1Ϫpt22 dt ϭ 1 LϪq

(2.39)

When the central ordinate and the area under the curve of a pulse are both unity, as in Eqs. (2.38) and (2.39), we say that the Gaussian pulse is a unit pulse. We conclude therefore that the unit Gaussian pulse is its own Fourier transform, as shown by

exp1Ϫpt22 Δ exp1Ϫpf 22

(2.40)

PROPERTY 10 Integration in the Time Domain Let g1t2 Δ G1f 2. Then provided that G102 ϭ 0, we have

t
g1t2 dt Δ

1 G1f2

LϪq

j2pf

(2.41)

That is, integration of a time function g1t2 has the effect of dividing its Fourier transform G1f2 by the factor j2pf, provided that G102 is zero.

This property is verified by expressing g1t2 as

g1t2 ϭ d c

t
g1t2 dt d

dt LϪq

and then applying the time-differentiation property of the Fourier transform to obtain
t
G1f2 ϭ 1j2pf2e Fc g1t2 dt d f LϪq

from which Eq. (2.41) follows immediately. It is a straightforward matter to generalize Eq. (2.41) to multiple integration; how-
ever, the notation becomes rather cumbersome. Equation (2.41) assumes that G102—that is, the area under g1t2—is zero. The more
general case pertaining to G102 0 is deferred to Section 2.4.

EXAMPLE 2.7 Triangular Pulse
Consider the doublet pulse g11t2 shown in Fig. 2.11(a). By integrating this pulse with respect to time, we obtain the triangular pulse g21t2 shown in Fig. 2.11(b). We note that the doublet pulse g11t2 consists of two rectangular pulses: one of amplitude A, defined for the interval ϪT Յ t Յ 0; and the other of amplitude ϪA, defined for the interval 0 Յ t Յ T. Applying
the time-shifting property of the Fourier transform to Eq. (2.10), we find that the Fourier transforms of these two rectangular pulses are equal to AT sinc1f T2 exp1jpf T2 and ϪAT sinc1f T2 exp1Ϫjpf T2, respectively. Hence, invoking the linearity property of the Fourier transform, we find that the Fourier transform G11f2 of the doublet pulse g11t2 of Fig. 2.11(a) is given by

G11f 2 ϭ AT sinc1f T23exp1jpf T2 Ϫ exp1Ϫjpf T24 ϭ 2jAT sinc1f T2 sin1pf T2

(2.42)

We further note from Eq. (2.42) that G1102 is zero. Hence, using Eqs. (2.41) and (2.42), we find that the Fourier transform G21f 2 of the triangular pulse g21t2 of Fig. 2.11(b) is given by

2.2 Properties of the Fourier Transform

35

g1(t) A

t

–T

0

T

–A (a)
g2(t) AT

–T

0

T

(b)

t FIGURE 2.11 (a) Doublet pulse g11t2. (b) Triangular pulse g21t2 obtained by integrating g11t2 with respect to time t.

G21f 2

ϭ

1 j2pf

G11f2

sin1pf T2 ϭ AT pf sinc1f T2

ϭ AT2 sinc21f T2

(2.43)

Note that the doublet pulse of Fig. 2.11(a) is real and odd-symmetric and its Fourier transform is therefore odd and purely imaginary, whereas the triangular pulse of Fig. 2.11(b) is real and symmetric and its Fourier transform is therefore symmetric and purely real.

EXAMPLE 2.8 Real and Imaginary Parts of a Time Function
Thus far in the chapter, we have discussed the Fourier representation of various signals, some being purely real, others being purely imaginary, yet others being complex valued with real and imaginary parts. It is therefore apropos that at this stage in the Fourier analysis of signals, we use this example to develop a number of general formulas pertaining to complex signals and their spectra.
Expressing a complex-valued function g1t2 in terms of its real and imaginary parts, we may write

g1t2 ϭ Re3g1t24 ϩ j Im3g1t24

(2.44)

where Re denotes “the real part of” and Im denotes the “imaginary part of.” The complex conjugate of g1t2 is defined by

g*1t2 ϭ Re3g1t24 Ϫ j Im3g1t24

(2.45)

36

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Adding Eqs. (2.44) and (2.45) gives

Re3g1t24 ϭ 1 3g1t2 ϩ g*1t24 2

(2.46)

and subtracting them yields

Im3g1t24 ϭ 1 3g1t2 Ϫ g*1t24 2

(2.47)

Therefore, applying the conjugation rule of Eq. (2.22), we obtain the following two Fouriertransform pairs:

Re3g1t24 Δ 1 3G1f 2 ϩ G*1Ϫf 24

2

t

Im3g1t2 Δ 1 3G1f 2 Ϫ G*1Ϫf 24

2

(2.48)

From the second line of Eq. (2.48), it is apparent that in the case of a real-valued time function g1t2, we have G1f 2 ϭ G*1Ϫf 2; that is, the Fourier transform G1f 2 exhibits conjugate symmetry, confirming a result that we stated previously in Section 2.2.

PROPERTY 11 Modulation Theorem Let g11t2 Δ G11f 2 and g21t2 Δ G21f 2. Then

q
g11t2g21t2 Δ LϪq G11l2G21f Ϫ l2 dl

(2.49)

To prove this property, we first denote the Fourier transform of the product g11t2g21t2 by G121f 2, so that we may write
g11t2g21t2 Δ G121f 2
where
q
G121f 2 ϭ LϪq g11t2g21t2 exp1Ϫj2pft2 dt

For g21t2, we next substitute the inverse Fourier transform
q
g21t2 ϭ LϪq G21f Ј2 exp1j2pf Јt2 df Ј

in the integral defining G121f 2 to obtain
qq
G121f 2 ϭ LϪq LϪq g11t2G21f Ј2 exp3Ϫj2p1f Ϫ f Ј2t4 df Јdt

Define l ϭ f Ϫ fЈ. Then, eliminating the variable fЈ and interchanging the order of integration, we obtain (after rearranging terms)

q

q

G121f 2 ϭ LϪq G21f Ϫ l2 c LϪq g11t2 exp1Ϫj2plt2 dt d dl

assuming that f is fixed. The inner integral (inside the square brackets) is recognized simply as G11l2; we may therefore write

2.2 Properties of the Fourier Transform

37

q
G121f 2 ϭ LϪq G11l2G21f Ϫ l2 dl

which is the desired result. This integral is known as the convolution integral expressed in the frequency domain, and the function G121f 2 is referred to as the convolution of G11f 2 and G21f 2. We conclude that the multiplication of two signals in the time domain is transformed into the convolution of their individual Fourier transforms in the frequency domain.
This property is also known as the modulation theorem. We have more to say on the prac-
tical implications of this property in subsequent chapters.
In a discussion of convolution, the following shorthand notation is frequently used:

G121f 2 ϭ G11f 2 ଙ G21f 2 Accordingly, we may reformulate Eq. (2.49) in the following symbolic form:

g11t2g21t2 Δ G11f 2 ଙ G21f 2

(2.50)

where the symbol ଙ denotes convolution. Note that convolution is commutative; that is,

G11f 2 ଙ G21f 2 ϭ G21f 2 ଙ G11f 2

which follows directly from Eq. (2.50).

PROPERTY 12 Convolution Theorem Let g11t2 Δ G11f 2 and g21t2 Δ G21f 2. Then

q
LϪq g11t2g21t Ϫ t2 dt Δ G11f 2G21f 2

(2.51)

Equation (2.51) follows directly by combining Property 4 (duality) and Property 11 (modulation). We may thus state that the convolution of two signals in the time domain is transformed into the multiplication of their individual Fourier transforms in the frequency domain. This property is known as the convolution theorem. Its use permits us to exchange a convolution operation in the time domain for a multiplication of two Fourier transforms, an operation that is ordinarily easier to manipulate. We have more to say on convolution later in the chapter when the issue of filtering is discussed.
Using the shorthand notation for convolution, we may rewrite Eq. (2.51) in the simple form

g11t2 ଙ g21t2 Δ G11f 2G21f 2

(2.52)

Note that Properties 11 and 12, described by Eqs. (2.49) and (2.51), respectively, are the dual of each other.

᭤ Drill Problem 2.5 Develop the detailed steps that show that the modulation and con-

volution theorems are indeed the dual of each other.

᭣

PROPERTY 13 Correlation Theorem Let g11t2 Δ G11f 2 and g21t2 Δ G21f 2. Then, assuming that g11t2 and g21t2 are complex valued,

q
LϪq g11t2g2…1t Ϫ t2 dt Δ G11f 2G2…1f 2

(2.53)

where G2…1f 2 is the complex conjugate of G21f 2, and t is the time variable involved in defining the inverse Fourier transform of the product G11f 2G2…1f 2.

38

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

To prove Eq. (2.53), we begin by reformulating the convolution integral with the roles of the time variables t and t interchanged, in which case we may simply rewrite Eq. (2.51) as

q
LϪq g11t2g21t Ϫ t2 dt Δ G11f 2G21f 2

(2.54)

As already pointed out in the statement of Property 13, the inverse Fourier transform of the product term G11f 2G21f 2 has t as its time variable; that is, exp1j2pft2 is its kernel. With the formula of Eq. (2.54) at hand, Eq. (2.53) follows directly by combining reflection rule (special case of the dilation property) and conjugation rule.
The integral on the left-hand side of Eq. (2.53) defines a measure of the similarity that may exist between a pair of complex-valued signals. This measure is called correlation, on which we have more to say later in the chapter.

᭤ Drill Problem 2.6 Develop the detailed steps involved in deriving Eq. (2.53), starting

from Eq. (2.51).

᭣

᭤ Drill Problem 2.7 Prove the following properties of the convolution process:
(a) The commutative property: g11t2 ଙ g21t2 ϭ g21t2 ଙ g11t2 (b) The associative property: g11t2 ଙ 3g21t2 ଙ g31t24 ϭ 3g11t2 ଙ g21t24 ଙ g31t2 (c) The distributive property: g11t2 ଙ 3g21t2 ϩ g31t24 ϭ g11t2 ଙ g21t2 ϩ g11t2 ଙ g31t2
᭣

PROPERTY 14 Rayleigh’s Energy Theorem Let g1t2 Δ G1f 2. Then

q

q

ƒg1t2ƒ2 dt ϭ

ƒG1f2ƒ2 df

LϪq

LϪq

(2.55)

To prove Eq. (2.55), we set g11t2 ϭ g21t2 ϭ g1t2 in Eq. (2.53), in which case the correlation theorem reduces to
q
g1t2g*1t Ϫ t2 dt Δ G1f2G*1f2 ϭ ƒG1f2ƒ2
LϪq

In expanded form, we may write

q

q

g1t2g*1t Ϫ t2 dt ϭ

ƒG1f2ƒ2 exp1j2pft2 df

LϪq

LϪq

(2.56)

Finally, putting t ϭ 0 in Eq. (2.56) and recognizing that g1t2g*1t2 ϭ ƒg1t2ƒ2, we get the
desired result. Equation (2.55), known as Rayleigh’s energy theorem, states that the total energy of
a Fourier-transformable signal equals the total area under the curve of squared amplitude spectrum of this signal. Determination of the energy is often simplified by invoking the Rayleigh energy theorem, as illustrated in the following example.

EXAMPLE 2.9 Sinc Pulse (continued)
Consider again the sinc pulse A sinc 12Wt2. The energy of this pulse equals
q
E ϭ A2 sinc212Wt2 dt LϪq

2.3 The Inverse Relationship Between Time and Frequency

39

The integral in the right-hand side of this equation is rather difficult to evaluate. However, we note from Example 2.4 that the Fourier transform of the sinc pulse A sinc12Wt2 is equal to 1A>2W2 rect1f>2W2; hence, applying Rayleigh’s energy theorem to the problem at hand, we readily obtain the desired result:

Eϭ

a

A

2
b

q
rect2 a

f

b df

2W LϪq

2W

ϭ

a

A

2
b

W
df

2W LϪW

ϭ A2 2W

(2.57)

This example clearly illustrates the usefulness of Rayleigh’s energy theorem.

᭤ Drill Problem 2.8 Considering the pulse function sinc1t2, show that

q

sinc21t2 dt ϭ 1.

LϪq

᭣

2.3 The Inverse Relationship
Between Time and Frequency
The properties of the Fourier transform discussed in Section 2.2 show that the time-domain and frequency-domain descriptions of a signal are inversely related to each other. In particular, we may make two important statements:
1. If the time-domain description of a signal is changed, the frequency-domain description of the signal is changed in an inverse manner, and vice versa. This inverse relationship prevents arbitrary specifications of a signal in both domains. In other words, we may specify an arbitrary function of time or an arbitrary spectrum, but we cannot specify both of them together.
2. If a signal is strictly limited in frequency, the time-domain description of the signal will trail on indefinitely, even though its amplitude may assume a progressively smaller value. We say a signal is strictly limited in frequency or strictly band limited if its Fourier transform is exactly zero outside a finite band of frequencies. The sinc pulse is an example of a strictly band-limited signal, as illustrated in Fig. 2.8. This figure also shows that the sinc pulse is only asymptotically limited in time. In an inverse manner, if a signal is strictly limited in time (i.e., the signal is exactly zero outside a finite time interval), then the spectrum of the signal is infinite in extent, even though the amplitude spectrum may assume a progressively smaller value. This behavior is exemplified by both the rectangular pulse (described in Fig. 2.2) and the triangular pulse (described in Fig. 2.11(b)). Accordingly, we may state that a signal cannot be strictly limited in both time and frequency.
᭿ BANDWIDTH
The bandwidth of a signal provides a measure of the extent of the significant spectral content of the signal for positive frequencies. When the signal is strictly band limited, the bandwidth is well defined. For example, the sinc pulse described in Fig. 2.8(a) has a bandwidth

40

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

equal to W. However, when the signal is not strictly band limited, which is generally the case, we encounter difficulty in defining the bandwidth of the signal. The difficulty arises because the meaning of the word “significant” attached to the spectral content of the signal is mathematically imprecise. Consequently, there is no universally accepted definition of bandwidth.
Nevertheless, there are some commonly used definitions for bandwidth. In this section, we consider three such definitions; the formulation of each definition depends on whether the signal is low-pass or band-pass. A signal is said to be low-pass if its significant spectral content is centered around the origin f ϭ 0. A signal is said be band-pass if its significant spectral content is centered around Ϯfc , where fc is a constant frequency.
When the spectrum of a signal is symmetric with a main lobe bounded by well-defined nulls (i.e., frequencies at which the spectrum is zero), we may use the main lobe as the basis for defining the bandwidth of the signal. The rationale for doing so is that the main spectral lobe contains the significant portion of the signal energy. If the signal is low-pass, the bandwidth is defined as one half the total width of the main spectral lobe, since only one half of this lobe lies inside the positive frequency region. For example, a rectangular pulse of duration T seconds has a main spectral lobe of total width 12>T2 hertz centered at the origin, as depicted in Fig. 2.2(b). Accordingly, we may define the bandwidth of this rectangular pulse as 11>T2 hertz. If, on the other hand, the signal is band-pass with main spectral lobes centered around Ϯfc , where fc is large, the bandwidth is defined as the width of the main lobe for positive frequencies. This definition of bandwidth is called the nullto-null bandwidth. For example, an RF pulse of duration T seconds and frequency fc has main spectral lobes of width 12>T2 hertz centered around Ϯfc , as depicted in Fig. 2.9(b). Hence, we may define the null-to-null bandwidth of this RF pulse as 12>T2 hertz. On the basis of the definitions presented here, we may state that shifting the spectral content of a low-pass signal by a sufficiently large frequency has the effect of doubling the bandwidth of the signal. Such a frequency translation is attained by using the process of modulation, which is discussed in detail in Chapter 3.
Another popular definition of bandwidth is the 3-dB bandwidth. Specifically, if the signal is low-pass, the 3-dB bandwidth is defined as the separation between zero frequency, where the amplitude spectrum attains its peak value, and the positive frequency at which the amplitude spectrum drops to 1> 12 of its peak value. For example, the decaying exponential and rising exponential pulses defined in Fig. 2.4 have a 3-dB bandwidth of 1a>2p2 hertz. If, on the other hand, the signal is band-pass, centered at Ϯfc , the 3-dB bandwidth is defined as the separation (along the positive frequency axis) between the two frequencies at which the amplitude spectrum of the signal drops to 1> 12 of the peak value at fc . The 3-dB bandwidth has an advantage in that it can be read directly from a plot of the amplitude spectrum. However, it has a disadvantage in that it may be misleading if the amplitude spectrum has slowly decreasing tails.
Yet another measure for the bandwidth of a signal is the root mean-square (rms) bandwidth, defined as the square root of the second moment of a properly normalized form of the squared amplitude spectrum of the signal about a suitably chosen point. We assume that the signal is low-pass, so that the second moment may be taken about the origin. As for the normalized form of the squared amplitude spectrum, we use the nonnega-
q
tive function ƒG1f2ƒ2n ƒG1f2ƒ2 df, in which the denominator applies the correct nor-
LϪq malization in the sense that the integrated value of this ratio over the entire frequency axis is unity. We may thus formally define the rms bandwidth of a low-pass signal g1t2 with Fourier transform G1f2 as follows:

2.3 The Inverse Relationship Between Time and Frequency

q

1>2

f 2 ƒ G1f 2 ƒ 2 df

Wrms

ϭ

§

LϪq
q

¥

ƒG1f2ƒ2 df
LϪq

41 (2.58)

An attractive feature of the rms bandwidth Wrms is that it lends itself more readily to mathematical evaluation than the other two definitions of bandwidth, although it is not as eas-
ily measured in the laboratory.

᭿ TIME-BANDWIDTH PRODUCT For any family of pulse signals that differ by a time-scaling factor, the product of the signal’s duration and its bandwidth is always a constant, as shown by
1duration2 ϫ 1bandwidth2 ϭ constant

The product is called the time-bandwidth product or bandwidth-duration product. The constancy of the time-bandwidth product is another manifestation of the inverse relationship that exists between the time-domain and frequency-domain descriptions of a signal. In particular, if the duration of a pulse signal is decreased by compressing the time scale by a factor a, say, the frequency scale of the signal’s spectrum, and therefore the bandwidth of the signal, is expanded by the same factor a, by virtue of Property 2 (dilation), and the time-bandwidth product of the signal is thereby maintained constant. For example, a rectangular pulse of duration T seconds has a bandwidth (defined on the basis of the positivefrequency part of the main lobe) equal to 11>T2 hertz, making the time-bandwidth product of the pulse equal unity. The important point to note here is that whatever definition we use for the bandwidth of a signal, the time-bandwidth product remains constant over certain classes of pulse signals. The choice of a particular definition for bandwidth merely changes the value of the constant.
To be more specific, consider the rms bandwidth defined in Eq. (2.58). The corresponding definition for the rms duration of the signal g1t2 is

q

1>2

t2 ƒ g1t2 ƒ 2 dt

Trms

ϭ

§

LϪq
q

¥

ƒg1t2ƒ2 dt
LϪq

(2.59)

where it is assumed that the signal g1t2 is centered around the origin. It may be shown that using the rms definitions of Eqs. (2.58) and (2.59), the time-bandwidth product has the following form:

TrmsWrms

Ն

1 4p

(2.60)

where the constant is 11>4p2. It may also be shown that the Gaussian pulse satisfies this condition with the equality sign. For the details of these calculations, the reader is referred to Problem 2.51.

42

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

2.4 Dirac Delta Function

Strictly speaking, the theory of the Fourier transform, as described in Sections 2.2 and 2.3, is applicable only to time functions that satisfy the Dirichlet conditions. Such functions include energy signals—that is, signals for which the condition
q
ƒg1t2ƒ2 dt Ͻ ϱ
LϪq

holds. However, it would be highly desirable to extend the theory in two ways:

1. To combine the theory of Fourier series and Fourier transform into a unified frame-

work, so that the Fourier series may be treated as a special case of the Fourier trans-

form. (A review of the Fourier series is presented in Appendix 2.)

2. To expand applicability of the Fourier transform to include power signals—that is,

signals for which the condition

lim
TSϱ

1 2T

T
LϪT

ƒ g1t2 ƒ 2

dt

Ͻ

ϱ

holds.

It turns out that both of these objectives are met through the “proper use” of the Dirac delta function or unit impulse.
The Dirac delta function, denoted by d1t2, is defined as having zero amplitude everywhere except at t ϭ 0, where it is infinitely large in such a way that it contains unit area under its curve. Specifically, d1t2 satisfies the pair of relations

d1t2 ϭ 0, t 0

(2.61)

and

q

d1t2 dt ϭ 1 LϪq

(2.62)

An implication of this pair of relations is that the delta function d1t2 must be an even func-

tion of time t.

For the delta function to have meaning, however, it has to appear as a factor in the

integrand of an integral with respect to time and then, strictly speaking, only when the

other factor in the integrand is a continuous function of time. Let g1t2 be such a function,

and consider the product of g1t2 and the time-shifted delta function d1t Ϫ t02. In light of the two defining equations (2.61) and (2.62), we may express the integral of the product

g1t2d1t Ϫ t02 with respect to time t as follows:

q

LϪq g1t2d1t Ϫ t02 dt ϭ g1t02

(2.63)

The operation indicated on the left-hand side of this equation sifts out the value g1t02 of the function g1t2 at time t ϭ t0 , where Ϫϱ Ͻ t Ͻ ϱ. Accordingly, Eq. (2.63) is referred to
as the sifting property of the delta function. This property is sometimes used as the defin-

ing equation of a delta function; in effect, it incorporates Eqs. (2.61) and (2.62) into a sin-

gle relation.

Noting that the delta function d1t2 is an even function of t, we may rewrite Eq. (2.63)

in a way that emphasizes its resemblance to the convolution integral, as shown by

q

g1t2d1t Ϫ t2 dt ϭ g1t2 LϪq

(2.64)

2.4 Dirac Delta Function g(t)

43
G(f )

1.0

t 0

f 0

(a)

(b)

FIGURE 2.12 (a) The Dirac delta function d1t2. (b) Spectrum of d1t2.

or, using the notation for convolution:

g1t2 ଙ d1t2 ϭ g1t2

In words, the convolution of any time function g1t2 with the delta function d1t2 leaves that function completely unchanged. We refer to this statement as the replication property of the delta function.
By definition, the Fourier transform of the delta function is given by
q
F3d1t24 ϭ d1t2 exp1Ϫj2pft2 dt LϪq

Hence, using the sifting property of the delta function and noting that exp1Ϫj2pft2 is equal to unity at t ϭ 0, we obtain
F3d1t24 ϭ 1

We thus have the Fourier-transform pair for the Dirac delta function:

d1t2 Δ 1

(2.65)

This relation states that the spectrum of the delta function d1t2 extends uniformly over the entire frequency interval, as shown in Fig. 2.12.
It is important to realize that the Fourier-transform pair of Eq. (2.65) exists only in a limiting sense. The point is that no function in the ordinary sense has the two properties of Eqs. (2.61) and (2.62) or the equivalent sifting property of Eq. (2.63). However, we can imagine a sequence of functions that have progressively taller and thinner peaks at t ϭ 0, with the area under the curve remaining equal to unity, whereas the value of the function tends to zero at every point except t ϭ 0, where it tends to infinity. That is, we may view the delta function as the limiting form of a pulse of unit area as the duration of the pulse approaches zero. It is immaterial what sort of pulse shape is used.
In a rigorous sense, the Dirac delta function belongs to a special class of functions known as generalized functions or distributions. Indeed, in some situations its use requires that we exercise considerable care. Nevertheless, one beautiful aspect of the Dirac delta function lies precisely in the fact that a rather intuitive treatment of the function along the lines described herein often gives the correct answer.

EXAMPLE 2.10 The Delta Function as a Limiting Form of the Gaussian Pulse

Consider a Gaussian pulse of unit area, defined by

g1t2

ϭ

1 t

exp

¢

Ϫ

pt 2 t2

≤

(2.66)

44

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

where t is a variable parameter. The Gaussian function g1t2 has two useful properties: (1) its derivatives are all continuous, and (2) it dies away more rapidly than any power of t. The delta function d1t2 is obtained by taking the limit t S 0. The Gaussian pulse then becomes infinitely narrow in duration and infinitely large in amplitude, yet its area remains finite and fixed at unity. Figure 2.13(a) illustrates the sequence of such pulses as the parameter t is permitted to decrease.
The Gaussian pulse g1t2, defined here, is the same as the unit Gaussian pulse exp1Ϫpt22 derived in Example 2.6, except for the fact that it is now scaled in time by the factor t and scaled in amplitude by the factor 1>t. Therefore, applying the linearity and dilation properties of the Fourier transform to the Fourier transform pair of Eq. (2.40), we find that the Fourier transform of the Gaussian pulse g1t2 defined in Eq. (2.66) is also Gaussian, as shown by
G1f2 ϭ exp1Ϫpt2f 22
g (t )
4.0

␶ = 0.25

–1.0

–0.5

␶ = 0.5

␶ =1

␶ =2

t

0

0.5

1.0

(a)

G(f ) 1.0

␶ = 0.25

␶ = 0.5

␶ =1

␶ =2

FIGURE 2.13

f (a) Gaussian pulses of

–1.0

–0.5

0

0.5

1.0

varying duration.

(b)

(b) Corresponding spectra.

2.4 Dirac Delta Function

45

Figure 2.13(b) illustrates the effect of varying the parameter t on the spectrum of the Gaussian pulse g1t2. Thus putting t ϭ 0, we find, as expected, that the Fourier transform of the delta function is unity.

᭿ APPLICATIONS OF THE DELTA FUNCTION
1. dc Signal. By applying the duality property to the Fourier-transform pair of Eq. (2.65) and noting that the delta function is an even function, we obtain

1 Δ d1f2

(2.67)

Equation (2.67) states that a dc signal is transformed in the frequency domain into a delta function d1t2 occurring at zero frequency, as shown in Fig. 2.14. Of course, this result is intuitively satisfying.
Invoking the definition of Fourier transform, we readily deduce from Eq. (2.67) the useful relation

q
exp1Ϫj2pft2 dt ϭ d1f2 LϪq

Recognizing that the delta function d1f2 is real valued, we may simplify this relation as follows:

q
cos12pft2 dt ϭ d1f2 LϪq

(2.68)

which provides yet another definition for the delta function, albeit in the frequency domain.

2. Complex Exponential Function. Next, by applying the frequency-shifting property to Eq. (2.67), we obtain the Fouriertransform pair

exp1j2pfct2 Δ d1f Ϫ fc2

(2.69)

for a complex exponential function of frequency fc . Equation (2.69) states that the complex exponential function exp1j2pfct2 is transformed in the frequency domain into a delta function d1f Ϫ fc2 occurring at f ϭ fc .

g(t)

G(f )

1.0
t 0 (a) FIGURE 2.14 (a) dc signal. (b) Spectrum.

f 0 (b)

46

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

G(f ) g (t)
1.0

t

f

–fc

0

fc

–1.0

—1 fc

(a)

(b)

FIGURE 2.15 (a) Cosine function. (b) Spectrum.

3. Sinusoidal Functions. Consider next the problem of evaluating the Fourier transform of the cosine function cos12pfct2. We first use Euler’s formula to write

cos12pfc t2

ϭ

1 2

3exp1j2pfc t2

ϩ

exp1Ϫj2pfc t24

(2.70)

Therefore, using Eq. (2.69), we find that the cosine function cos12pfct2 is represented by the Fourier-transform pair

1

cos12pfc t2

Δ

3d1f 2

Ϫ

fc2

ϩ

d1f

ϩ

fc24

(2.71)

In other words, the spectrum of the cosine function cos12pfct2 consists of a pair of delta functions occurring at f ϭ Ϯfc , each of which is weighted by the factor 1>2, as shown in Fig. 2.15.
Similarly, we may show that the sine function sin12pfct2 is represented by the Fouriertransform pair

sin12pfc t2

Δ

1 3d1f 2j

Ϫ

fc2

Ϫ

d1f

ϩ

fc24

(2.72)

which is illustrated in Fig. 2.16.

g (t) 1.0
t
–1.0 —1 fc
(a) FIGURE 2.16 (a) Sine function. (b) Spectrum.

jG(f )

–fc

f

0

fc

(b)

2.4 Dirac Delta Function

47

᭤ Drill Problem 2.9 Determine the Fourier transform of the squared sinusoidal signals:

(i) g1t2 ϭ cos212pfc t2

(ii) g1t2 ϭ sin212pfc t2

᭣

4. Signum Function. The signum function sgn1t2 equals ϩ1 for positive time and Ϫ1 for negative time, as shown by the solid curve in Fig. 2.17(a). The signum function was defined previously in Eq. (2.18); this definition is reproduced here for convenience of presentation:

ϩ1, sgn1t2 ϭ c 0,
Ϫ1,

tϾ0 tϭ0 tϽ0

The signum function does not satisfy the Dirichlet conditions and therefore, strictly speaking, it does not have a Fourier transform. However, we may define a Fourier transform for the signum function by viewing it as the limiting form of the odd-symmetric doubleexponential pulse

exp1Ϫat2, t Ͼ 0

g1t2 ϭ c 0,

tϭ0

Ϫexp1at2, t Ͻ 0

(2.73)

as the parameter a approaches zero. The signal g1t2, shown as the dashed curve in Fig. 2.17(a), does satisfy the Dirichlet conditions. Its Fourier transform was derived in

g (t ) +1.0

t 0 –1.0 (a) |G ( f ) |

FIGURE 2.17 (a) Signum function

(continuous curve), and double-

exponential pulse (dashed curve).

f (b) Amplitude spectrum of signum

0

function (continuous curve), and

that of double-exponential pulse

(b)

(dashed curve).

48

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Example 2.3; the result is given by [see Eq. (2.19)]: Ϫj4pf
G1f2 ϭ a2 ϩ 12pf 22

The amplitude spectrum ƒG1f2ƒ is shown as the dashed curve in Fig. 2.17(b). In the limit
as a approaches zero, we have

Ϫ4jpf

F3sgn1t24

ϭ

lim
aS0

a2

ϩ

12pf 22

ϭ1 jpf

That is,

sgn1t2

Δ

1 jpf

(2.74)

The amplitude spectrum of the signum function is shown as the continuous curve in Fig. 2.17(b). Here we see that for small a, the approximation is very good except near the origin on the frequency axis. At the origin, the spectrum of the approximating function g1t2 is zero for a Ͼ 0, whereas the spectrum of the signum function goes to infinity.

5. Unit Step Function. The unit step function u(t) equals ϩ1 for positive time and zero for negative time. Previously defined in Eq. (2.11), it is reproduced here for convenience:

1,

u1t2

ϭ

d

1 ,

2

0,

tϾ0 tϭ0 tϽ0

The waveform of the unit step function is shown in Fig. 2.18(a). From this defining equation and that of the signum function, or from the waveforms of Figs. 2.17(a) and 2.18(a), we see that the unit step function and signum function are related by

u1t2 ϭ 1 3sgn11t2 ϩ 124 2

(2.75)

Hence, using the linearity property of the Fourier transform and the Fourier-transform pairs of Eqs. (2.67) and (2.75), we find that the unit step function is represented by the Fourier-transform pair

g(t)

|G(f )|

1.0 —1 2
t 0
(a)

f 0 (b)

FIGURE 2.18 (a) Unit step function. (b) Amplitude spectrum.

2.4 Dirac Delta Function

49

u1t2

Δ

1 j2pf

ϩ

1 d1f2 2

(2.76)

This means that the spectrum of the unit step function contains a delta function weighted by a factor of 1>2 and occurring at zero frequency, as shown in Fig. 2.18(b).

6. Integration in the Time Domain (Revisited).

The relation of Eq. (2.41) describes the effect of integration on the Fourier transform of a

signal g1t2, assuming that G102 is zero. We now consider the more general case, with no

such assumption made.

Let
t

y1t2 ϭ g1t2 dt LϪq

(2.77)

The integrated signal y1t2 can be viewed as the convolution of the original signal g1t2 and the unit step function u1t2, as shown by
q
y1t2 ϭ g1t2u1t Ϫ t2 dt LϪq
where the time-shifted unit step function u1t Ϫ t2 is itself defined by

1,

u1t

Ϫ

t2

ϭ

d

1 ,

2

0,

tϽt tϭt tϾt

Recognizing that convolution in the time domain is transformed into multiplication in the frequency domain in accordance with Property 12, and using the Fourier-transform pair of Eq. (2.76) for the unit step function u1t2, we find that the Fourier transform of y1t2 is

Y1f 2

ϭ

G1f

2

c

1 j2pf

ϩ

1 2

d1f

2

d

(2.78)

where G1f2 is the Fourier transform of g1t2. According to the sifting property of a delta function formulated in the frequency domain, we have

G1f2d1f2 ϭ G102d1f2

Hence, we may rewrite Eq. (2.78) in the equivalent form:

Y1f2 ϭ 1 G1f2 ϩ 1 G102d1f2

j2pf

2

In general, the effect of integrating the signal g1t2 is therefore described by the Fourier-

transform pair

t
g1t2 dt Δ

1 G1f2 ϩ 1 G102d1f2

LϪq

j2pf

2

(2.79)

This is the desired result, which includes Eq. (2.41) as a special case (i.e., G102 ϭ 0).

᭤ Drill Problem 2.10 Consider the function

g1t2 ϭ d a t ϩ 1 b Ϫ d a t Ϫ 1 b

2

2

50

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

which

consists

of

the

difference

between

two

delta

functions

at

t

ϭ

Ϯ

1 .

The

integration

of

2

g1t2 with respect to time t yields the unit rectangular function rect1t2. Using Eq. (2.79), show that

rect1t2 Δ sinc1f 2

which is a special form of Eq. (2.10).

᭣

2.5 Fourier Transforms
of Periodic Signals

It is well known that by using the Fourier series, a periodic signal can be represented as a

sum of complex exponentials. (Appendix 2 presents a review of the Fourier series.) Also,

in a limiting sense, Fourier transforms can be defined for complex exponentials, as demon-

strated in Eqs. (2.69), (2.71), and (2.72). Therefore, it seems reasonable to represent a peri-

odic signal in terms of a Fourier transform, provided that this transform is permitted to

include delta functions.

Consider then a periodic signal gT01t2, where the subscript T0 denotes the period of the signal. We know that gT01t2 can be represented in terms of the complex exponential Fourier series as (see Appendix 2)

q

gT01t2

ϭ

a cn
n ϭ Ϫq

exp1j2pnf0t2

(2.80)

where cn is the complex Fourier coefficient, defined by 1 T0>2
cn ϭ T0 LϪT0>2 gT01t2 exp1Ϫj2pnf0t2 dt

(2.81)

and f0 is the fundamental frequency defined as the reciprocal of the period T0 ; that is,

f0

ϭ

1 T0

(2.82)

Let g1t2 be a pulselike function, which equals gT01t2 over one period and is zero elsewhere; that is,

g1t2 ϭ c gT01t2, 0,

Ϫ T0 Յ t Յ T0

2

2

elsewhere

(2.83)

The periodic signal gT01t2 may now be expressed in terms of the function g1t2 as the infinite summation

q

gT01t2

ϭ

a g1t
m ϭ Ϫq

Ϫ

mT02

(2.84)

Based on this representation, we may view g1t2 as a generating function, in that it gener-

ates the periodic signal gT01t2. Being pulselike with some finite energy, the function g1t2 is Fourier transformable. Accordingly, in light of Eqs. (2.82) and (2.83), we may rewrite the

formula for the complex Fourier coefficient cn as follows:

q

cn ϭ f0 LϪqg1t2 exp1Ϫj2pnf0t2 dt

ϭ f0G1nf02

(2.85)

2.5 Fourier Transforms of Periodic Signals

51

where G1nf02 is the Fourier transform of g1t2, evaluated at the frequency f ϭ nf0 . We may

thus rewrite the formula of Eq. (2.80) for the reconstruction of the periodic signal

gT01t2 as

q

gT01t2

ϭ

f0 a G1nf02
n ϭ Ϫq

exp1j2pnf0t2

(2.86)

Therefore, eliminating gT01t2 between Eqs. (2.84) and (2.86), we may now write

q

q

a g1t Ϫ mT02 ϭ f0 a G1nf02 exp1j2pnf0t2

m ϭ Ϫq

n ϭ Ϫq

(2.87)

which defines one form of Poisson’s sum formula. Finally, using Eq. (2.69), which defines the Fourier transform of a complex expo-
nential function, in Eq. (2.87), we deduce the Fourier-transform pair:

q

q

a g1t Ϫ mT02 Δ f0 a G1nf02d1f Ϫ nf02

m ϭ Ϫq

n ϭ Ϫq

(2.88)

for the periodic signal gT01t2 whose fundamental frequency f0 ϭ 11>T02. Equation (2.88) simply states that the Fourier transform of a periodic signal consists of delta functions
occurring at integer multiples of the fundamental frequency f0 , including the origin, and that each delta function is weighted by a factor equal to the corresponding value of G1nf02. Indeed, this relation merely provides a method to display the frequency content of the periodic signal gT01t2.
It is of interest to observe that the pulselike function g1t2, constituting one period of the periodic signal gT01t2, has a continuous spectrum defined by G1f 2. On the other hand, the periodic signal gT01t2 itself has a discrete spectrum. In words, we may therefore sum up the transformation embodied in Eq. (2.88) as follows:

Periodicity in the time domain has the effect of changing the spectrum of a pulselike signal into a discrete form defined at integer multiples of the fundamental frequency, and vice versa.

EXAMPLE 2.11 Ideal Sampling Function

An ideal sampling function, or Dirac comb, consists of an infinite sequence of uniformly spaced delta functions, as shown in Fig. 2.19(a). We denote this waveform by

q

dT01t2

ϭ

a d1t
m ϭ Ϫq

Ϫ

mT02

(2.89)

We observe that the generating function g1t2 for the ideal sampling function dT01t2 consists simply of the delta function d1t2. We therefore have G1f 2 ϭ 1, and

G1nf02 ϭ 1 for all n

Thus, the use of Eq. (2.88) yields the new result

q

q

a d1t Ϫ mT02 Δ f0 a d1f Ϫ nf02

m ϭ Ϫq

n ϭ Ϫq

(2.90)

Equation (2.90) states that the Fourier transform of a periodic train of delta functions, spaced
T0 seconds apart, consists of another set of delta functions weighted by the factor f0 ϭ 11>T02 and regularly spaced f0 Hz apart along the frequency axis as in Fig. 2.19(b). In the special case of T0 ϭ 1, a periodic train of delta functions is, like a Gaussian pulse, its own Fourier transform.

52

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

␦T0(t)

–3T0

–2T0

–T0

0

(a)

F|␦T0(t)|

t

T0

2T0

3T0

f

– —T50– – —T40– – —T30– – —T20– – —T10–

0

—T10– —T20– —T30– —T40– —T50–

(b)

FIGURE 2.19 (a) Dirac comb. (b) Spectrum.

Applying the inverse Fourier transform to the right-hand side of Eq. (2.90), we get the

relationship

q

q

a d1t Ϫ mT02 ϭ f0 a exp1j2pnf0t2

m ϭ Ϫq

n ϭ Ϫq

(2.91)

On the other hand, applying the Fourier transform to the left-hand side of Eq. (2.90), we get

the dual relationship:

q

q

T0 a exp1j2pmf T0 2 ϭ a d1f Ϫ nf02

m ϭ Ϫq

n ϭ Ϫq

(2.92)

where we have used the relation of Eq. (2.82) rewritten in the form T0 ϭ 1>f0. Equations (2.91) and (2.92) are the dual of each other, in that in the delta functions show up in the time domain
in Eq. (2.91) whereas in Eq. (2.92) the delta functions show up in the frequency domain.

᭤ Drill Problem 2.11

Using the Euler formula cos x ϭ 1 3exp1jx2 ϩ exp1Ϫjx24, refor2

mulate Eqs. (2.91) and (2.92) in terms of cosinusoidal functions.

᭣

2.6 Transmission of Signals Through
Linear Systems: Convolution Revisited
With the Fourier transform theory presented in the previous sections at our disposal, we are now ready to turn our attention to the study of a special class of systems known to be linear. A system refers to any physical device or phenomenon that produces an output signal in response to an input signal. It is customary to refer to the input signal as the excitation and to the output signal as the response. In a linear system, the principle of superposition holds; that is, the response of a linear system to a number of excitations applied simultaneously is equal to the sum of the responses of the system when each excitation is applied individually. Important examples of linear systems include filters and communication channels operating in their

2.6 Transmission of Signals Through Linear Systems: Convolution Revisited

53

linear region. A filter refers to a frequency-selective device that is used to limit the spectrum of a signal to some band of frequencies. A channel refers to a physical medium that connects the transmitter and receiver of a communication system. We wish to evaluate the effects of transmitting signals through linear filters and communication channels. This evaluation may be carried out in two ways, depending on the description adopted for the filter or channel. That is, we may use time-domain or frequency-domain ideas, as described below.

᭿ TIME RESPONSE
In the time domain, a linear system is described in terms of its impulse response, which is defined as the response of the system (with zero initial conditions) to a unit impulse or delta function d1t2 applied to the input of the system. If the system is time invariant, then this property implies that a time-shifted unit impulse at the input of the system produces an impulse response at the output, shifted by exactly the same amount. In other words, the shape of the impulse response of a linear time-invariant system is the same no matter when the unit impulse is applied to the system. Thus, assuming that the unit impulse or delta function is applied at time t ϭ 0, we may denote the impulse response of a linear time-invariant system by h1t2. Let this system be subjected to an arbitrary excitation x1t2, as in Fig. 2.20(a). To determine the response y1t2 of the system, we begin by first approximating x1t2 by a staircase function composed of narrow rectangular pulses, each of duration ¢t, as shown in Fig. 2.20(b). Clearly the approximation becomes better for smaller ¢t. As ¢t approaches zero, each pulse approaches, in the limit, a delta function weighted by a factor equal to the height of the pulse times ¢t. Consider a typical pulse, shown shaded in Fig. 2.20(b), which occurs at t ϭ t. This pulse has an area equal to x1t2¢t. By definition, the response of the system to a unit impulse or delta function d1t2, occurring at t ϭ 0, is h1t2. It follows therefore that the response of the system to a delta function, weighted by the factor x1t2¢t and occurring at t ϭ t, must be x1t2h1t Ϫ t2¢t. To find the response y1t2 at some time t, we

Input x(t)

Impulse response
h(t)
(a)

Output y(t)

x(t)

x(␶) approximation

0

␶

⌬␶

(b)

t
FIGURE 2.20 (a) Linear system with input x1t2 and output y1t2. (b) Staircase approximation of input x1t2.

54

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

apply the principle of superposition. Thus, summing the various infinitesimal responses due to the various input pulses, we obtain in the limit, as ¢t approaches zero,

q

y1t2 ϭ x1t2h1t Ϫ t2 dt LϪq

(2.93)

This relation is called the convolution integral. In Eq. (2.93), three different time scales are involved: excitation time t, response time
t, and system-memory time 1t Ϫ t2. This relation is the basis of time-domain analysis of linear time-invariant systems. It states that the present value of the response of a linear time-invariant system is a weighted integral over the past history of the input signal, weighted according to the impulse response of the system. Thus, the impulse response acts as a memory function for the system.
In Eq. (2.93), the excitation x1t2 is convolved with the impulse response h1t2 to produce the response y1t2. Since convolution is commutative, it follows that we may also write

q
y1t2 ϭ h1t2x1t Ϫ t2 dt LϪq

(2.94)

where h1t2 is convolved with x1t2.

EXAMPLE 2.12 Tapped-Delay-Line Filter

Consider a linear time-invariant filter with impulse response h1t2. We make two assumptions:

1. Causality, which means that the impulse response h1t2 is zero for t Ͻ 0.

2. Finite support, which means that the impulse response of the filter is of some finite duration Tf , so that we may write h1t2 ϭ 0 for t Ն Tf .

Under these two assumptions, we may express the filter output y1t2 produced in response to the input x1t2 as

Tf
y1t2 ϭ h1t2x1t Ϫ t2 dt L0

(2.95)

Let the input x1t2, impulse response h1t2, and output y1t2 be uniformly sampled at the rate 11> ¢t2 samples per second, so that we may put

t ϭ n ¢t

and

t ϭ k ¢t

where k and n are integers, and ¢t is the sampling period. Assuming that ¢t is small enough for the product h1t2x1t Ϫ t2 to remain essentially constant for k ¢t Յ t Յ 1k ϩ 12 ¢t for all values of k and t, we may approximate Eq. (2.95) by a convolution sum as shown by
NϪ1
y1n ¢t2 ϭ a h1k ¢t2x1n ¢t Ϫ k ¢t2 ¢t
kϭ0

where N ¢t ϭ Tf . Define the weight
wk ϭ h1k ¢t2 ¢t, k ϭ 0, 1, Á , N Ϫ 1
We may then rewrite the formula for y1n ¢t2 as
NϪ1
y1n ¢t2 ϭ a wkx1n ¢t Ϫ k ¢t2
kϭ0

(2.96) (2.97)

2.6 Transmission of Signals Through Linear Systems: Convolution Revisited

55

Sampled input x(n⌬␶)

Delay ⌬␶

Delay ⌬␶

Delay ⌬␶

Delay ⌬␶

Weights w0

w1

w2

wN – 3

wN –2

wN –1

... ...

Σ

FIGURE 2.21 Tapped-delay-line filter.

Sampled output y(n⌬␶)

Equation (2.97) may be realized using the structure shown in Fig. 2.21, which consists of a set of delay elements (each producing a delay of ¢t seconds), a set of multipliers connected to the delay-line taps, a corresponding set of weights supplied to the multipliers, and a summer for adding the multiplier outputs. This structure is known as a tapped-delay-line filter or transversal filter. Note that in Fig. 2.21 the tap-spacing or basic increment of delay is equal to the sampling period of the input sequence 5x1n ¢t26.

᭿ CAUSALITY AND STABILITY
A system is said to be causal if it does not respond before the excitation is applied. For a linear time-invariant system to be causal, it is clear that the impulse response h1t2 must vanish for negative time, as stated in Example 2.12. That is, we may formally state that the necessary and sufficient condition for a linear time-invariant system to be causal is

h1t2 ϭ 0, t Ͻ 0

(2.98)

Clearly, for a system operating in real time to be physically realizable, it must be causal. However, there are many applications in which the signal to be processed is only available in stored form; in these situations, the system can be noncausal and yet physically realizable.
The system is said to be stable if the output signal is bounded for all bounded input signals. We refer to this requirement as the bounded input–bounded output (BIBO) stability criterion, which is well suited for the analysis of linear time-invariant systems. Let the input signal x1t2 be bounded, as shown by

ƒx1t2ƒ Ͻ M for all t

where M is a positive real finite number. Taking the absolute values of both sides of Eq. (2.94), we have

q
ƒy1t2ƒ ϭ ` h1t2x1t Ϫ t2 dt `
LϪq

(2.99)

56

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Next, we recognize that the absolute value of an integral is bounded by the integral of the absolute value of the integrand, as shown by

q

q

` h1t2x1t Ϫ t2 dt ` Յ ƒh1t2x1t Ϫ t2ƒ dt

LϪq

LϪq

q

ϭ M ƒh1t2ƒ dt
LϪq

Hence, substituting this inequality into Eq. (2.99) yields the important result
q
ƒy1t2ƒ Յ M ƒh1t2ƒ dt
LϪq

It follows therefore that for a linear time-invariant system to be stable, the impulse response h1t2 must be absolutely integrable. That is, the necessary and sufficient condition for BIBO stability of a linear time-invariant system is described by

q
ƒh1t2ƒ dt Ͻ ϱ
LϪq

(2.100)

where h1t2 is the impulse response of the system.

᭿ FREQUENCY RESPONSE
Consider next a linear time-invariant system of impulse response h1t2, which is driven by a complex exponential input of unit amplitude and frequency f; that is,

x1t2 ϭ exp1j2pft2

(2.101)

Using Eqs. (2.101) in (2.94), the response of the system is obtained as
q
y1t2 ϭ h1t2 exp3j2pf1t Ϫ t24 dt LϪq
q
ϭ exp1j2pft2 h1t2 exp1Ϫj2pft2 dt LϪq

(2.102)

Define the transfer function or frequency response of the system as the Fourier transform of its impulse response, as shown by

q
H1f2 ϭ h1t2 exp1Ϫj2pft2 dt LϪq

(2.103)

The terms transfer function and frequency response are used interchangably. The integral in the last line of Eq. (2.102) is the same as that of Eq. (2.103), except for the fact that t is used in place of t. Hence, we may rewrite Eq. (2.102) in the form

y1t2 ϭ H1f2 exp1j2pft2

(2.104)

Equation (2.104) states that the response of a linear time-invariant system to a complex exponential function of frequency f is the same complex exponential function multiplied by a constant coefficient H1f2.

2.6 Transmission of Signals Through Linear Systems: Convolution Revisited

57

Equation (2.103) is one definition of the transfer function H1f2. An alternative definition of the transfer function may be deduced by dividing Eq. (2.104) by (2.101) to obtain

H1f2

ϭ

y1t2 x1t2

`
x1t2 ϭ exp1j2pft2

(2.105)

Consider next an arbitrary signal x1t2 applied to the system. The signal x1t2 may be

expressed in terms of its inverse Fourier transform as

q

x1t2 ϭ X1f2 exp1j2pft2 df LϪq

(2.106)

Equivalently, we may express x1t2 in the limiting form

q

x1t2

ϭ

lim
¢f S 0 f ϭ k ¢f

k

a
ϭ Ϫq

X1f2

exp1j2pft2

¢f

(2.107)

That is, the input signal x1t2 may be viewed as a superposition of complex exponentials of incremental amplitude. Because the system is linear, the response to this superposition of complex exponential inputs is given by

q

y1t2 ϭ lim ¢f S 0 f ϭ k ¢f

a H1f2X1f2 exp1j2pft2 ¢f
k ϭ Ϫq

q

ϭ H1f2X1f2 exp1j2pft2 df LϪq

(2.108)

The Fourier transform of the output signal y1t2 is therefore readily obtained as

Y1f2 ϭ H1f2X1f2

(2.109)

According to Eq. (2.109), a linear time-invariant system may thus be described quite simply in the frequency domain by noting that the Fourier transform of the output is equal to the product of the frequency response of the system and the Fourier transform of the input.
Of course, we could have deduced the result of Eq. (2.109) directly by recognizing two facts:
1. The response y1t2 of a linear time-invariant system of impulse response h1t2 to an arbitrary input x1t2 is obtained by convolving x1t2 with h1t2, in accordance with Eq. (2.93).
2. The convolution of a pair of time functions is transformed into the multiplication of their Fourier transforms.

The alternative derivation of Eq. (2.109) above is presented primarily to develop an understanding of why the Fourier representation of a time function as a superposition of complex exponentials is so useful in analyzing the behavior of linear time-invariant systems.
The frequency response H1f2 is a characteristic property of a linear time-invariant system. It is, in general, a complex quantity, so that we may express it in the form

H1f2 ϭ ƒH1f2ƒ exp3jb1f24

(2.110)

where ƒH1f2ƒ is called the amplitude response or magnitude response, and b1f2 the phase
or phase response. In the special case of a linear system with real-valued impulse response h1t2, the frequency response H1f2 exhibits conjugate symmetry, which means that

ƒH1f2ƒ ϭ ƒH1Ϫf2ƒ

58

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

and

b1f2 ϭ Ϫb1Ϫf2

That is, the amplitude response ƒH1f2ƒ of a linear system with real-valued impulse response
is an even function of frequency, whereas the phase b1f2 is an odd function of frequency. In some applications it is preferable to work with the logarithm of H1f2, expressed
in polar form, rather than with H1f2 itself. Define the natural logarithm

ln H1f2 ϭ a1f2 ϩ jb1f2

(2.111)

where

a1f2 ϭ lnƒH1f2ƒ

(2.112)

The function a1f2 is one definition of the gain of the system. It is measured in nepers, whereas the phase b1f2 is measured in radians. Equation (2.111) indicates that the gain a1f2 and phase b1f2 are the real and imaginary parts of the natural logarithm of the frequency response H1f2, respectively. The gain may also be expressed in decibels (dB) by using the definition

aЈ1f 2 ϭ 20 log10 ƒ H1f 2 ƒ
The two gain functions a1f2 and aЈ1f2 are related by

(2.113)

aЈ1f2 ϭ 8.69a1f2

(2.114)

That is, 1 neper is equal to 8.69 dB.
From the discussion presented Section 2.3, we note that the bandwidth of a system
is specified by the constancy of its amplitude response. The bandwidth of a low-pass sys-
tem is thus defined as the frequency at which the amplitude response ƒH1f2ƒ is 1> 12 times
its value of zero frequency or, equivalently, the frequency at which the gain aЈ1f2 drops by 3 dB below its value at zero frequency, as illustrated in Fig. 2.22(a). Correspondingly, the
bandwidth of a band-pass system is defined as the range of frequencies over which the
amplitude response ƒH1f2ƒ remains within 1> 12 times its value at the mid-band frequency,
as illustrated in Fig. 2.22(b).

᭿ PALEY–WIENER CRITERION

A necessary and sufficient condition for a function a1f2 to be the gain of a causal filter is

the convergence of the integral.

q
LϪq

¢

ƒ a1f
1ϩ

2 f

ƒ
2

≤

df

Ͻ

ϱ

(2.115)

This condition is known as the Paley–Wiener criterion. It states that, provided the gain a1f2 satisfies the condition of Eq. (2.115), then we may associate with this gain a suitable phase b1f2 such that the resulting filter has a causal impulse response that is zero for negative time. In other words, the Paley–Wiener criterion is the frequency-domain equivalent of the causality requirement. A system with a realizable gain characteristic may have infinite attenuation [i.e., a1f2 ϭ Ϫϱ] for a discrete set of frequencies, but it cannot have infinite attenuation over a band of frequencies; otherwise, the Paley–Wiener criterion is violated.

᭤ Drill Problem 2.12 Discuss the following two issues, citing examples for your answers:

(a) Is it possible for a linear time-invariant system to be causal but unstable?

(b) Is it possible for such a system to be noncausal but stable?

᭣

2.6 Transmission of Signals Through Linear Systems: Convolution Revisited

59

|H(f )|

|H(0)|

|H(0)| ———
/2

f

–B

0

B

(a)

|H(f )|

|H(fc)|

—|H—(f—c)| /2

f

–fc – B –fc + B

0

fc – B

fc + B

–fc

(b)

fc

FIGURE 2.22 Illustration of the definition of system bandwidth. (a) Low-pass system. (b) Band-pass system.

᭤ Drill Problem 2.13 The impulse response of a linear system is defined by the Gaussian function

h1t2

ϭ

exp

¢Ϫ

t2 2t2

≤

where t is an adjustable parameter that defines pulse duration. Determine the frequency response

of the system.

᭣

᭤ Drill Problem 2.14 A tapped-delay-line filter consists of N weights, where N is odd. It is symmetric with respect to the center tap; that is, the weights satisfy the condition

wn ϭ wNϪ1Ϫn , 0 Յ n Յ N Ϫ 1

(a) Find the amplitude response of the filter.

(b) Show that this filter has a linear phase response. What is the implication of this property?

(c) What is the time delay produced by the filter?

᭣

60

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

2.7 Ideal Low-Pass Filters

As previously mentioned, a filter is a frequency-selective system that is used to limit the spectrum of a signal to some specified band of frequencies. Its frequency response is characterized by a passband and a stopband. The frequencies inside the passband are transmitted with little or no distortion, whereas those in the stopband are rejected. The filter may be of the low-pass, high-pass, band-pass, or band-stop type, depending on whether it transmits low, high, intermediate, or all but intermediate frequencies, respectively. We have already encountered examples of low-pass and band-pass systems in Fig. 2.22.
Filters, in one form or another, represent an important functional block in building communication systems. In this book, we will be concerned with the use of high-pass, lowpass, and band-pass filters.
In this section, we study the time response of the ideal low-pass filter, which transmits, without any distortion, all frequencies inside the passband and completely rejects all frequencies inside the stopband, as illustrated in Fig. 2.23. According to this figure, the frequency response of an ideal low-pass filter satisfies two necessary conditions:
1. The amplitude response of the filter is a constant inside the passband ϪB Յ f Յ B. (The constant in Fig. 2.23 is set equal to unity for convenience of presentation.)
2. The phase response varies linearly with frequency inside the passband of the filter. (Outside the passband, the phase response may assume arbitrary values.)
In mathematical terms, the transfer function of an ideal low-pass filter is therefore defined by

H1f 2 ϭ b exp1Ϫj2pft02, 0,

ϪB Յ f Յ B
ƒf ƒ Ͼ B

(2.116)

|H(f )| 1.0

(a)

f

–B

0

B

arg[H(f )]

(b)

f

–B 0

B

FIGURE 2.23 Frequency response of

ideal low-pass filter. (a) Amplitude

response. (b) Phase response; outside

the band ϪB Յ f Յ B, the phase

Slope = –2␲ t0

response assumes an arbitrary form (not shown in the figure).

2.7 Ideal Low-Pass Filters

61

h(t) 2B

t

0

t0

1 B
FIGURE 2.24 Impulse response of ideal low-pass filter.

The parameter B defines the bandwidth of the filter. The ideal low-pass filter is, of course, noncausal because it violates the Paley–Wiener criterion. This observation may also be confirmed by examining the impulse response h1t2. Thus, by evaluating the inverse Fourier transform of the transfer function of Eq. (2.116), we get

B

h1t2

ϭ LϪB

exp3j2pf1t

Ϫ

t024

df

(2.117)

where the limits of integration have been reduced to the frequency band inside which H1f2 does not vanish. Equation (2.117) is readily integrated, yielding

h1t2

ϭ

sin32pB1t p1t Ϫ

Ϫ t024 t02

ϭ 2B sinc32B1t Ϫ t024

(2.118)

The impulse response has a peak amplitude of 2B centered on time t0 , as shown in Fig. 2.24 for t0 ϭ 1>B. The duration of the main lobe of the impulse response is 1>B, and the buildup time from the zero at the beginning of the main lobe to the peak value is 1>2B. We see from Fig. 2.24 that, for any finite value of t0 , there is some response from the filter before the time t ϭ 0 at which the unit impulse is applied to the input; this observation confirms that the ideal low-pass filter is noncausal. Note, however, that we can always make the delay t0 large enough for the condition
ƒsinc32B1t Ϫ t024ƒ V 1, for t Ͻ 0

to be satisfied. By so doing, we are able to build a causal filter that approximates an ideal low-pass filter, with the approximation improving with increasing delay t0 .

᭿ PULSE RESPONSE OF IDEAL LOW-PASS FILTERS
Consider a rectangular pulse x1t2 of unit amplitude and duration T, which is applied to an ideal low-pass filter of bandwidth B. The problem is to determine the response y1t2 of the filter.
The impulse response h1t2 of the filter is defined by Eq. (2.118). Clearly, the delay t0 has no effect on the shape of the filter response y1t2. Without loss of generality, we may

62

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

therefore simplify the exposition by setting t0 ϭ 0, in which case the impulse response of Eq. (2.118) reduces to

h1t2 ϭ 2B sinc12Bt2

(2.119)

With the input x1t2 ϭ 1 for Ϫ1T>22 Յ t Յ 1T>22, the resulting response of the filter is

given by the convolution integral

q

y1t2 ϭ x1t2h1t Ϫ t2 dt LϪq

T>2

ϭ 2B

sinc32B1t Ϫ t24 dt

LϪT>2

T>2 sin32pB1t Ϫ t24

ϭ 2B

a

b dt

LϪT>2 2pB1t Ϫ t2

(2.120)

Define a new dimensionless variable

l ϭ 2pB1t Ϫ t2

Then, changing the integration variable from t to l, we may rewrite Eq. (2.120) as

y1t2

ϭ

1 p

2pB1tϩT>22 a sin l b

L2pB1t Ϫ T>22

l

dl

ϭ

1 p

c

L0

2pB1t ϩ T>22

a

sin l

l

b

dl

Ϫ

2pB1tϪT>22 a sin l b

L0

l

dl d

ϭ

1 p

5Si32pB1t

ϩ

T>224

Ϫ

Si32pB1t

Ϫ

T>2246

(2.121)

In Eq. (2.121), we have introduced a new expression called the sine integral, which is

defined by

Si1u2

u
ϭ L0

sin x

x

dx

(2.122)

Unfortunately, the sine integral Si(u) cannot be evaluated in closed form in terms of elementary functions. However, it can be integrated in a power series, which, in turn, leads to the graph plotted in Fig. 2.25. From this figure we make three observations:
1. The sine integral Si1u2 is an oscillatory function of u, having odd symmetry about the origin u ϭ 0.
2. It has its maxima and minima at multiples of p. 3. It approaches the limiting value 1p>22 for large positive values of u.

In Fig. 2.25, we see that the sine integral Si1u2 oscillates at a frequency of 1>2p. Correspondingly, the filter response y1t2 will also oscillate at a frequency equal to the cutoff frequency (i.e., bandwidth) B of the low-pass filter, as indicated in Fig. 2.26. The maximum value of Si1u2 occurs at umax ϭ p and is equal to

p 1.8519 ϭ 11.1792 ϫ a b
2

We may show that the filter response y1t2 has maxima and minima at

tmax ϭ

ϮTϮ 1 2 2B

Si(u) y(t)

2.7 Ideal Low-Pass Filters

63

—␲2

0

– —␲2

–8␲ –6␲ –4␲ –2␲ 0 u

2␲ 4␲ 6␲ 8␲

FIGURE 2.25 The sine integral Si(u).

with

y1tmax2

ϭ

1 p

3Si1p2

Ϫ

Si1p

Ϫ

2pBT24

ϭ

1 p

3Si1p2

ϩ

Si12pBT

Ϫ

p24

where, in the second line, we have used the odd symmetric property of the sine integral. Let

p Si12pBT Ϫ p2 ϭ 2 11 Ϯ ¢2

9% 1.0
—B1

0

– —T2

0

—T2

Time t

FIGURE 2.26 Ideal low-pass filter response for a square pulse.

64

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

where ¢ is the absolute value of the deviation in the value of Si12pBT Ϫ p2 expressed as a fraction of the final value ϩp>2. Thus, recognizing that

Si1p2 ϭ 11.17921p>22

we may redefine y1tmax2 as

y1tmax2

ϭ

1 2

11.179

ϩ

1

Ϯ

¢2

ഠ 1.09 Ϯ 1 ¢ 2

(2.123)

For a time-bandwidth product BT W 1, the fractional deviation ¢ has a very small value, in which case we may make two important observations from Eq. (2.123):

1. The percentage overshoot in the filter response is approximately 9 percent. 2. The overshoot is practically independent of the filter bandwidth B.

The basic phenomenon underlying these two observations is called the Gibbs phenomenon. Figure 2.26 shows the oscillatory nature of the filter response and the 9 percent overshoot characterizing the response, assuming that BT W 1.
Figure 2.27, occupying pages 65 and 66, shows the filter response for four time-bandwidth products: BT ϭ 5, 10, 20, and 100, assuming that the pulse duration T is 1 second. Table 2.1 shows the corresponding frequencies of oscillations and percentage overshoots for these time-bandwidth products, confirming observations 1 and 2.

TABLE 2.1 Oscillation Frequency and Percentage Overshoot for Varying Time-Bandwidth Product

BT

Oscillation Frequency

Percentage Overshoot

5

5 Hz

9.11

10

10 Hz

8.98

20

20 Hz

8.99

100

100 Hz

9.63

Figure 2.28, occupying pages 67 and 68, shows the filter response for periodic squarewave inputs of different fundamental frequencies: f0 ϭ 0.1, 0.25, 0.5, and 1 Hz, and with the bandwidth of the low-pass filter being fixed at B ϭ 1 Hz. From Fig. 2.28 we may make the following observations:
᭤ For f0 ϭ 0.1 Hz, corresponding to a time-bandwidth product BT ϭ 5, the filter somewhat distorts the input square pulse, but the shape of the input is still evident at the filter output. Unlike the input, the filter output has nonzero rise and fall times that are inversely proportional to the filter bandwidth. Also, the output exhibits oscillations (ringing) at both the leading and trailing edges.
᭤ As the fundamental frequency f0 of the input square wave increases, the low-pass filter cuts off more of the higher frequency components of the input. Thus, when f0 ϭ 0.25 Hz, corresponding to BT ϭ 2, only the fundamental frequency and the first harmonic component pass through the filter; the rise and fall times of the output are now significant compared with the input pulse duration T. When f0 ϭ 0.5 Hz, corresponding to BT ϭ 1, only the fundamental frequency component of the input square wave is preserved by the filter, resulting in an output that is essentially sinusoidal.

2.7 Ideal Low-Pass Filters

65

y(t)

1.2
1.0
0.8
0.6
0.4
0.2
0
–0.2 –1.0 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1.0 Time t (s) (a)
1.2
1.0
0.8
0.6
0.4
0.2
0
–0.2 –1.0 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1.0 Time t (s) (b)
FIGURE 2.27 Pulse response of ideal low-pass filter for pulse duration T ϭ 1s and varying time-bandwidth 1BT2 product. (a) BT ϭ 5. (b) BT ϭ 10.

y(t)

66

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

y(t)

1.2
1.0
0.8
0.6
0.4
0.2
0
–0.2 –1.0 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1.0 Time t (s) (c)
1.2
1.0
0.8
0.6
0.4
0.2
0
–0.2 –1.0 –0.8 –0.6 –0.4 –0.2 0 0.2 0.4 0.6 0.8 1.0 Time t (s) (d)
FIGURE 2.27 (continued) (c) BT ϭ 20. (d) BT ϭ 100.

y(t)

2.7 Ideal Low-Pass Filters

67

1.0

y(t)

0

–1.0

–10 –8 –6 –4 –2

0

2

4

6

8 10

Time t (s)

(a)

1.0

y(t)

0

–1.0

–10 –8 –6 –4 –2

0

2

4

6

8 10

Time t (s)

(b)

FIGURE 2.28 Response of ideal low-pass filter to a square wave of varying frequency f0 . (a) f0 ϭ 0.1 Hz. (b) f0 ϭ 0.25 Hz.

68

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

1.0

y(t)

0

–1.0

–10 –8 –6 –4 –2

0

2

4

6

8 10

Time t (s)

(c)

1.0

y(t)

0

–1.0

–10 –8 –6 –4 –2

0

2

4

6

Time t (s)

(d)

FIGURE 2.28 (continued) (c) f0 ϭ 0.5 Hz. (d) f0 ϭ 1 Hz.

8 10

2.7 Ideal Low-Pass Filters

69

᭤ When the fundamental frequency of the input square wave is increased further to the high value f0 ϭ 1Hz, which corresponds to a time-bandwidth product BT ϭ 0.5, the dc component becomes the dominant output, and the shape of the input square wave is completely destroyed by the filter.
From these results, we draw an important conclusion: When using an ideal low-pass filter, we must use a time-bandwidth product BT Ն 1 to ensure that the waveform of the filter input is recognizable from the resulting output. A value of BT greater than unity tends to reduce the rise time as well as decay time of the filter pulse response.

᭿ APPROXIMATION OF IDEAL LOW-PASS FILTERS

A filter may be characterized by specifying its impulse response h1t2 or, equivalently, its transfer function H1f2. However, the application of a filter usually involves the separation of signals on the basis of their spectra (i.e., frequency contents). This, in turn, means that the design of filters is usually carried out in the frequency domain. There are two basic steps involved in the design of a filter:

1. The approximation of a prescribed frequency response (i.e., amplitude response, phase response, or both) by a realizable transfer function.
2. The realization of the approximating transfer function by a physical device.

For an approximating transfer function H1f2 to be physically realizable, it must represent a stable system. Stability is defined here on the basis of the bounded input-bounded output criterion described in Eq. (2.100) that involves the impulse response h1t2. To specify the corresponding condition for stability in terms of the transfer function, the traditional approach is to replace j2pf with s and recast the transfer function in terms of s. The new variable s is permitted to have a real part as well as an imaginary part. Accordingly, we refer to s as the complex frequency. Let HЈ1s2 denote the transfer function of the system, defined in the manner described herein. Ordinarily, the approximating transfer function HЈ1s2 is a rational function, which may therefore be expressed in the factored form

HЈ1s2 ϭ H1f 2 ƒ j2pfϭs

ϭ

K

1s 1s

Ϫ Ϫ

z121s p121s

Ϫ Ϫ

z22 Á 1s p22 Á 1s

Ϫ Ϫ

zm2 pn2

where K is a scaling factor; z1 , z2 , Á , zm are called the zeros of the transfer function, and p1 , p2 , Á , pn are called its poles. For a low-pass transfer function, the number of zeros, m, is less than the number of poles, n. If the system is causal, then the bounded
input–bounded output condition for stability of the system is satisfied by restricting all the
poles of the transfer function HЈ1s2 to be inside the left half of the s-plane; that is to say,

Re13pi42 Ͻ 0, for all i
Note that the condition for stability involves only the poles of the transfer function HЈ1s2; the zeros may indeed lie anywhere in the s-plane. Two types of systems may be distinguished, depending on locations of the m zeros in the s-plane:

᭤ Minimum-phase systems, characterized by a transfer function whose poles and zeros are all restricted to lie inside the left hand of the s-plane.
᭤ Nonminimum-phase systems, whose transfer functions are permitted to have zeros on the imaginary axis as well as the right half of the s-plane.

70

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Minimum-phase systems distinguish themselves by the property that the phase response of this class of linear time-invariant systems is uniquely related to the gain response.
In the case of low-pass filters where the principal requirement is to approximate the ideal amplitude response shown in Fig. 2.23, we may mention two popular families of filters: Butterworth filters and Chebyshev filters, both of which have all their zeros at s ϭ ϱ. In a Butterworth filter, the poles of the transfer function HЈ1s2 lie on a circle with origin as the center and 2pB as the radius, where B is the 3-dB bandwidth of the filter. In a Chebyshev filter, on the other hand, the poles lie on an ellipse. In both cases, of course, the poles are confined to the left half of the s-plane.
Turning next to the issue of physical realization of the filter, we see that there are two basic options to do this realization, one analog and the other digital:
᭤ Analog filters, built using (a) inductors and capacitors, or (b) capacitors, resistors, and operational amplifiers. The advantage of analog filters is the simplicity of implementation.
᭤ Digital filters, for which the signals are sampled in time and their amplitude is also quantized. These filters are built using digital hardware; hence the name. An important feature of a digital filter is that it is programmable, thereby offering a high degree of flexibility in design. In effect, complexity is traded off for flexibility.

2.8 Correlation and Spectral Density:
Energy Signals
In this section, we continue the characterization of signals and systems by considering the class of energy signals and therefore focusing on the notion of energy. (The characterization of signals and systems is completed in Section 2.9, where we consider the other class of signals, power signals.) In particular, we introduce a new parameter called spectral density, which is defined as the squared amplitude spectrum of the signal of interest. It turns out that the spectral density is the Fourier transform of the correlation function, which was first introduced under Property 13 in Section 2.2.

᭿ AUTOCORRELATION FUNCTION

Consider an energy signal x1t2 that, for the purpose of generality, is assumed to be com-

plex valued. Following the material presented under the correlation theorem (Property 13) in Section 2.2, we formally define the autocorrelation function of the energy signal x1t2 for

a lag t as

q

Rx1t2

ϭ

x1t2x*1t LϪq

Ϫ

t2

dt

(2.124)

According to this formula, the autocorrelation function Rx1t2 provides a measure of the similarity between the signal x1t2 and its delayed version x1t Ϫ t2. As such, it can be mea-
sured using the arrangement shown in Fig. 2.29. The time lag t plays the role of a scan-
ning or searching variable. Note that Rx1t2 is complex valued if x1t2 is complex valued. From Eq. (2.124) we readily see that the value of the autocorrelation function Rx1t2
for t ϭ 0 is equal to the energy of the signal x1t2; that is,

q

Rx102

ϭ

ƒx1t2ƒ2 dt
LϪq

2.8 Correlation and Spectral Density: Energy Signals

71

x(t)

Rx(␶)

Integrator

x*(t – ␶)

Adjustable delay ␶

Complex conjugation

FIGURE 2.29 Scheme for measuring the autocorrelation function Rx1t2 of an energy signal x1t2 for lag t.

᭿ ENERGY SPECTRAL DENSITY

The Rayleigh energy theorem, discussed under Property 14 in Section 2.2, is important because it not only provides a useful method for evaluating the energy of a pulse signal, but also it highlights the squared amplitude spectrum as the distribution of the energy of the signal measured in the frequency domain. It is in light of this theorem that we formally define the energy spectral density or energy density spectrum of an energy signal x1t2 as

cx1f 2 ϭ ƒ X1f 2 ƒ 2

(2.125)

where ƒX1f2ƒ is the amplitude spectrum of x1t2. Clearly, the energy spectral density cx1f2
is a nonnegative real-valued quantity for all f, even though the signal x1t2 may itself be com-
plex valued.

᭿ WIENER–KHITCHINE RELATIONS FOR ENERGY SIGNALS

Referring to the correlation theorem described in Eq. (2.53), let g11t2 ϭ g21t2 ϭ x1t2, where x1t2 is an energy signal and therefore Fourier transformable. Under this condition, the
resulting left-hand side of Eq. (2.53) defines the autocorrelation function Rx1t2 of the signal x1t2. Correspondingly, in the frequency domain, we have G11f 2 ϭ G21f 2 ϭ X1f 2, in which case the right-hand side of Eq. (2.53) defines the energy spectral density cx1f 2. On this basis, we may therefore state that given an energy signal x1t2, the autocorrelation func-
tion Rx1t2 and energy spectral density cx1f 2 form a Fourier-transform pair. Specifically, we have the pair of relations:

q

cx1f2 ϭ LϪq Rx1t2 exp1Ϫj2pft2 dt

(2.126)

and

q
Rx1t2 ϭ LϪq cx1f2 exp1j2pft2 df

(2.127)

Note, however, that the Fourier transformation in Eq. (2.126) is performed with respect to the adjustable lag t. The pair of equations (2.126) and (2.127) constitutes the Wiener–Khitchine relations for energy signals.
From Eqs. (2.126) and (2.127) we readily deduce the following two properties:

1. By setting f ϭ 0 in Eq. (2.126), we have
q
LϪq Rx1t2 dt ϭ cx102

72

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

which states that the total area under the curve of the complex-valued autocorrelation function of a complex-valued energy signal is equal to the real-valued energy spectral cx102 at zero frequency. 2. By setting t ϭ 0 in Eq. (2.127), we have
q
LϪq cx1f 2 df ϭ Rx102
which states that the total area under the curve of the real-valued energy spectral density of an energy signal is equal to the total energy of the signal. This second result is merely another way of starting the Rayleigh energy theorem.

EXAMPLE 2.13 Autocorrelation Function of Sinc Pulse From Example 2.4, the Fourier transform of the sinc pulse

x1t2 ϭ A sinc12Wt2

is given by

X1f 2 ϭ A recta f b

2W

2W

Since the rectangular function rect (f>2W) is unaffected by squaring, the energy spectral density of x1t2 is therefore

cx1f 2

ϭ

a

A

2
b

2W

f recta b
2W

Taking the inverse Fourier transform of cx1f2, we find that the autocorrelation function of the sinc pulse A sinc12Wt2 is given by

Rx1t2

ϭ

A2 2W

sinc12Wt2

(2.128)

which has a similar waveform, plotted as a function of the lag t, as the sinc pulse itself.

This example teaches us that sometimes it is easier to use an indirect procedure based on the energy spectral density to determine the autocorrelation function of an energy signal rather than using the formula for the autocorrelation function.

᭿ EFFECT OF FILTERING ON ENERGY SPECTRAL DENSITY

Suppose now the energy signal x1t2 is passed through a linear time-invariant system of transfer function H1f2, yielding the output signal y1t2 as illustrated in Fig. 2.20(a). Then, according to Eq. (2.109), the Fourier transform of the output y1t2 is related to the Fourier transform of the input x1t2 as follows:

Y1f2 ϭ H1f2X1f2

Taking the squared amplitude of both sides of this equation, we readily get

cy1f 2 ϭ ƒ H1f 2 ƒ 2cx1f 2

(2.129)

where, by definition, cx1f 2 ϭ ƒ X1f 2 ƒ 2 and cy1f 2 ϭ ƒ Y1f 2 ƒ 2. Equation (2.129) states that
when an energy signal is transmitted through a linear time-invariant filter, the energy spec-

tral density of the resulting output equals the energy spectral density of the input multiplied

2.8 Correlation and Spectral Density: Energy Signals

73

by the squared amplitude response of the filter. The simplicity of this statement emphasizes the importance of spectral density as a parameter for characterizing the distribution of the energy of a Fourier transformable signal in the frequency domain.
Moreover, on the basis of the Wiener–Khintchine equations (2.126) and (2.127) and the relationship of Eq. (2.129), we may describe an indirect method for evaluating the effect of linear time-invariant filtering on the autocorrelation function of an energy signal:
1. Determine the Fourier transforms of x1t2 and h1t2, obtaining X1f2 and H1f2, respectively.
2. Use Eq. (2.129) to determine the energy spectral density cy1f 2 of the output y1t2. 3. Determine Ry1t2 by applying the inverse Fourier transform to cy1f 2 obtained under
point 2.

EXAMPLE 2.14 Energy of Low-pass Filtered Version of Rectangular Pulse

A rectangular pulse of unit amplitude and unit duration is passed through an ideal low-pass filter of bandwidth B, as illustrated in Fig. 2.30(a). Part (b) of the figure depicts the waveform of the rectangular pulse. The amplitude response of the filter is defined by (see Fig. 2.30(c))

ƒH1f 2ƒ

ϭ

b

1, 0,

ϪB Յ f Յ B otherwise

The rectangular pulse constituting the filter input has unit energy. We wish to evaluate the effect of varying the bandwidth B on the energy of the filter output.
We start with the Fourier transform pair:
rect1t2 Δ sinc1f 2
which represents the normalized version of the Fourier-transform pair given in Eq. (2.10). Hence, with the filter input defined by
x1t2 ϭ rect1t2

Input x(t)

Ideal low-pass
filter (a)
x(t)
1.0

Output y(t)

t – —12 0 —12
(b)
|H(f )| 1.0

f FIGURE 2.30 (a) Ideal low-pass

–B

0

B

filtering. (b) Filter input. (c)

(c)

Amplitude response of the filter.

74

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

1.0

0.8

0.6

sinc2 (f )

0.4

0.2

0

0

1.0

2.0

3.0

Normalized frequency, f

FIGURE 2.31 Energy spectral density of the filter input x1t2; only the values for positive frequencies are shown in the figure.

its Fourier transform equals

X1f 2 ϭ sinc1f 2

The energy spectral density of the filter input therefore equals
cx1f 2 ϭ ƒ X1f 2 ƒ 2
ϭ sinc21f 2

(2.130)

This normalized energy spectral density is plotted in Fig. 2.31. To evaluate the energy spectral density cy1f 2 of the filter output y1t2, we use Eq. (2.129),
obtaining

cy1f 2 ϭ ƒ H1f 2 ƒ 2cx1f 2

ϭ b cx1f 2, ϪB Յ f Յ B

0,

otherwise

(2.131)

The energy of the filter output is therefore

q

Ey

ϭ LϪq

cy1f

2

df

B

ϭ LϪB

cx1f

2

df

B

ϭ2 L0

cx1f 2 df

B
ϭ 2 sinc21f 2 df L0

(2.132)

Since the filter input has unit energy, we may also view the result given in Eq. (2.132) as the ratio of the energy of the filter output to that of the filter input for the general case of a rectangular pulse of arbitrary amplitude and arbitrary duration, processed by an ideal low-pass

2.8 Correlation and Spectral Density: Energy Signals

75

1.0

0.8

Output energy Input energy

0.6

0.4

0.2

0

0

1.0

2.0

3.0

Normalized bandwidth of low pass filter

FIGURE 2.32 Output energy-to-input energy ratio versus normalized bandwidth.

filter of bandwidth B. Accordingly, we may in general write

Energy of filter output rϭ
Energy of filter input
B
ϭ 2 sinc21f 2 df L0

(2.133)

According to Fig. 2.30(b), the rectangular pulse applied to the filter input has unit duration; hence, the variable f in Eq. (2.133) represents a normalized frequency. Equation (2.133) is plotted in Fig. 2.32. This figure shows that just over 90 percent of the total energy of a rectangular pulse lies inside the main spectral lobe of this pulse.

᭿ INTERPRETATION OF THE ENERGY SPECTRAL DENSITY
Equation (2.129) is important because it not only relates the output energy spectral density of a linear time-invariant system to the input energy spectral density, but it also provides a basis for the physical interpretation of the concept of energy spectral density itself. To be specific, consider the arrangement shown in Fig. 2.33(a), where an energy signal x1t2 is passed through a narrow-band filter followed by an energy meter. Figure 2.33(b) shows the idealized amplitude response of the filter. That is, the filter is a band-pass filter whose amplitude response is defined by

ƒH1f2ƒ ϭ c 1,
0,

¢f

¢f

fc Ϫ 2 Յ ƒ f ƒ Յ fc ϩ 2

otherwise

(2.134)

We assume that the filter bandwidth ¢f is small enough for the amplitude response of the input signal x1t2 to be essentially flat over the frequency interval covered by the passband

76

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

Input x(t)

Narrow-band filter, H(f )

Filter output
y(t)

Energy meter

Output energy
Ey

(a)

|H(f )|

1.0

f

0

–fc

fc

⌬f

⌬f

(b)

␺y(f ) ␺x(fc)

f

0

–fc

fc

⌬f

⌬f

(c)

FIGURE 2.33 (a) Block diagram of system for measuring energy spectral density. (b) Idealized amplitude response of the filter. (c) Energy spectral density of the filter output.

of the filter. Accordingly, we may express the amplitude spectrum of the filter output by the approximate formula

ƒY1f2ƒ ϭ ƒH1f2ƒ ƒX1f2ƒ

¢f

¢f

ഠ c ƒ X1fc2 ƒ , fc Ϫ 2 Յ ƒ f ƒ Յ fc ϩ 2

0,

otherwise

(2.135)

Correspondingly, the energy spectral density cy1f2 of the filter output y1t2 is approximately related to the energy spectral density cx1f2 of the filter input x1t2 as follows:

cy1f2 ഠ c cx1fc2,

¢f

¢f

fc Ϫ 2 Յ ƒ f ƒ Յ fc ϩ 2

(2.136)

0,

otherwise

This relation is illustrated in Fig. 2.33(c), which shows that only the frequency components of the signal x1t2 that lie inside the narrow passband of the ideal band-pass filter

2.8 Correlation and Spectral Density: Energy Signals

77

reach the output. From Rayleigh’s energy theorem, the energy of the filter output y1t2 is

given by

q

Ey

ϭ LϪq

cy1f 2

df

q

ϭ2 L0

cy1f 2 df

In light of Eq. (2.136), we may approximate Ey as

Ey ഠ 2cx1fc2 ¢f

(2.137)

The multiplying factor 2 accounts for the contributions of negative as well as positive frequency components. We may rewrite Eq. (2.137) in the form

cx1fc2

ഠ

Ey 2¢f

(2.138)

Equation (2.138) states that the energy spectral density of the filter input at some frequency fc equals the energy of the filter output divided by 2¢f, where ¢f is the filter bandwidth centered on fc . We may therefore interpret the energy spectral density of an energy signal for any frequency f as the energy per unit bandwidth, which is contributed by frequency components of the signal around the frequency f.
The arrangement shown in the block diagram of Fig. 2.33(a) thus provides the basis for measuring the energy spectral density of an energy signal. Specifically, by using a variable band-pass filter to scan the frequency band of interest and determining the energy of the filter output for each midband frequency setting of the filter, a plot of the energy spectral density versus frequency is obtained. Note, however, for the formula of Eq. (2.138) to hold and therefore for the arrangement of Fig. 2.33(a) to work, the bandwidth ¢f must remain fixed for varying fc .

᭿ CROSS-CORRELATION OF ENERGY SIGNALS

The autocorrelation function provides a measure of the similarity between a signal and its own time-delayed version. In a similar way, we may use the cross-correlation function as a measure of the similarity between one signal and the time-delayed version of a second signal. Let x1t2 and y1t2 denote a pair of complex-valued energy signals. The cross-correlation function of this pair of signals is defined by

q

Rxy1t2

ϭ x1t2y*1t LϪq

Ϫ

t2

dt

(2.139)

We see that if the two signals x1t2 and y1t2 are somewhat similar, then the cross-correla-
tion function Rxy1t2 will be finite over some range of t, thereby providing a quantitative measure of the similarity, or coherence, between them. The energy signals x1t2 and y1t2 are
said to be orthogonal over the entire time interval if Rxy102 is zero — that is, if

q
x1t2y*1t2 dt ϭ 0 LϪq

(2.140)

Equation (2.139) defines one possible value for the cross-correlation function for a specified value of the delay variable t. We may define a second cross-correlation function for

78

CHAPTER 2 ᭿ FOURIER REPRESENTATION OF SIGNALS AND SYSTEMS

the energy signals x1t2 and y1t2 as

q

Ryx1t2

ϭ y1t2x*1t LϪq

Ϫ

t2

dt

(2.141)

From the definitions of the cross-correlation functions Rxy1t2 and Ryx1t2 just given, we obtain the fundamental relationship

Rxy1t2 ϭ Ry…x1Ϫt2

(2.142)

Equation (2.142) indicates that unlike convolution, correlation is not in general commu-

tative; that is, Rxy1t2 Ryx1t2. To characterize the cross-correlation behavior of energy signals in the frequency

domain, we introduce the notion of cross-spectral density. Specifically, given a pair of com-

plex-valued energy signals x1t2 and y1t2, we define their cross-spectral densities, denoted

by cxy1f 2 and cyx1f 2, as the respective Fourier transforms of the cross-correlation functions Rxy1t2 and Ryx1t2, as shown by

q

cxy1f 2

ϭ LϪq

Rxy1t2

exp1Ϫj2pft2

dt

(2.143)

and

q
cyx1f 2 ϭ LϪq Ryx1t2 exp1Ϫj2pft2 dt

(2.144)

In accordance with the correlation theorem (i.e., Property 13 of Section 2.2), we thus have

cxy1f 2 ϭ X1f 2Y*1f 2

(2.145)

and

cyx1f 2 ϭ Y1f 2X*1f 2

(2.146)

From this pair of relations, we readily see two properties of the cross-spectral density.

1. Unlike the energy spectral density, cross-spectral density is complex valued in general. 2. cxy1f 2 ϭ cy…x1f 2 from which it follows that, in general, cxy1f 2 cyx1f 2.

᭤ Drill Problem 2.15 Derive the relationship of Eq. (2.142) between the two cross-

correlation functions Rxy1t2 and Ryx1t2.

᭣

᭤ Drill Problem 2.16 Consider the decaying exponential pulse

exp1Ϫat2, t Ͼ 0

g1t2 ϭ c 1,

tϭ0

0,

tϽ0

Determine the energy spectral density of the pulse g1t2.

᭣

᭤ Drill Problem 2.17 Repeat Problem 2.16 for the double exponential pulse

exp1Ϫat2, t Ͼ 0

g1t2 ϭ c 1,

tϭ0

᭣

exp1at2, t Ͻ 0

