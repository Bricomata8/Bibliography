Centrality Measures on Big Graphs: Exact, Approximated, and Distributed Algorithms
Francesco Bonchi1,2 Gianmarco De Francisci Morales3
Matteo Riondato4
1ISI Foundation, Turin (Italy) 2Eurecat, Technological Center of Catalonia, Barcelona (Spain)
3Qatar Computing Research Institute, Doha (Qatar) 4Two Sigma Investments LP, NYC (USA)
WWW’16 – Montréal, April 11–15, 2016
1/200

Slides available at http://matteo.rionda.to/centrtutorial/
2/200

Acknowledgements
• Paolo Boldi • Andreas Kaltenbrunner • Evgenios M. Kornaropoulos • Nicolas Kourtellis • Eli Upfal • Sebastiano Vigna
3/200

Roadmap
• Introduction • motivation, history, and deﬁnitions • closeness and betweenness centrality • axioms: what to look for in a centrality measure
• Exact algorithms • exact algorithms on static graphs • exact algorithms on dynamic graphs
• Approximation algorithms • approximation algorithms on static graphs • approximation algorithms on dynamic graphs
• Conclusions • open problems and research directions
4/200

Introduction
5/200

Social network analysis
• Social network analysis is the study of social entities and their interactions and relationships
• The interactions and relationships can be represented with a network or graph, • each vertex represents an actor • each link represents a relationship
• From the graph, we can study the properties of its structure, and the role, position, and prestige of each social entity.
• We can also ﬁnd various kinds of sub-graphs, e.g., communities formed by groups of actors.
6/200

Centrality in networks
• Important or prominent actors are those that are extensively linked or involved with other actors
• A person with extensive contacts (links) or communications with many other people in the organization is considered more important than a person with relatively fewer contacts
• A central actor is one involved in many ties • Graph centrality is a topic of uttermost importance in social
sciences • Also related to the problem of ranking in the context of Web
Search: • Each webpage is a social actor • Each hyperlink is an endorsement relationship • Centrality measures provide a query independent link-based score of importance of a web page
7/200

History of centrality (in a nutshell)
• ﬁrst attempts in the late 1940s at MIT (Bavelas 1946), in the framework of communication patterns and group collaboration;
• in the following decades, various measures of centralities were proposed and employed by social scientists in a myriad of contexts (Bavelas 1951; Katz 1953; Shaw 1954; Beauchamp 1965; Mackenzie 1966; Burgess 1969; Anthonisse 1971; Czapiel 1974...) item a new interest raised in the mid-90s with the advent of search engines: a “reincarnation” of centrality.
Freeman, 1979
“several measures are often only vaguely related to the intuitive ideas they purport to index, and many are so complex that it is diﬃcult or impossible to discover what, if anything, they are measuring.”
8/200

Types of centralities
Starting point: the central vertex of a star is the most important! Why?
1 the vertex with largest degree; 2 the vertex that is closest to the other vertexes (e.g., that has
the smallest average distance to other vertexes); 3 the vertex through which all shortest paths pass; 4 the vertex with the largest number of incoming paths of
length k, for every k; 5 the vertex that maximizes the dominant eigenvector of the
graph adjacency matrix; 6 the vertex with highest probability in the stationary
distribution of the natural random walk on the graph. These observations lead to corresponding competing views of centrality.
9/200

Types of centralities
This observation leads to the following classes of indices of centrality:
1 measures based on distances [degree, closeness, Lin’s index]; 2 measures based on paths [betweenness, Katz’s index]; 3 spectral measures [dominant eigenvector, Seeley’s index,
PageRank, HITS, SALSA].
The last two classes are largely the same (even if that wasn’t fully understood for a long time.)
10/200

Geometric centralities

• degree (folklore): cdeg(x ) = d−(x )

• closeness (Bavelas, 1950): cclos(x ) = c(x ) =

1 y d(y ,x )

• Lin (Lin, 1976): cLin(x ) =

r (x )2 y d(y ,x )

where

r (x )

is

the

number

of vertexes that are co-reachable from x

• harmonic (Boldi and Vigna, 2013) charm(x ) =

1 y =x d(y ,x )

11/200

Path-based centralities

• betweenness (Anthonisse, 1971):

cbet(x ) = b(x ) =

y ,z=x ,σyz =0

σyz (x ) σyz

where

σyz

is

the

number

of shortest paths y → z, and σyz (x ) is the number of such

paths passing through x

• Katz (Katz, 1951): cKatz(x ) = t≥0 βt pt (x ) where pt (x ) is the number of paths of length t ending in x , and β is a

parameter (β < 1/ρ)

12/200

Spectral centralities
• dominant (Wei, 1953): cdom(x ) is the dominant (right) eigenvector of G
• Seeley (Seeley, 1949): cSeeley(x ) is the dominant (left) eigenvector of Gr
• PageRank (Brin, Page et al., 1999): cPR(x ) is the dominant (left) eigenvector of αGr + (1 − α)1T 1/n (where α < 1)
• HITS (Kleinberg, 1997): cHITS(x ) is the dominant (left) eigenvector of GT G
• SALSA (Lempel, Moran, 2001): cSALSA(x ) is the dominant (left) eigenvector of GcT Gr
Where G denotes the adjacency matrix of the graph, Gr is the adjacency matrix normalized by row, and Gc is the adjacency matrix normalized by column.
13/200

Closeness and Betweenness
14/200

Closeness centrality
Motivation It measures the ability to quickly access or pass information through the graph;
Deﬁnition (Closeness Centrality)
• closeness centrality c(x ) of a vertex x
1 c(x ) = y=x∈V d (y , x ).
• d(y , x ) is the length of a shortest path between y and x . • The closeness of a vertex is deﬁned as the inverse of the sum
of the Shortest Path (SP) distances between the vertex and all other vertexes of the graph. • When multiplied by n − 1, it is eﬀectively the inverse of the average SP distance.
15/200

Betweenness centrality
Motivation It measures the frequency with which a user appears in a shortest path between two other users.

Deﬁnition (Betweennes centrality)

• betweenness centrality b(x ) of a vertex x :

b(x ) =

σst (x )

s=x=t∈V σst

s =t

• σst : number of SPs from s to t
• σst (x ): how many of them pass through x

Example retrieved from Wikipedia 16/200

Betweenness centrality
• Can be deﬁned also for edges (similarly to vertexes) • Edges with high betweenness are known as “weak ties” • They tend to act as bridges between two communities
The strength of weak ties (Granovetter 1973)
• Dissemination and coordination dynamics are inﬂuenced by links established to vertexes of diﬀerent communities.
• The importance of these links has become more and more with the rise of social networks and professional networking platforms.
17/200

Weak ties
Bakshy et al. 2012 Weak links have a greater potential to expose links to new contacts that otherwise would not have been discovered.
18/200

Weak ties
Grabowicz et al. 2012 • Personal interactions are more likely to occur in internal links within communities (strong links) • Events or new information is propagated faster by intermediate links (weak links).
19/200

Girvan-Newman algorithm for community detection (Girvan and Newman 2002)
Hierarchical divisive clustering by recursively removing the “weakest tie”:
1 Compute edge betweenness centrality of all edges; 2 Remove the edge with the highest betweenness centrality; 3 Repeat from 1.
20/200

Comparison
Which vertex is the most central? • for Degree Centrality: • for Closeness Centrality: • for Betweenness Centrality:
21/200

Comparison
Which vertex is the most central? • for Degree Centrality: user A • for Closeness Centrality: • for Betweenness Centrality:
22/200

Comparison
Which vertex is the most central? • for Degree Centrality: user A • for Closeness Centrality: users B and C • for Betweenness Centrality:
23/200

Comparison
Which vertex is the most central? • for Degree Centrality: user A • for Closeness Centrality: users B and C • for Betweenness Centrality: user D
24/200

Visual Comparison
A Degree Centrality B Closeness Centrality C Betweenness Centrality
25/200

Axioms for centrality (Boldi and Vigna 2013)
26/200

Assessing
Question Is there a robust way to convince oneself that a certain centrality measure is better than another? Answer Axiomatization. . .
• . . . hard axioms (characterize a centrality measure completely) • . . . soft axioms (like the Ti axioms for topological spaces)
27/200

Sensitivity to size
Idea: size matters! Sk,p be the union of a k-clique and a p-cycle.
• if k → ∞, every vertex of the clique becomes ultimately strictly more important than every vertex of the cycle
• if p → ∞, every vertex of the cycle becomes ultimately strictly more important than every vertex of the clique
28/200

Sensitivity to density
Idea: density matters! Dk,p be made by a k-clique and a p-cycle connected by a single bidirectional bridge:
• if k → ∞, the vertex on the clique-side of the bridge becomes more important than the vertex on the cycle-side.
29/200

Score monotonicity
Adding an edge x → y strictly increases the score of y . Doesn’t say anything about the score of other vertexes!
30/200

Rank monotonicity
Adding an edge x → y . . . • if y used to dominate z, then the same holds after adding the edge • if y had the same score as z, then the same holds after adding the edge • strict variant: if y had the same score as z, then y dominates z after adding the edge
31/200

Rank monotonicity

Centrality Harmonic Degree Katz PageRank Seeley Closeness Lin Betweenness Dominant HITS SALSA

Monotonicity

General Strongly connected

Score Rank Score Rank

yes yes* yes

yes*

yes yes* yes

yes*

yes yes* yes

yes*

yes yes* yes

yes*

no no yes

yes

no no yes

yes

no no yes

yes

no no no

no

no no ?

?

no no no

no

no no no

no

Other axioms

Size yes only k only k no no no only k only p only k only k no

Density yes yes yes yes yes no no no yes yes yes
32/200

Kendall’s τ
Hollywood collaboration network
.uk (May 2007 snapshot)
33/200

Correlation
• most geometric indices and HITS are rather correlated to one another;
• Katz, degree and SALSA are also highly correlated; • PageRank stands alone in the ﬁrst dataset, but it is correlated
to degree, Katz, and SALSA in the second dataset; • Betweenness is not correlated to anything in the ﬁrst dataset,
and could not be computed in the second dataset due to the size of the graph (106M vertices).
34/200

Exact Algorithms
35/200

Outline
1 Exact algorithms for static graphs 1 the standard algorithm for closeness 2 the standard algorithm for betweenness 3 a faster betweenness algorithm through shattering and compression 4 a GPU-Based algorithm for betweenness
2 Exact algorithms for dynamic graphs 1 a dynamic algorithm for closeness 2 four dynamic algorithms for betweenness 3 a parallel streaming algorithm for betweenness
36/200

Exact Algorithms for Static Graphs
37/200

Exact Algorithm for Closeness Centrality
(folklore)
38/200

Exact Algorithm for Closeness

Recall the deﬁnition: c(x ) =

1 y=x d (x , y )

Fastest known algorithm for closeness: All-Pairs Shortest Paths • Runtime: O(nm + n2 log n)
Too slow for web-scale graphs! • Later we’ll discuss an approximation algorithm

39/200

A Faster Algorithm for Betweenness Centrality
U. Brandes
Journal of Mathematical Sociology (2001)
40/200

Why faster?
Let’s take a step back. Recall the deﬁnition
σst (x ) s=x=t∈V σst
s =t
• σst : no. of S (SPs) from s to t • σst (x ): no. of S from s to t that go through x We could: 1 obtain all the σst and σst (x ) for all x , s, t via APSP; and then 2 perform the aggregation to obtain b(x ) for all x . The ﬁrst step takes O(nm + n2 log n), but the second step takes. . . Θ(n3) (a sum of O(n2) terms for each of the n vertices).
Brandes’ algorithm interleaves the SP computation with the aggregation, achieving runtime O(nm + n2 log n)
I.e., it is faster than the APSP approach
41/200

Dependencies

Deﬁne: Dependency of s on v :

δs (v )

=

t =s =v

σst (v ) σst

Hence:

b(v ) = δs (v )
s =v

Brandes proved that δs (v ) obeys a recursive relation:

δs (v )

=

w :v ∈Ps (w )

σsv σsw

(1

+

δs (w ))

We can leverage this relation for eﬃcient computation of betweenness

42/200

Recursive relation

Theorem (Simpler form) If there is exactly one S from s to each t, then

δs (v ) =

(1 + δs (w ))

w :v ∈Ps (w )

Proof sketch: • The Sdag from s is a tree; • Fix t. v is either on the single S from s to t or not. • v lies on all and only the SPs to vertices w for which v is a predecessor (one S for each w ) and the SPs that these lie on. Hence the thesis.
The general version must take into account that not all SPs from s to w go trough v .
43/200

Brandes’ Algorithm

1 Initialize δs (v ) to 0 for each v , s and b(w ) to 0 for each w .

2 Iterate the following loop for each vertex s:

1 Run Dijkstra’s algorithm from s, keeping track of σsv for each encountered vertex v , and inserting the vertices in a max-heap

H by distance from s; 2 While H is not empty:

1 Pop the max vertex t in H;

2

For

each

w

∈ Ps (t),

increment

δs (w )

by

σsw σst

(1

+

δs

(t ));

3 Increment b(t) by δs (t);

44/200

Shattering and Compressing Networks for Betweenness Centrality
A. E. Sarıyüce, E. Saule, K. Kaya, Ü. V. Çatalyürek
SDM ’13: SIAM Conference on Data Mining
45/200

Intuition
Observations: • There are vertices with predictable betweenness (e.g., 0, or equal to one of their neighbors). We can remove them from the graph (compression) • Partitioning the (compressed) graph into small components allows for faster SP computation (shattering)
Idea: We can iteratively compress & shatter until we can’t reduce the graph any more.
Only at this point we run (a modiﬁed) Brandes’s algorithm and then aggregate the “partial” betweenness in diﬀerent components.
46/200

Introductory deﬁnitions
• Graph G = (V , E ) • Induced graph by V ⊆ V : GV = (V , E = V × V ∩ E ) • Neighborhood of a vertex v : Γ(v ) = {u : (v , u) ∈ E } • Side vertex: a vertex v such that GΓ(v) is a clique • Identical vertices: two vertices u and v such that either
Γ(u) = Γ(v ) or Γ(u) ∪ {u} = Γ(v ) ∪ {v }
47/200

Compression
Empirical / intuitive observations • if v has degree 1, then b(v ) = 0 • if v is a side vertex, then b(v ) = 0 • if u and v are identical, then b(v ) = b(w )
Compression: • remove degree-1 vertices and side vertices; and • merge identical vertices
48/200

Shattering
• Articulation vertex: vertex v whose deletion makes the graph disconnected
• Bridge edge: an edge e = (u, v ) such that G = (V , E \ {e}) has more components than G (u and v are articulation vertexes)
Shattering: • remove bridge edges • split articulation vertices in two copies, one per resulting component
49/200

Example of shattering and compression

1, 8v 2 V

a

b

b b'

c

d

c{d}

c{d,e} f

g

e

h

1

2

3

4

5

Figure 1: (1) a is a degree-1 vertex and b is an articulation
vertex. The framework removes a and create a copy b0 to represent b in the bottom component. (2) There is no degree-1, articulation, or identical vertex, or a bridge.
0

50/200

Issues
Issues to take care of when iteratively compressing & shattering: Example of issue A vertex may have degree 1 only after we removed another vertex: we can’t just remove and forget it, as its original betweenness was not 0. Example of issue When splitting an articulation vertex into component copies, we need to know, for each copy, how many vertices in other components are reachable through that vertex. ...and more
51/200

Solution
(Sketch) • When we remove a vertex u, one of its neighbors (or an identical vertex) v is elected as the representative for u (and for all vertices that u was a representative of) • We adjust the (current) values of b(v ) and b(u) to appropriately take into account the removal of u the details are too hairy for a talk. . . • When splitting articulation vertices or removing bridges, similar adjustments take place • Brandes’ algorithm is slightly modiﬁed to take the number of vertices that a vertex represents into consideration when computing the dependencies and the betweenness values
52/200

Speedup

“org.” is Brandes’ algorithm, “best” is compress & shatter

name
Power Add32 HepTh PGPgiant ProtInt AS0706 MemPlus Luxemb. AstroPh Gnu31 CondM05

Graph |V |
4.9K 4.9K 8.3K 10.6K 9.6K 22.9K 17.7K 114.5K 16.7K 62.5K 40.4K

Epinions Gowalla bcsstk32 NotreDame RoadPA Amazon0601 Google WikiTalk

131K 196K 44.6K 325K 1,088K 403K 875K 2,394K

Time (in sec.)

|E|

org. best Sp.

6.5K

1.47 0.60 2.4

9.4K

1.50 0.19 7.6

15.7K

3.48 1.49 2.3

24.3K 10.99 1.55 7.0

37.0K 11.76 7.33 1.6

48.4K 43.72 8.78 4.9

54.1K 19.13 9.28 2.0

119.6K 771.47 444.98 1.7

121.2K 40.56 19.41 2.0

147.8K 422.09 188.14 2.2

175.6K 217.41 97.67 2.2

geometric mean 2.8

711K 2,193

839 2.6

950K 5,926 3,692 1.6

985K

687

41 16.5

1,090K 7,365

965 7.6

1,541K 116,412 71,792 1.6

2,443K 42,656 36,736 1.1

4,322K 153,274 27,581 5.5

4,659K 452,443 56,778 7.9

geometric mean 3.8

4.2 Shat each graph improveme with o, do, BFS orderi is articulat is side vert order of tec
We mea tation time the runtim Brandes’ a 7 stacked b scribed abo edges rema phase are g ures, compo 6 combinat
53/200

Composition of runtime

• Preproc is the time needed to compress & shatter, Phase 1 is SSSP, Phase 2 is aggregation
• Diﬀerent column for diﬀerent variants of the algorithm (e.g., only compression of 1-degree vertices, only shattering of edges)
• the lower the better

1.4

1

Phase 1

1.2

Phase 2

Preproc

1

Relative time

0.8

0.6

0.4

0.2

0

54/200

Betweenness Centrality on GPUs and Heterogeneous Architectures
A. E. Sarıyüce, K. Kaya, E. Saule, Ü. V. Çatalyürek
GPGPU ’13: Workshop on General Purpose Processing Using GPUs
55/200

Parallelism
• Fine grained: single concurrent BFS • Only one copy of auxiliary data structures • Synchronization needed • Better for GPUs, which have small memory • Coarse grained: many independent BFSs • Sources are independent, embarrassingly parallel • More memory needed • Better for CPUs, which have large memory
56/200

GPU
A GPU is especially well-suited to address problems that can be expressed as data-parallel computations - the same program is executed on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic operations to memory operations. Because the same program is executed for each data element, there is a lower requirement for sophisticated ﬂow control, and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches.1

1docs.nvidia.com/cuda/cuda-c-programming-guide/index.html

57/200

Execution model
• One thread per data element • Thread scheduled in blocks
with barriers (wait for others at the end) • Program runs on the whole data (kernel) • Minimize synchronization • Balance load • Coalesce memory access
58/200

Intuition
• GPUs have huge number of cores • Use them to parallelize BFS • One core per vertex, or one core per edge • Vertex-based parallelism creates load imbalance for graphs
with skewed degree distribution • Edge-based parallelism requires high memory usage • Use vertex-based parallelism • Virtualize high-degree vertices to address load imbalance • Reduce memory usage by removing predecessors lists
59/200

Diﬀerence

u

v1

...

...

vk

...

Vertex-based BFS

u

v1

...

...

vk

...

Edge-based BFS

60/200

m ⌧ n2 of them. To store the same information, Jia et al.

Vertex-based used an array of size m. For an edge e 2 E, indexed as in the order of CSR adj array, they set P[e] to 1 if e is a

successor-predecessor edge and leave it 0, otherwise.

Let u be a vertex at level `, when u is being processed in

the backward-step kernel, it gathers all [v]s from its succes-

sor vertices, i.e., all v 2 V such that Pv[u] = 1. As Figure 1

• For each level, for each

shows, the vertex-based approach requires n+m+1 memory in total to store the graph. Here and in the rest of the paper,

vertex in parallel

the memory usage of each graph representation is given in terms of the number of entries it contains.

• If vertex is on level

Algorithm 2: Vertex: vertex-based parallel BC

···

• For each neighbor,

`0 .Forward phase while cont = true do

adjust P and σ

cont false .Forward-step kernel

for each u 2 V in parallel do

•

Atomic update on σ needed

1 2

if d[u] = ` then for each v 2 (u) do

3

if d[v] = 1 then

(multiple paths can be

d[v] ` + 1, cont true else if d[v] = ` 1 then Pv[u] 1

discovered concurrently)

4

if d[v] = ` + 1 then [v] atomic [v] + [u]

` `+1

• While backtracking, if u ∈ P(v ) accumulate δ(u) = δ(u) + δ(v )
• Possible load imbalance if

···

.Backward phase

while ` > 1 do

` `1

.Backward-step kernel

for each u 2 V in parallel do

if d[u] = ` then

5

for each v 2 (u) do

6

if Pv[u] = 1 then [u] [u] + [v]

.Update bc values by using Equation (5)

···

degree skewed

3.2 Edge-based parallelism
A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. That is there are many vertices with a degree that is lower than av-

is in th will be value i
Alth based tions, (u, v) ation o succes curren per su based both m vertex lescing
Algo
··· ` .For whil
c . f
1
` ··· .Bac whil
` . f
2
.Upd ···
3.3
The and th 61/a2to0m0 ic

in the order of CSR adj array, they set P[e] to 1 if e is a

value in is array is either the same or one more.

Edge-based successor-predecessor edge and leave it 0, otherwise. Let u be a vertex at level `, when u is being processed in

Although the updates in the backward-phase of the vertexbased approach are handled without using atomic instruc-

the backward-step kernel, it gathers all [v]s from its successor vertices, i.e., all v 2 V such that Pv[u] = 1. As Figure 1 shows, the vertex-based approach requires n+m+1 memory in total to store the graph. Here and in the rest of the paper,

tions, in edge-based parallelism, when Pv[u] = 1 for an edge (u, v) which is currently being processed, the update operation on [u] must be atomic. Because, there can be other successor-predecessor edges (u, v0) 2 E being processed con-

the memory usage of each graph representation is given in

currently by other threads. In total, two atomic operations

For each level, for each edge • terms of the number of entries it contains.

per successor-predecessor relationship are needed in edge-

based parallelism. Hence, the edge-based approach uses

in parallel Algorithm 2: Vertex: vertex-based parallel BC

both more memory and more atomic operations than the

···

vertex-based one. But it beneﬁts from better memory coa-

If edge endpoint is on level • ` 0
.Forward phase

lescing and better load distribution.

while cont = true do
Same as above... • cont false
.Forward-step kernel

Algorithm 3: Edge: edge-based parallel BC ···

for each u 2 V in parallel do

`0

1 2

•if dW[fuo]r=hea`ictlhheevn2ba(uc) kdotracking, if

.Forward phase while cont = true do

3

if d[v] = 1 then

u ∈ P (v ) accumulate d[v] ` + 1, cont true else if d[v] = ` 1 then Pv[u] 1

cont false .Forward-step kernel
for each (u, v) 2 E in parallel do

4

δ(u) = δ(u) + δ(v ) if d[v] = ` + 1 then [v] atomic [v] + [u]

` `+1

atomically · · ·
.Backward phase

while ` > 1 do

• Multiple ` ` 1
.Backward-step kernel

edges

can

try

to

for each u 2 V in parallel do

5

if

update d[u] = ` then for each v 2

δ(u)cdoo ncurrently

6

if Pv[u] = 1 then [u] [u] + [v]

More memory (edge-based • .Update bc values by using Equation (5)
···

1

if d[u] = ` then

· · · .same as vertex-based forward step

` `+1

···

.Backward phase

while ` > 1 do

` `1

.Backward-step kernel

for each (u, v) 2 E in parallel do

if d[u] = ` then

2

if Pv[u] = 1 then [u] atomic [u] + [v]

.Update bc values by using Equation (5)

···

layout) and more atomic

3.2 Edgeo-bpaeserdaptaioranllselism
A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. That is there are many vertices with a degree that is lower than average, and there are some with very high degrees, yielding a very skewed degree distribution. Social networks we have

3.3 Vertex virtualization for BC
The vertex-based parallelism su↵ers from load balancing, and the edge-based parallelism uses more memory and more atomic operations. Here, we propose a vertex virtualization technique to alleviate both of these problems at the same time. The technique replaces the high-degree vertices with

62/200

Vertex virtualization

• AKA, edge batching, hybrid between vertex- and edge-based
• Split high degree vertices into virtual ones with maximum degree mdeg
• Equivalently, pack up to mdeg edges belonging to the same vertex together
• Very small mdeg = 4
• Need additional auxiliary maps

Algorithm 4: Virtual: BC with virtual vertices

···

`0

.Forward phase

while cont = true do

cont false

.Forward-step kernel

for each virtual vertex uvir in parallel do

u vmap[uvir]

if d[u] = ` then

1

for each v 2 vir(uvir) do

2

if d[v] = 1 then

d[v] ` + 1, cont true

3

if d[v] = ` + 1 then [v] atomic [v] + [u]

` `+1

··· .Backward phase while ` > 1 do
` `1 .Backward-step kernel for each virtual vertex uvir in parallel do
u vmap[uvir]
if d[u] = ` then sum 0

4

for each v 2 (u) do

5

if d[v] = ` + 1 then sum sum + [v]

6

[u] atomic [u] + sum

.Update bc values by using Equation (5) ···

In the forward phase, each thread processes the edges of a virtual vertex uvir. The real vertex u is reached via vmap and [u] is used to update the number of shortest paths to

e a n c A
4. I o H m i
3.3.1
As w sentati improv mentio beled the sam of adjs warp a of war lesced.
The lows b ptrs a the sam the vir shows By usi 63/of20v0to

Beneﬁts
• Compared to vertex-based: • Reduce load imbalance
• Compared to edge-based: • Reduce number of atomic operations • Reduce memory footprint
• Predecessors stored implicitly in the Sdag level (reduced memory usage)
• Memory layout can be further optimized to coalesce latency via striding: • Distribute edges to virtual vertices in round-robin • When accessed in parallel, they create faster sequential memory access pattern
64/200

Speedup"wrt"CPU"1"thread"

Results

11"

10"

GPU"vertex" GPU"edge"

9"

GPU"virtual"

8"

GPU"stride"

7"

6"

5"

4"

3"

2"

1"

0"

Speedup oveFr iBgurarend4e:s’CoonmCpParUisoonn orefaGl PgrUapihmspwleitmhe3n2t-actoiorensG. PU (s = 1k, . . . , 100k)

1.8E+07%

1.4E+08%

•

Results

1c.6oEm+07p% utedCPUo%1n%thlyreaod%n

a

sam1.2pE+le08%of

GPU%
sources

and

1.4E+07%

extrapol1a.2tEe+0d7% linearly

1.0E+08%

5.3 Heter
In the last of using CPU together for B mance obtain ing only GPU presented) an same time (la later (heterog CPU to dedic
The source average paral allel CPU imp e cient. (Fig is a factor of 2 not parallelism
The GPU S mance than t (amazon0601, wiki-Talk), w performance o loc-gowalla). the parallel C tation reach t in the a6v5/e2r0a0g

Exact Algorithms for Dynamic Graphs
66/200

A Fast Algorithm for Streaming Betweenness Centrality
O. Green, R. McColl, D. A. Bader
SocialCom ’12: International Conference on Social Computing
67/200

Intuition
• Make Brandes’ algorithm incremental • Keep additional data structures to avoid recomputing partial
results • Rooted Sdag for each source s ∈ V • Depth in the tree for t = distance of t from s
• Re-run parts of modiﬁed Brandes’ algorithm on edge update • Support only edge addition (on unweighted graphs)
68/200

Data structures
• One Sdags for each source s ∈ V , which contains for each other vertex t ∈ V : • Distance dst , paths σst , dependencies δs (t), predecessors Ps (t) • Additional per-level queues for exploration
• On addition of edge (u, v ), let dd = |dsu − dsv |: • dd = 0 same level • dd = 1 adjacent level • dd > 1 non-adjacent level
69/200

Same level addition

Figure 1. Insertion of edge e = (u, v) connects two vertices that are on the same level in the BFS
s d=1

• dd = 0

d=2

• Edge creates no new
shortest paths d=i
• No change to betweenness
due to this source

e

u

v

Figure 2. Insertion of edge e = (u, v) connects two vertices that are in adjacent levels in BFS tree of root s. The new change its position in the given BFS tree.

s

70/200

Adjacent level addition

• dd = 1

Figure 2. Insertion of edge e = (u, v) connects two vertices that are in adjacent levels in BFS tree of root s. The new

Let u = u, u = v •

hcihgahnge its position lion wthe given BFS tree.

• Edge creates new shortest

paths

s

d=1

• Sdag unchanged

d=2

• Changes in σ conﬁned to

sub-dag rooted in ulow

• Changes in δ also spread d=i

above to decrease old

d=i+1

dependency and account for

uhigh

w e
ulow

new dependency

• Example: w and predecessors have now only 1/2 of dependency on sub-dag rooted in ulow
71/200

ei.o...u, splvy.,

QBF S empty queue; for level 1 to V do

Algorithm eAdhjaacveent Level InsQer[ltieovnel] empty queue;

During exploration: During backtracking: nepewnttinhoeiendss.gsseubbseetwctdte•i[oePvnn][vvw]eertiNpcreo0ests,-etvThnoat2tutcha8hreeVeadi;lng,oavrditj2hacme8nVfto;lrevinelsserotifnga dgorhilvnnnltohecseeewntshsrhB=tne)ttarnaieeecFncp=orrwetuegaSeetssetdwhsc.aih(shnniuotadghhtrnhotiguegeireetdSˆˆrneohl[onntgPtop)[[urwdvuaqqe+tablu[g]etuuloue=sheo1weeets,wsl.wuuott]aovT2ha]weeey•••sahet]fv-iuuhnto[isehsverllBtDeooibˆ]hFMEpsditww,Fenesho[esvreauinsSiwptrxsn!!ome[aioli2ulqronclortcscetwhtir;σcurctk.eoQQe8eieao]daHgneanVsvrn+[Bh.tvroaudseeide];rwnPFidri[;ciedssurno,setSioFPi.vatolag;tmhofierlfTg[weeotrue.e,htsd]rdoletot]2oiha;tfs.twfheBhtrvtaueW]hetFnen;ieriSunceinrtnmevgsthtsdeerfiberereocaeerttnertiirtericooaooeosntulnesffl.low
psohrireetniostheptmnbhsteeeeu2dd. oiTn-chtoehwdejehufsioftdfloileoﬁlerorqcQwtauhatieeinlnolugnonenteLfewvoeeirmgmahtmphlbgetaooQysprr.i;dstwehoumdoocf acvnoddbeoemfaoduendwiilnl

mma 3. Given vertex shortest paths from

tuihfleowdr,o[iwfothtt],e[=wso,n]a(l=ydre[vveNth]rote+itcv-e1Tesro)ttuihtccahehtseewnfdoilultnhhdaevinne

for all neighbor w of v do

if d[w] = (d[v] + 1) then

if t[w] = Not-Touched then

enqueue w ! QBF S;

enqueue w ! Q[d[w]];

•

t[w] Down;

d[w] d[v] + 1;
•dP [Fwi]x δdPa[nv]d; b

els•e Recurse up the whole

dP ˆ[w]

[Swˆd][wa] g+dPd[Pw[]v+];

dP

[v];

Stage 3 - modiﬁed dependency accumulation [ˆv] 0, v 2 8V ; level V ; while level>0 do
while Q[level] not empty do dequeue w Q[level]; for all v 2 P [w] do if t[v] =Not-Touched then

BFS subtree starting seerrstailnsgtarting at ulow

at ulow einnqsu’esuBeFwS !treeQ. TBhFeSB; FS can onlyenmqouveeudeowwn!s’sQB[FdS[wtr]e]e;.

enqueue v ! Q[level 1]; t[v] Up;

ﬁldnseitnoioof ntae1.

ˆs(v) is

the new

numt[bwe]r of d[w]

sDhoorwtenst; d[v] +

p1a;ths

to v.

mnvthsae.rfbe)rareoSrerntaeirttirgsyaioaoegoonuddlneffpl.P. d1T,ahwtoeefhdeAnirufelgmtdohbPreeirt[rehvmo]afirse2ntneheˆˆweel[sswwn(epvudd]pa)mPPathtbh[[sewwsrˆw,]]o[oiswflt(lhvn]ebe)+rdd.ewwPPAmdissf[[hPeatvwoei]n[rri];tvtte+Sa]rsi;ettnamdepgPdaaetihn[i1nvss,];

ˆ[v] [v];

ˆ[v]

ˆ[v]

+

ˆ[v] ˆ[w]

(1

+

ˆ[w]);

if t[v] = Up ^(v 6= uhigh _ w 6= ulow) then

ˆ[v] ˆ[v]

[v] [w]

(1

+

[w]);

if w 6= r then

CB[w] CB[w] + ˆ[w] [w];

ﬁdnvueneenrtirthdttwiiiecocieeinblnssle.2g.inˆnsi(nSwvg[)ˆthvaoii]sgfleetShtle3ae0gvn-e,eevwlm3>,2ao0ˆcds8cd(iuVvﬁom)e;udisllaeditvnieveitepliaeslunizmdeVedfno;trcoyvzeearrtcoecxfuovmr. ulatfioo[rvn]lvev2eˆlV[vd],olvev2el8V

1; ;

72/200

Non-adjacent level addition

Figure 3. IFnisgeurtrieon3.of Iendsgeertieon=o(fue,dvg)e ceon=ne(cuts, vtw) ocovnenrteicctess ttwhoat vaerreticneost tahdajtacaernetntot eaadcjhacoe vertex is moverdte(xpuisllemdouvpe)d, (vp.uFlloerdoutph)e,r vs.ceFnoarriootshearnsecnetniareriosusbatnreenmtiroevessubatsreceanmboveesseeans cinan(

s

s

d=1

d=1

d=1

• dd > 1

d=2

d=2

d=2

• Edge creates new shortest

paths

uhigh

uhigh

•

Changes

to

Sdag

d=i
(new

d=i

d=i

distances)

d=i+1 d=i+1

d=i+1

• Algorithm only sketched (most details missing) d=i+c d=i+c

ulow

ulow

(a) Before ed(ag)e Binesfeorrteioend.ge insertion.

(b) After edg(ebi)nAs addition7a3l/v2ea0rdt0idcie

Complexity
• Time: O(n2 + nm) ← same as Brandes’ • In practice, algorithm is much faster • Space: O(n2 + nm) ← higher than Brandes’ • For each source, a Sdag of complexity n + m
74/200

90
graphs.

Speedup

Results

R-MAT graph speedup
300

250

200

150

100

50

0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 Density percentage(%)

scale 10

scale 11

scale 12

Speedup over Brandes’ on synthetic graphs (n = 4096) (b) Speedup of the streaming algorithm for R-MAT dense graphs.

75/200

Conclusions
• Up to 2 orders of magnitude speedup • Super-quadratic space bottleneck
76/200

QUBE: a Quick algorithm for Updating BEtweenness centrality
M. Lee, J. Lee, J. Park, R. Choi, C. Chung
WWW ’12: International World Wide Web Conference
77/200

Intuition
• No need to update all vertices when a new edge is added • Prune vertices whose b does not change • Large reduction in all-pairs shortest paths to be re-computed • Support both edge additions and removals
78/200

Minimum Cycle Basis
• G = (V , E ) undirected graph • Cycle C ⊆ E s.t. ∀v ∈ V , v incident to even number of edges
in C • Represented as edge incidence vector ν ∈ {0, 1}|E|, where
ν(e) = 1 ⇐⇒ e ∈ C • Cycle Basis = set of linearly independent cycles • Minimum Cycle Basis = on weighted graph with non-negative
weights we, cycle basis of minimum total weight w (C ) = i w (Ci ) where w (Ci ) = e∈Ci we
79/200

Basis is a cMyicnlime umbaCsyicsleCBasoisfExmaminpliemum total

hich minimizes w(C) =

v i=1

w(Ci),

where

w

w .e • Three cycle basis sets: {C1, C2}, {C1, C3}, {C2, C3}

• If all edges have same weight we = 1, MCB = {C1, C2}

v1

v3

c3

c1

c2

v5

v2

v4

80/200

Minimum Union Cycle
• Given a MCB C and minimum cycles Ci ∈ C • Let VCi be the set of vertices induced by Ci • Recursively union two VCi if they share at least one vertex • The ﬁnal set of vertices is a Minimum Union Cycle MUC • MUC s are disjoint sets of vertices • MUC (v ) = the MUC which contains vertex v
81/200

Connection Vertex
• Articulation Vertex = vertex v whose deletion makes the graph disconnected
• Biconnected graph = graph with no articulation vertex • Vertex v is an articulation vertex ⇐⇒ v belongs to two
biconnected components • Connection Vertex = vertex v that
• is an articulation vertex • has an edge to vertex w ∈ MUC (v )
82/200

connected component of Gj . VGj is the set of vertices

Connection Vertex Example be how to compute the between-
mentioned in Section 4, after an edge e(vi, vj ), we guarantee that s of vertices in M U C(vi) can be we ﬁnd the reduced set of vertices,

Gj . In Figure 5, G1, G2 and G3 represent disconnected su
graphs originated from the deletions of connection vertice v1, v2, and v3, respectively. G12 and G22 are connected com ponents of G2. If the dotted edge is inserted, M U CU {v1, v2, v3, v4} and connection vertices of M U CU to G1, G

, we need to eﬃciently calculate

and G3 are v1, v2, and v3, respectively.

ss centralities of the vertices in

dated vertices belong. From now

G1

MUC as M U CU .

G

•

If (v3, v4) is added,

MUC
c(v1)

(G1v3)

=0G.5’{v1,

v2,

v3,

v4}

v5

• cv(v12,) v2,1v3 ar0e.5connection cv(ve3r)tice0.s5 of 0M.5UC (v3)

|VG1|=5 G3 v3
|VG3|=6

v1

MUCU

v4 v2

•

ccsL((uvve45b))t gGra3i 0.p5bhe

t0h.5e disconnected generated by

G2

of the depenrdeemncoyvionfgthvie be-

|VG2|=4

v6

ss centralities of the vertices in a e occurs is expensive, because in involves computation of all pair ph. In the previous section, we et of vertices whose betweenness d. This set of vertices is referred

v8

v5

v7

G21 |VG21|=3

G22 |VG22|=1

Figure 5: An example of updating the betweenne

centrality (vertices in G1 and G3 are omitted.)
83/200

Finding MUCs
• Finding an MCB is well studied • Kavitha, Mehlhorn, Michail, Paluch. “A faster algorithm for
minimum cycle basis of graphs”. ICALP 2004 • Finding MUC from MCB relatively straightforward (just union
sets of vertices) • Also ﬁnd connection vertices for each MUC • All done as a preprocessing step • Need to be updated at runtime
84/200

4.3 UpdatinUgpdMatUingCMs UCs – Addition

v2

b v12

v6

v9

v1 a v4

v7

v8

v10

v3

v5

c v11

(a) Insertion
• Adding a does not aﬀect the MUC (endpoints in the same

MUC )

•

Adding b
MUC )c

crvea2tes

a new
b

MvU6C

(endpoints

dovn9ot

belong

to

a

• Adding c merges two MUC s (merge MUC s of vertices on the

S betvw1een endpoinvt4s)

v7

v8 a v10

85/200

Updatin(ag)MI nUsCesrt–ioRnemoval

c v2 b

v1

v4

v6 v7

v9 v8 a v10

v3

v5

v11

(b) Deletion
• Removing a destroys the MUC (cycle is removed → no
Fbigicuonrneect3ed: cAomnpoenexnat)mple of updating MUC
• Removing b does not aﬀect the MUC (MUC is still
biconnected)
We •noRwemopvrinegsecnstpliotsutrhetMecUhCniinqtuweo (osningmle vaeirntetxaianppineagrs ain set of MUCs, aallsSetbeotwf eceonnenndepcotiinotsn) vertices for each M U C and dis-
86/200

CENTRALITY
In this sBecettiwone,ewnne edsesscCriebnethraolwitytoDceopmepnudteentchye between-
nictnhheseas••esnrbgtcOtHreieoeeotonqdnwnwbluy.teieerroTvveeaureenslphrrind,tntdeeyeeearrxewslteeecesvfstodoasicmhirloneueonpsn,reiutdtsaoetre.isffantttlgtAehiphrteaasiewletMlhmseecsUdeoeﬁtngCnfontetsrvdtiaeohoelt(irfenthvtitereiiehedcs,eservtfsieoujnodr)pif,nudtSthawcehMeteceeedMtdgUgisorUeueaCnnaCtpdr(h4opavs,fotinii)vlantleeftctsreaetnnrticeheeabadsnet, which w•eShreorfteersttpoatahss tMo vUerCtic,ews oeutnseideedthteoMeUﬃCciently calculate and up•daStheorttehset pbatehtswteheant npaessssthcreonugthratlhietiMesUCof the vertices in the MUC to which the updated vertices belong. From now on, we simply denote such MUC as M U CU .

v1

G'’

v3

v4

v2

G v5

G G’'

c(v1) 1

0.5

c(v2) 1

0.5

c(v3) 0.5 0.5

c(v4) 3.5 0.5

c(v5) 0

Figure 4: An example of the dependency of the be- 87/200

Betweenness Centrality outside the MUC

• Let s ∈ VGj , t ∈ MUC , • Let j ∈ MUC be a connection vertex to subgraph Gj • Each vertex in Sjt is also in Sst • Therefore, betweenness centrality due to vertices outside the
MUC :

bo(v ) =

|VGj | σst
0

if v ∈ {Sjt \ t} otherwise

88/200

Betweenness Centrality trough the MUC
• Let s ∈ VGj , t ∈ VGk , • Let j ∈ MUC be a connection vertex to subgraph Gj • Let k ∈ MUC be a connection vertex to subgraph Gk • Each vertex in Sjk is also in Sst • Therefore, betweenness centrality due to paths through the
MUC :

bx (v ) =

|VGj ||VGk | σst
0

if v ∈ Sjk otherwise

More caveats apply for subgraphs that are disconnected, as every path that connects vertices in diﬀerent connected component passes through v
89/200

Updating Betweenness Centrality

b(v ) = bMUC (v ) + bo(v ) +

bx (v )

Gj ⊂G

Gj ,Gk ⊂G

90/200

Networks

QUBE algorithm
April 16–20, 2012, Lyon, France

n-

Algorithm 3: QUBE(M U CU )

U
st

input : M U CU - Minimum Union Cycle that updated vertices belong to

output : C[vi] - Updated Betweenness Centrality Array

hs

1 begin

es

2 Let SP be the set of all pair shortest paths in M U CU ;

✷

3 Let C[vi] be an empty array, vi ∈ M U CU ;

4 SP , C[vi] ← Betweenness() ;

ess

5 for each shortest path <va, . . . , vb> in SP do

6

if va is a connecting vertex then

7

Ga := Subgraph connected by a connection

ot

vertex va ;

or

8

for each vi ∈ <va, . . . , vb> - {vb} do

9

C [vi ]

:=

C [vi ]

+

|VGa | |SP (va,vb)|

;

10

if vb is also a connecting vertex then

11

Gb := Subgraph connected by a

91/200

es

2 Let SP be the set of all pair shortest paths in M U CU ;

✷

3 Let C[vi] be anQeUmpBtyEaarrlagyo, rviith∈mMUCU ;

4 SP , C[vi] ← Betweenness() ;

ess

5 for each shortest path <va, . . . , vb> in SP do

6

if va is a connecting vertex then

7

Ga := Subgraph connected by a connection

ot

vertex va ;

or

8

for each vi ∈ <va, . . . , vb> - {vb} do

9

C [vi ]

:=

C [vi ]

+

|VGa | |SP (va,vb)|

;

10

if vb is also a connecting vertex then

11

Gb := Subgraph connected by a

3) 12

connection vertex vb ; for each vi ∈ < va, . . . , vb > do

13

C [vi ]

:= C[vi]

+

|VGa |·|VGb | |SP (va,vb)|

;

es
14 15
e-

if Ga is disconnected then

C[va] := C[va] + |VGa |2 −

n l=1

(|VGla

|2

)

ts

a

h

is

4)2. Then for each shortest path between the vertices in 92/200

QUBE + Brandes
• QUBE is a pruning rule that reduces the search space for betweenness recomputation
• Can be paired with any existing betweenness algorithm to compute bMUC
• In the experiments, Brandes’ is used • Quantities computed by Brandes’ (e.g., σ) reused by QUBE
for bo and bx
93/200

Results

Time(ms)

400000 350000 300000 250000 200000 150000 100000
50000 0

QUBE+Brandes Brandes

10 20 30 40 50 60 70 80

Proportion

(c) |V|=5000 Update time as a function of the percentage of vertices of the graph in
the updated MUC for synthetic Erdös-Rényi graphs (n = 5000)
94/200

b c

hhttttpp::////swtwuﬀw..mcse.tcaoﬁrnlteelrl..Cecdoomun/c/cliounufsoriodsenusms/cps/685/2002fa/

Time (ms, log scale)

10000000

1000000

100000

10000

1000

100

10

1 Eva

QUBE+Brandes 106

Brandes

256326

Erdos02 12289 486267

Erdos972 Pgp 8640 270419 297100 3538417

Epa 34056 227158

Contact 1150801 4600805

Wikivote 361362 1082843

CAGrQc 101895 210831

Foni•grueI(rmbaeip-lcr7oodvn:aentmeTacethnetedndebespese)tnwdseehinghnleysosn csternucttruarleitoyf thuepgdraapthe time
• From 2 orders of magnitude (best) to 2 times (worst) faster up inthTanabBleran2dsehs’ows how fast the updatable version of the
95/200

Incremental Algorithm for Updating Betweenness Centrality in
Dynamically Growing Networks
M. Kas, M. Wachs, K. M. Carley, L. R. Carley
ASONAM ’13: International Conference on Advances in Social Networks analysis and Mining
96/200

Intuition
• Extend an existing dynamic all-pairs shortest path algorithm to betweenness
• G. Ramalingam and T. Reps, “On the Computational Complexity of Incremental Algorithms,” CS, Univ. of Wisconsin at Madison, Tech. Report 1991
• Relevant quantities: number of shortest paths σ, distances d, predecessors P
• Keep a copy of the old quantities while updating • Support only edge addition (on weighted graphs)
97/200

Edge update
• Compute new shortest paths from updated endpoints (u, v ) • If a new shortest path of the same length is found, updated
number of paths as
σst = σst + σsu × σvt
• If a new shorter shortest path to any vertex is found, update d, clear σ
• Betweenness decreased if new shortest path found • Edge betweenness updates backtrack via DFS over Ps (t)
b(w ) = b(w ) − σsw × σwt/σst
98/200

Edge update
• Complex bookkeeping: need to consider all aﬀected vertices which have new alternative shortest paths of equal length (not covered in the original algorithm)
• Amend P during update propagation → concurrent changes to the Sdag
• Need to track now-unreachable vertices separately • After having ﬁxed d, σ, b, increase b due to new paths • Update needed ∀s, t ∈ V aﬀected by changes (tracked from
previous phase) • Betweenness increase analogous to above decrease
99/200

relations between High-REenseurlgtsy Physics researchers) [22] P2P Communication Network (P2P file sharing) [23].

TABLE 5- PERFORMANCE OF INCREMENTAL BETWEENNESS ALGORITHM

REAL LIFE NETWORKS.

Avg

Network

D? #(N) #(E) Speedup Affect%

SocioPatterns U 113 4392 9.58 x

38.26%

FB-like

D 1896 20289 18.48 x 27.67%

HEP Coauthor U 7507 19398 357.96 x 42.08%

P2P Comm.

D 6843 7572 36732 x 0.02%

TABLE 6- NSEpTeWedOuRpKovSeTrABTrIaSnTdIeCs’SoCnOrLeaLlE-wCoTrEldDgOraNphRsEAL LIFE NETWORK

Std.

Avg.

Dev. Diam Path Clus N•etwSopreekdup depMenadxsBotnwtopAovlogg.icBatlwchaBrtawcteristeictser(e.g., Len. Coef SocidoiPaamtteetrenr,sclust.42c3o.e4ﬀ7.)7 36.752 51.139 3 1.65 0.53

FB-like

146171.2 2848.62 9753.8 8 3.19 0.08

HEP Coauthor 820318.2 13553.29 38024 15 5.74 0.46 100/200

hops. However,Cionmwpeaigrhistoend nweittwhoQrkUs,BwEhen an edge from x to
y is inserted, it is still necessary to check the paths of

equivalent length before ruling out all previously known

shortest paths between x and y.

TABLE 7- PERFORMANCE COMPARISON OF QUBE AND OUR PROPOSED

ALGORITHM.

Incremental

Network Type

#(Node) #(Edge) QuBE Betweenness

Eva [24] Ownership 4457 4562 2418.17 25425.87

CAGrQc [25] Collaboration 4158 13422 2.06

67.86

We compare our algorithm against the QuBE algorithm

using theSpdeeadtuapseotvsertBhreanadeust’hinorcsomupsaeridsoninwitthheQiUrBpEaper [11]. We

select two of their datasets: the dataset on which QuBE

pppeee••rrrfffDAooobrrrammmtoauassstent1ttcshheofeerrdorleemborsewuotshlfettemsstQ(afEUgo(vCnrBiatAE1)u,0dGp0eaarpQfnraeadsrcnt)ed.trhoTtemhaabndulaeQptdaU7asBetEertespooonnrtswthhtehicenheatwvQeourraBkgsEe.

For purposes of fair comparison, the updates included

shrinking network updates as well, which were handled by an

incremental shrinking network update algorithm we have

101/200

Betweenness Centrality – Incremental and Faster
M. Nasre, M. Pontecorvi, V. Ramachandran
MFCS ’14: Mathematical Foundations of Computer Science
102/200

Intuition
• Keep Sdag for each vertex • Re-use information from Sdag of updated edge endpoints • Adding new edges will not make old edges part of a S • Support only edge addition (on weighted graphs)
103/200

Main Result
• Let E ∗ = e ⊆ E be the set of edges that are part of any
e∈S
shortest path • Let m∗ = |E ∗| and ν∗ = max |Sdagv | the maximum number
v ∈V
of edges in shortest paths through any single vertex v • n < ν∗ < m∗ < m • After incremental update, betweenness can be recomputed in
• O(ν∗n) time using O(ν∗n) space • O(m∗n) time using O(n2) space • Bounded by O(mn + n2) • Logarithmic factor better than Brandes’ (on weighted graphs)
104/200

Lemma 1
• Edge (u, v ) ∈ Sxu ∧ (u, v ) ∈ Svx as edge weights are positive
105/200

Lemma 2

• Updates to σ and d in constant time • Need to update P to complete Sdag update

106/200

Sdag Update for each edge (a, b) in either DAG, it decides whether to include it in H based
on the value of f lag(s, b). For the updated edge (u, v) there is a separate check
(Steps 9–10). The algorithm clearly takes time linear in the size of DAG(s) and DAG(v), i.e., O(ν∗) time.

Algorithm 3. Update-DAG(s, w′(u, v))

Input: DAG(s), DAG(v), and f lag(s, t), ∀t ∈ V . Output: An edge set H after decrease of weight on edge (u, v), and Ps′(t), ∀t ∈ V −{s}.
1: H ← ∅.

2: for each v ∈ V do Ps′(v) = ∅. 3: for each edge (a, b) ∈ DAG(s) and (a, b) ̸= (u, v) do

4: if f lag(s, b) = UN-changed or f lag(s, b) = NUM-changed then

5:

H ← H ∪ {(a, b)} and Ps′(b) ← Ps′(b) ∪ {a}.

6: for each edge (a, b) ∈ DAG(v) do

7: if f lag(s, b) = NUM-changed or f lag(s, b) = WT-changed then

8:

H ← H ∪ {(a, b)} and Ps′(b) ← Ps′(b) ∪ {a}.

9: if f lag(s, v) = NUM-changed or f lag(s, v) = WT-changed then

10: H ← H ∪ {(u, v)} and Ps′(v) ← Ps′(v) ∪ {u}.

Lemma 3. Let H be the set of edges output by Alg. 3. An edge (a, b) ∈ H if
an•d oUnlNy -ifch(aa,nb)ge∈dD→AG′d(sd).= 0

Pr•oofN. SUinMce-cthheanugpeddate→is dadn i=ncr1emental update on edge (u, v), we note that

f(oi)•r πas′nWbyisbT,a-acshshhoaorntretgseetstdppa→tahthidnπdsG′ b.>fTroh1mersefotorebeivnerGy′

can edge

be on

of two such a

types: path is

present

in

DAG(s) and each such edge is added to H in Steps 3–5 of Alg. 3.

(ii) π′ is not a shortest path in G. However, since π′ is a shortest path in G′, 107/200

Edge Update every s, t pair, the updated d′(s, t) and σs′ t, as well as f lag(s, t). Using Lemma 2,
we spend constant time for each s, t pair, hence O(n2) time for all pairs. In
Step 3, instead of Dijkstra’s algorithm, we run Alg. 3 to obtain the updated predecessor lists Ps′(t), for all s, t. This step requires time O(ν∗) for a source s, and O(ν∗ · n) over all sources. The last diﬀerence is in Step 4: we place in the stack S the vertices in reverse topological order in DAG′(s), instead of non-
increasing distance from s. This requires time linear in the size of the updated DAG. Thus the time complexity of Edge-Update is O(ν∗ · n).

Algorithm 4. Edge-Update(G = (V, E), w′(u, v))

Input: updated edge with w′(u, v), d(s, t) and σst, ∀ s, t ∈ V ; DAG(s), ∀ s ∈ V . Output: BC′(v), ∀ v ∈ V ; d′(s, t) and σs′ t ∀ s, t ∈ V ; DAG′(s), ∀ s ∈ V . 1: for every v ∈ V do BC′(v) ← 0.
for every s, t ∈ V do compute d′(s, t), σs′ t, f lag(s, t). // use Lemma 2
2: for every s ∈ V do

3: Update-DAG(s, (u, v)).

// use Alg. 3

4: Stack S ← vertices in V in a reverse topological order in DAG′(s).

5: Accumulate-dependency(s, S).

// use Alg. 2

Undirected Graphs. For an undirected G, we construct the corresponding directed graph GD in which every undirected edge is replaced with 2 directed edges. An incremental update on an undirected edge (u, v) is equivalent to two edge updates on (u, v) and (v, u) in GD. Thus, Theorem 1 holds for undirected graphs.

Space Eﬃcient Implementation. In order to obtain O(n2) space complexity,
we do not store the SSSP DAGs rooted at every source. Instead, we only store the edge set E∗. After an incremental update on edge (u, v) we ﬁrst construct 108/200

Space-Eﬃcient Variant O(n2)
• Do not store the Sdag • Store only E ∗ • Updated Sdag can be build in O(m∗) time
• Time O(m∗ n) • Compute E ∗ from E ∗, then Sdags from E ∗ • Space O(m∗ + n2) to store E ∗ and n2 distances d(s, t) and shortest paths σst
109/200

received attention, and these results for incremental and in some cases, decre-
Comparison mental, BC are listed in the table below. All of these results except [16] deal with
unweighted graphs as opposed to our results, which are for the weighted case. Further, while all give encouraging experimental results or match the Brandes worst-case time complexity, none prove any worst-case improvement. As mentioned in the Introduction, BC is also widely used in weighted networks (see [4,18,31,32]); however, only the heuristic in Kas et al. [16], which has no worstcase bounds, addresses this version.

Paper

Year Space

Time

Weights Update Type

Brandes static [3] 2001 O(m + n)

Lee et al. [21] 2012 O(n2 + m)

Green et al. [12] 2012 O(n2 + mn)

Kourtellis+ [19] 2014 O(n2)

Singh et al. [10] 2013

–

O(mn) Heuristic O(mn) O(mn) Heuristic

NO Static Alg. NO Single Edge NO Single Edge NO Single Edge NO Vertex update

Brandes static [3] 2001 O(m + n) O(mn + n2 log n)

Kas et al. [16] 2013 O(n2 + mn) Heuristic

This paper 2014 O(ν∗ · n)

O(ν∗ · n)

This paper 2014 O(n2)

O(m∗ · n)

YES YES YES YES

Static Alg. Single Edge Vertex Update Vertex Update

Our ﬁrst algorithm, which takes time O(ν∗ · n) in a weighted graph even for a vertex update, improves on all previous results when ν∗ = o(m). By slightly relaxing the time complexity to O(m∗·n), we are also able to match the best space
complexity in any of the previous results, while matching their time complexities and improving on all of them when m∗ = o(m).

110/200

Conclusions
• Provably faster than Brandes’ on weighted graphs • However m∗ can be large in practice • No experiments • Hard to parallelize (need to access pairs of Sdag at a time) • Still has main bottleneck of most algorithms: O(n2) memory
111/200

Incremental Algorithms for Closeness Centrality
A. E. Sarıyüce, K. Kaya, E. Saule, U. V. Çatalyürek
IEEE BigData ’13: International Conference on Big Data
112/200

Intuition

• Algorithm with pruning based on level diﬀerence (similar to Green et al.)

• Additional pruning by bi-connected decomposition (similar to QUBE)

• Applied to closeness centrality (still solves APSP)

• Reminder: closeness centrality

• c(v ) =

1 d(u, v )

u∈V

113/200

search (BFS) from s, computes the distances to the other
vertices and far[s],Pthreelsiumminoaf rtiheesdistances which are
different than 1. As the last step, it computes cc[s]. Since
• BeastBsFtSattiackaeslgOor(imthm+ nO)(tnimme), tainmden SSSPs are required in
total, the complexity follows.

Algorithm 1: CC: Basic centrality computation

Data: G = (V, E) Output: cc[.] 1 for each s 2 V do

.SSSP(G, s) with centrality computation Q empty queue

d[v] 1, 8v 2 V \ {s} Q.push(s), d[s] 0

far[s] 0 while Q is not empty do
v Q.pop() for all w 2 G(v) do
if d[w] = 1 then Q.push(w)

d[w] d[v] + 1

far[s] far[s] + d[w]

cc[s] = return cc[.]

1 far[s]

the updated that for a v then cc[s] = such vertice
Theorem two vertice cc[s] = cc0
Proof: will not ch connected t dG(s, v)| is new, larger connected insertion in
Case 1: d u–v P 0 t is
dG(s, u) = with one le
Case 2
dG(s, u) < path in G0
114/200

ering*with*level*diCﬀaseesrences*
pon#edge#inserAon,#breadth[ﬁrst#search#tree#of#eac ertex#will#change.#Three#possibiliAes:#

ase#1#a•nUdsu#a2l #cwaseisl:l#dndo=t#0c, hdda=ng1,ed#dc>c#1of#s!#

No#need#to#apply#SSSP#from#them#

ust#Case#3#

How#to#ﬁnd#such#verAces?#

115/200

the update algorithm in case of an edge insertion is given

B

in Algorithm 2P. rWunhienng a-n leevdgeel duivﬀeirseinncseerted/deleted, to are

employ the ﬁlter, we ﬁrst compute the distances from u and cla

v to all other vertices. And, we ﬁlter the vertices satisfying

L

the statement of Theorem 1.

for

Algorithm 2: Simple work ﬁltering
Data: G = (V, E), cc[.], uv Output: cc0[.] G0 (V, E [ {uv})

the C
typ tra

du[.] SSSP(G, u) . distances from u in G dv[.] SSSP(G, v) . distances from v in G

C.

for each s 2 V do if |du[s] dv[s]|  1 then cc0[s] = cc[s]

T be

else

of

. use the computation in Algorithm 1 with G0
return cc0[.]

eac if d use

dis

B. Utilization of Special Vertices

ma
116/200

What#if#thPeru#gnrinagp-hb#ihcoanvneec#taedrAcocmuplaonAeontns #points?#
v AuB
Chan•gIef g#rianph#Aha#scaartincu#lcathionapnoigntes #cc#of#any#vertex#in#A# Com•puChAanngegin#tAhcean#cchhanagne cgloese#nfeossro#fuan#yisv#eertenx oinuBgh#for#ﬁn chan•gaIetdisdse#edfnofoourrg#thhaetnorecysot#mvopfeuBtre)tcehaxng#evf#oirnu#B(co#(nsctaontnfsacttaornist#factor
117/200

FilterinMg*awinittha*ibniicnognbnieccotnende*cctoemd pdoencoemntpso* sition
•  Maintain#the#biconnected#decomposiAon#

• Assume edge (b, d) added

• Similar to QUBE

IEEE BigData’13

Incremental*Algorithms*for*Closeness*Centrality*

edge b-d added
11 118/200

sssp hybridization
• BFS can be performed in two ways • Top-down: process vertices at distance d to ﬁnd vertices at
distance d + 1 • Bottom-up: after vertices at distance d are found, process all
unprocessed vertices to see if they are neighbors of the frontier • Top-down is better for initial rounds, bottom-up better for
ﬁnal rounds • Hybridization: use best option at each round
119/200

by ﬁltering using leveFlrdaicftfieornenocfesc.asTehserefore, level ﬁltering is more useful for the graphs having characteristics similar
toPsrmoablla-wboilriltdy*nDetiwstorrikbsu. Jon*

0.6"

0.4"

Pr(X"="0)" Pr(X"="1)"

0.2"

Pr(X">"1)"

0"

F|eddigG•gu  e(rdBue••uia,4ﬀvrw.eMPs#i)rrssooeThsbnathoadceebwdeddiGlbesg#itta#ed(yhisrnv.esdat,i#orswsdeth#irt)seiohb|tawurrsiietynbiteotchuno#aecAsfaetoodhssrniersle#estoerv#wfieb#clrhuaadtesniiﬀenodesn#roaewmnnoc#fee#evrdiadangdrnveidae#oibssmtl#ieign#avostaefe#rrdlietaevbwdelhe#l#eXn

= an

Filtering with identical vertices is not as useful as the 120/200

Speedup

Graph hep-th PGPgiantcompo astro-ph cond-mat-2005 Geometric mean soc-sign-epinions loc-gowalla web-NotreDame amazon0601 web-Google wiki-Talk DBLP-coauthor Geometric mean

CC 1.413 4.960 14.567 77.903 9.444 778.870 2,267.187 2,845.367 14,903.080 65,306.600 175,450.720 115,919.518 13,884.152

Time (secs)

CC-B

CC-BL

0.317

0.057

0.431

0.059

9.431

0.809

39.049

5.618

2.663

0.352

257.410

20.603

1,270.820

132.955

579.821

118.861

11,953.680

540.092

22,034.460 2,457.660

25,701.710 2,513.041

18,501.147

288.269

4,218.031

315.777

CC-BLI 0.053 0.055 0.645 4.687 0.306 19.935
135.015 83.817
551.867 1,701.249 2,123.096
251.557 273.036

CC-BLIH 0.048 0.045 0.359 2.865 0.217 6.254 53.182 53.059
298.095 824.417 922.828 252.647 139.170

CC-B 4.5
11.5 1.5 2.0 3.5 3.0 1.8 4.9 1.2 3.0 6.8 6.2 3.2

Speedups

CC-BL CC-BLI

24.8

26.6

84.1

89.9

18.0

22.6

13.9

16.6

26.8

30.7

37.8

39.1

17.1

16.8

23.9

33.9

27.6

27.0

26.6

38.4

69.8

82.6

402.1

460.8

43.9

50.8

CC-BLIH 29.4
111.2 40.5 27.2 43.5 124.5 42.6 53.6 50.0 79.2 190.1 458.8 99.7

Filter time (secs)
0.001 0.001 0.004 0.010 0.003 0.041 0.063 0.050 0.158 0.267 0.491 0.530 0.146

Table II

EXECUTION TIMES IN SECONDS OF ALL THE ALGORITHMS AND SPEEDUPS WHEN COMPARED WITH THE BASIC CLOSENESS

CENTRALITY ALGORITHM CC. IN THE TABLE CC-B IS THE VARIANT WHICH USES ONLY BCDS, CC-BL USES BCDS AND FILTERING

WITH LEVELS, CC-BLI USES ALL THREE WORK FILTERING TECHNIQUES INCLUDING IDENTICAL VERTICES. AND CC-BLIH USES

Speedup of 2 orders of magnitude •

ALL THE TECHNIQUES DESCRIBED IN THIS PAPER INCLUDING SSSP HYBRIDIZATION.

The impact of level ﬁltering can also be seen on Figure 5. NPRP grant 4-1454-1-233 from the Qatar National Research

60% of th•e edMgesoinstthlye mdauinebitcoonnleectvedelcopmrpuonnenint dgo Fund (a member of Qatar Foundation). The statements made
not change the closeness values of many vertices and the herein are solely the responsibility of the authors.

Biconnected decomposition and hybridization also give good updates th•at are induced by their addition take less than 1
second. The remaining edges trigger more expensive updates
speedups upon insertion. Within these 30% expensive edge insertions,

REFERENCES
[1] S. Beamer, K. Asanovic´, and D. Patterson. Direction-optimizing breadth-ﬁrst search. In Proc. of Supercomputing, 2012.

using identical vertices and SSSP hybridization provide a

[2] U. Brandes. A faster algorithm for betweenness centrality. Journal

signiﬁcant improvement (not shown in the ﬁgure). Better Speedups on Real Temporal Data: The best

of Mathematical Sociology, 25(2):163–177, 2001. [3] S. Y. Chan, I. X. Y. Leung, and P. Lio`. Fast centrality approximation
in modular networks. In Proc. of CIKM-CNIKM, 2009.

speedups are obtained on the DBLP coauthor network which

[4] D. Eppstein and J. Wang. Fast approximation of centrality. In Proc.

of SODA, 2001.

121/200

Scalable Online Betweenness Centrality in Evolving Graphs
Scalable Online Betweenness Centrality in Evolving Graphs
N. Kourtellis, G. De-Francisci-Morales, F. Bonchi
TKDE: IEEE Transactions on Knowledge and Data Engineering (2015)
122/200

Intuition
• Incremental, exact, space-eﬃcient, out-of-core, parallel version of Brandes’
• Handles edge addition and removal • Vertex and edge betweenness • Scalable to graphs with millions of vertices
123/200

Algorithm

• Run a modiﬁed Brandes’ on the initial graph

• Keep track of d, σ, δ in a Sdag (no P)

• On edge update, adjust the Sdag and update b

, NO. Y, APRIL 2014

3

er-

Input: Graph G(V, E) and edge update stream ES

of

Output: V BC0[V 0] and EBC0[E0] for updated G0(V 0, E0)

er ld is up is wo

Step 1: Execute Brandes’ alg. on G to create & store data structures for incremental betweenness.
Step 2: For each update e2ES, execute Algorithm 1. Step 2.1 Update vertex and edge betweenness. Step 2.2 Update data structures in memory or disk for next edge addition or removal.
Fig. 1: The proposed algorithmic framework.

is v 2 V . It runs in two phases. During the ﬁrst phase, 124/200

Data structure
• Sdags for each source s ∈ V • Sdag contains d, σ, δ for each other vertex t ∈ V • No predecessors P, re-scan neighbors and use d to ﬁnd them
• Save memory - space complexity O(n2) • Fixed size data structure - eﬃcient out-of-core management • Same time complexity O(nm) - in practice, makes the
algorithm faster
125/200

Pivot
• When adding or removing an edge, consider dd = |dsu − dsv | • Three cases: dd = 0, dd = 1, dd > 1 (analogous to Green et
al.) • Last case dd > 1 hardest - structural changes in Sdag • Find pivots to discover structural changes
Deﬁnition (Pivot) Let s be the current source, let d and d be the distance before and after an update, respectively, we deﬁne pivot a vertex p | d(s, p) = d (s, p) ∧ ∃ w ∈ Γ(p): d(s, w )=d (s, w ).
• Pivots’ distance unchanged → use as starting points to correct distances
126/200

Finding pivots

• Addition - pivots in sub-dag rooted in uL = v • vertices moved closer must be reachable from uL • Can be found during exploration while ﬁxing σ

• Removal - pivots may be anywhere
• Need one exploration to ﬁnd them
VOL. X, NO•. Y, ANPeReILd20s1e4parate exploration from found pivots to correct 5 distances

dded at

(a)

s

0

s

(b)

ype of se:

uH δ δ

k

δ
uH

δ

δ

uL

k+1

uL

r

e same

BFS k+2

BFS1

δ

pv

ch that s, and

δ

δ

BFS2

δ

δ

127/200

X, NO. Y, APRIL 2014

Structural changes

7

n .

Before
case 1 (a)

After Addition

(b)

(c)

jY<i

After Deletion

(d)

(e)

(f)

X Yi Xj i X Yj i

Y X Yj i

case 2

X j>i Y

X

i X j i X Yj i Xj i X j i

Y

Y

Y X Y j>i

Y

w

X j i+2

Fig. 3: Possible conﬁgurations of an edge before and after a•n Cuopndsaidteer txha∈tΓc(ayu),sexscsatnruecitthuerrablecha asnibgliensg. or a predecessor
of y • Each case requires slightly diﬀerent combination of corrections o((b••clneiansdfyRteoehieris1ems2cd0bpoos,)iv)a.vvσ,ameoI,lyftrδfeeoyiidnrslemc1ﬁavadordes,sevld2te1aeeid,ddsn2cxtfttawoh,nebotbuhBelteeFovwSBpetaFlaismsSftwienzfroeo.rdrt.tth.(bfepuxeirvfuoto(hpcrtaeedysrateithesee2xscp(iabl)li,ldoninidretgaitt6woii)oof.inlnxl) 128/200

Scalability
• Out-of-core - stream Sdag from disk • In-place update on disk to minimize writes
• Columnar storage for d, σ, δ • Read only d, skip rest if dd = 0
• Parallelization - coarse grained over s • Implementation in MapReduce • Amenable to Apache Storm/Flink/Spark

129/200

Results
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. Y, AP

CDF

1k-MP 1k-DO 1k-MO
1 0.8 0.6 0.4 0.2
0 1

10k-MP 10k-DO 10k-MO
10 Speedup

GrQc-MP

GrQc-DO

GrQc-MO

1

0.8

0.6

0.4

0.2

0

100

1

we-MP we-DO we-MO

10

100

Speedup

Fig. S5p:eeSdpupeeodveur pBraonfdetsh’eonfrsaynmtheewticoarnkd’srea3l gvraeprhssio(nn s= o1n0k)syn-

thetic and real graphs executed on single machines (ad-

dition).

• In-memory (M-) version faster than out-of-core (D-)

ce•ssoWrsithloisuttspr(eMdPec)e,ss(o2r) (i-nO)malewmayos rfyastweritthhoanutwipthrepdreedceecsessosorrss lists ((-MP)O), (3) on disk without predecessors lists (DO).

te an

O 130/200

Results
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. Y, APRIL 2014

CDF

1 1k

we

0.8 10k

fb

0.6 100k

sd

1000k

ep

0.4

dblp

0.2

amz

0

1

10

100 1

10

100 1

10

DATA ENGINEERING, VOL. X, NO. Y, APRIL 20(1a4)Speedup(additions,synthetic) (b)Speedup(removals,synth1e2tic)

(c)Speedup(additions,r

Fig. 6: Speedup of DO version on synthetic/real graphs executed on a MapReduc

we

Wall-clock computational time per new edge (s)

ifsoramnaesifndbpnccerewasheenin

the variability of the framework’s peraccessing the disks on the MapReduce

102

clusdtebrlp. This effect is partly due to contention on disk

10k
100 edges 200 edges 300 edges

acceasms zon the cluster caused by concurrent jobs, as well

101

as increased computation load per machine and source.

1

10

10O0 ve1rall, the use1o0f parallel e1x0e0cut1ion leads to10improved 100 100

) (b)Speedup(removals,synthestpice) edup(cs)Sfopereldaurpg(earddgitrioanpsh,rseatlh) at wou(ldd)Sbpeeeidmupp(oresmsiobvlaelst,roeal)

1

10

process on a single machine. As an additional beneﬁt,

Mappers (a)

Computational time for workload ratio (s)

ofkntshsoeynnfrtthahmeeSgtierpcMwa/epaoreeprhdakRsl’uesgpd(rpnuaeocpr=ev-hsewaTr1elhxgMeBFeeoiacgrr)lusiaus1te0tohnr2etemuddgp6eeoo(tsbnni’rs)eaafsdsohMMuirom12c100aawe000oipkpdleesadduRRggrweetteessh-ddateoouullfCcc--ceetDchloccoeFcllruukoepssftttreievesmr1rvep02ife(reooa.seurdidosdrueniptmeixoooopnvvnes12e1ar/000ri00ls0rmkeeyoeBddggmfenreenassetontdhv.dgeaeBeltsssyi’.)c.

and

1r0e4 al
103

10k
r=10 r=20 r=30

Wall-clock computational time per new edge (s)
Wall-clock computational time per new edge (s)

woritonoomunculcadlorecrnhabetdineensnteimttjiooao••pbnniosdm,sosOSapsniosrbpoulwdeverticeees-tedlkoodl. fu-imissnnpplciccegeorrdehueeridtaaaep11lussyn0001iipnnv1tshggeopdigretrthshho2eideeeporusoMggnparrtrpt3haapdo01e0sipap0rneesnchh⇡d(crgaare)ssswseli1iaezzho2seseee.fnstfIuo1rmn0fopa0r1mdota0hmdt0gi1io0snnk1k⇡0gci111ttva0oe0Mus1eder01d,tg0iectetv0oehskes,eM,⇡ravbp3ttspe0ehpie0r4crceestea5di(eegbuc.me)dsessWseue,dphttiaehhinn1ees000

102 10

15

20

25

Mappers (c)

Fig. 7: (a-b) Com1p31u/t2a0t0io

Conclusions
• Fully dynamic (addition and removal) • Algorithm can scale to graphs with realistic size • Ideal horizontal scalability • O(n2) space bottleneck
132/200

Approximation Algorithms
133/200

Why should we look for an approximation?
Static graphs • many interesting networks are web-scale; • computing the exact centralities can be extremely expensive; • is there a real reason (i.e., application) to require the exact values?
Dynamic graphs • exact centralities change at all times; • not worth chasing for highly volatile quantities;
In both cases, high quality approximations are suﬃcient in practice
134/200

What kind of approximation
• v : vertex with exact centrality c(v ) • c˜(v ): value that “approximates” c(v )
Deﬁnition (Absolute error) errabs(v ) = |c(v ) − c˜(v )|
Deﬁnition (Relative error) errrel(v ) = |c(v ) − c˜(v )|/c(v )
Deﬁnition ((ε, δ)-approximation)
• Let ε ∈ (0, 1) and δ ∈ (0, 1); • a (ε, δ)-approximation is a set {c˜(v ), v ∈ V } of n values, such
that Pr (∃v ∈ V s.t. err(v ) > ε) ≤ δ;
• it oﬀers uniform probabilistic guarantees over all the nodes; • it assumes normalized versions of centrality (i.e., in [0, 1]). 135/200

Sampling
Many of the algorithms we present are sampling-based. General Sampling Based Algorithm
1 Select independently at random (not all uniformly) a small set of objects (e.g., single vertices, pair of vertices, shortest paths);
2 Perform some computation using these objects (e.g., SSSP from vertex);
3 Use the results of the computation to estimate the centrality of all nodes;
136/200

Sampling
Why sampling? By only select a small subset of the “objects” (instead of the whole set), computing the approximation is faster than computing the exact values Questions for sampling algorithms
• What “objects” to sample? • How to sample?
If sampling procedure is slow, then the advantages are lost; • How many objects to sample in order to guarantee an
(ε, δ)-approximation?
137/200

Outline
• Approximation algorithms for static graphs • A sampling-based algorithm for closeness • A sampling+pivoting algorithm for closeness • Two sampling-based algorithms for betweenness
• Approximation algorithms for dynamic graphs • Two sampling-based algorithms for betweenness
138/200

Approximation Algorithms for Static Graphs
139/200

Fast approximation of centrality
D. Eppstein, J. Wang
Journal of Graph Algorithms and Applications (2004)
140/200

Idea

Interested in approximating closeness: n−1
c(x ) = y=x d(x , y )
(inverse of the average distance) Fastest-known exact algorithm: APSP
I.e., run Dijkstra’s algorithm from each vertex v Idea: only run Dijkstra from a few sources!

Warning

The algorithm actually computes an approximation for the inverse

of closeness:

c−1(v ) =

y=x d (u, v ) n−1

(eﬀectively the average distance)

141/200

Algorithm

• Let k be the number of sources to obtain the desired

approximation;

• For i = 1, . . . , k:

• pick a vertex ui uniformly at random • run Dijkstra from ui

• Let

c−1(v )

=

n

n −

1

k i =1

d

(ui

,

vi

)

k

Theorem E c−1(v ) = c−1(v ).
Question How large should k be to get a good approximation of c−1?

142/200

How much to sample

Lemma Let ∆ be the diameter of the graph and let ε, δ ∈ (0, 1). If

k

≥

2 ε2

1 ln 2 + ln n + ln δ

Then, with probability at least 1 − δ

c−1(v ) − c−1(v ) ≤ ∆ε, for all v ∈ V

Proof
1 Hoeﬀding inequality to bound the error of a single vertex; 2 Union bound to get uniform guarantees.

Running time: O

log

n−log ε2

δ

(n

log

n

+

m)

.

143/200

Computing Classic Closeness Centrality, at Scale
E. Cohen, D. Delling, T. Pajor, R. F. Werneck
COSN ’14: ACM Conference on Social Networks (2014)
144/200

Issues with sampling

• Assume that the distance distribution from a vertex v has a heavy tail, then the average distance

c−1(v ) =

u=v d (u, v ) n−1

is dominated by few distant vertices; • it is unlikely that these vertices are among the k that are
sampled • Hence the sample average

c−1(v )

=

n

n −

1

k i =1

d

(ui

,

vi

)

k

is a poor estimator of the average distance c−1(v ). • Sampling along can’t give us small relative error

145/200

Pivoting

Deﬁnition
Pivot The pivot p(v ) of a vertex v is the sampled vertex which is closest to v (p(v ) ∈ S).

• We have the exact value of c−1(p(v )), can we leverage it? • The average SP distance c−1(v ) of v is “close” to c−1(p(v )):

c−1(p(v )) − d(v , p(v )) ≤ c−1(v ) ≤ c−1(p(v )) − d(v , p(v )) • One can actually prove that, with high probability,

c−1(p(v )) + d(v , p(v )) ≤ 3c−1(v ) + O(1)

Pivoting by itself is not satisfactory: the relative error is still somewhat large.
Idea: combine sampling and pivoting into a hybrid estimator

146/200

Hybrid Estimator

For each vertex v with pivot p(v ), split the set V \ S into three

sets:

• L(v ): vertices in V \ S at distance at most d(v , p(v )) from p(v );

• HC (v ): vertices in S with distance greater than d(v , p(v ))

from p(v ).

• H(v ): vertices in V \ S at distance greater than d(v , p(v )) from p(v ).

The hybrid estimator is 

c−1(v

)

=

n

1 −

1



d(p(v ), u) +

d(u, v )

u∈H(v )

u∈HC(v )

+

|L(v )| |L(v ) ∩ S|

d(u, v )

u∈L(v )∩S

We have E[c−1(v )] = c−1(v ).

147/200

Guarantees
Theorem • With k = 1/ε3, the hybrid estimator has normalized RMSE O(ε). • With k = ε−3 ln n, the maximum relative error is O(ε) w.h.p.
148/200

Experiments

Table 1. Evaluating algorithms on undirected instances. For each instance, we report its number of nodes and edges, and for several algorithms the running time and average relative error.

Exact Sampling Pivoting

Hyb.-0.1

Hyb.-ad

type instance

|V | [·103]

|E| [·103]

time err. time err. time err. ¥ [h:m] [%] [sec] [%] [sec] [%]

time err. [sec] [%]

time [sec]

road ﬂa-t

1 070 1 344

59:30 5.4 24.4 3.2 21.6 2.5

28.3 2.8

73.2

usa-t

23 947 28 854 44 222:06 2.9 849.4 3.7 736.4 2.0 2 344.3 2.6 9 937.9

grid grid20

1 049 2 095

70:34 4.3 26.5 3.5 26.8 2.9

29.2 3.3

69.7

triang buddha

544 1 631

19:07 3.6 14.5 3.3 13.6 2.4

15.9 3.2

30.7

buddha-w

544 1 631

21:25 3.5 16.4 2.6 15.5 2.2

18.5 2.9

38.1

del20-w

1 049 3 146

72:06 2.7 27.4 3.6 26.7 2.6

32.6 2.7

71.0

del20

1 049 3 146

67:54 4.1 25.6 5.3 25.2 3.7

27.0 3.6

54.7

game FrozenSea

753 2 882

38:25 3.0 22.1 4.1 20.2 2.1

24.0 3.4

49.3

sensor rgg20

1 049

6 894

137:36 1.6 54.2 3.8 49.3 2.1

63.7 2.2 123.3

rgg20-w

1 049

6 894

160:29 1.6 61.2 3.8 57.1 2.1

73.3 2.3 142.3

Tcohmep

hySMkbeittrtreoirdSecesti12m2659a05 to12r11

is094
643

bet22t64e98::r5217tha00..76n

ju5592s..71t-s1a24..m33 pl45i75n..52g

a00..n67 d

ju6513s..62t-p03i..v36 oti1n0993g..52.

social rws20

1 049

3 146

113:40 0.9 45.6 3.0 41.3 0.9

49.4 0.9

98.6

rba20

1 049

6 291

132:35 0.8 56.8 9.7 48.4 0.8

60.2 1.0 117.4

Hollywood 1 069 56 307

226:42 1.0 86.5 14.6 81.8 1.0

85.7 1.9 117.6

Orkut

3 072 117 185 2 973:09 1.7 377.4 7.2 367.6 1.7 376.4 2.1 553.0

1066 RAM, running Windows 2008R2 Server. Each CPU has 8 cores (2.90 GHz, 8 ◊ 64 kiB L1, 8 ◊ 256 kiB, and 20 MiB L3 cache), but all runs are sequential. We use 32-bit integers to represent arc lengths.

and solution quality. We consider two versions of our algorithm, both based on Algorithm 1: the ﬁrst uses ‘ = 1/k = 0.1; the adaptive version picks, for each node, the ‘ value from {0.001, 0.025, 0.05, 0.1, 0.2, 0.5, 0.99} that minimizes the estimated14e9r/ro2r0.0

Summary for closeness
• Sampling can help, but not alone • Pivoting alone is not good • The hybrid approach is promising, but the sample size results
are somewhat disappointing (very large sample sizes!) More work to do!
150/200

Centrality Estimation in Large Networks
U. Brandes, C. Pich
International Journal of Bifurcation and Chaos (2007)
151/200

Betweenness centrality

We consider a normalized version:

b(v )

=

1 n(n −

1)

s ,t =v

σst (v ) σst

∈

[0, 1]

• σst : number of SPs from s to t • σst (v ): number of SPs from s to t going through v Exact algorithm: Brandes’ Algorithm 1 Run Dijkstra’s algorithm from each source vertex s 2 After each run, perform aggregation by walking SP DAG
backwards Idea: run Dijkstra only from a few sources (as in EW’01)

152/200

How can one get an (ε, δ)-approximation?

k

←

1 ε2

ln

n

+

ln

2

+

ln

1 δ

// sample size

b˜(v ) ← 0, for all v ∈ V

for i ← 1, . . . , k do // Brandes’ algo iterates over V

vi ← random vertex from V , chosen uniformly
Perform single-source SP computation from vi Perform partial aggregation, updating b˜(u), u ∈ V , like in

exact algorithm

end Output {b(˜v ), v ∈ V }

Theorem The output is a (ε, δ)-approximation:
Pr ∃v ∈ V s.t. |b˜(v ) − bv | > ε ≤ δ

153/200

How do they prove it?

Start with bounding the deviation for a single vertex v (Hoeﬀding

inequality):

Pr(|b˜(v ) − b(v )| > ε) ≤ 2e−2kε2

Then take the union bound over n vertices to ensure uniform convergence
The sample size k must be such that

2e−2kε2 ≤ δ n
That is, to get an (ε, δ)-approximation, we need

k

≥

1 2ε2

1 ln n + ln 2 + ln δ

154/200

Better Approximation of Betweenness Centrality
R. Geisberger, P. Sanders, D. Schultes
ALENEX (2008)
155/200

Issues with standard estimator

The standard estimator

b˜(v ) = 1 k

k

δui (v )

i =1

produces large overestimates for unimportant vertices close to a sampled vertex

Example

• Let v be a degree-two vertex connecting a degree-one vertex u to the rest of the network;
• If u is sampled, then b˜(v ) overestimates b(v ) by a factor of n/k

Possible solution: stop vertices from “proﬁting” for being near a sampled vertex.

156/200

A new sampling scheme
Idea: sample pairs (s, d) of vertex and direction (‘forward” or “backward”)
• When sampling (s, forward) • run Dijkstra from s
• When sampling (t, backward) • virtually ﬂip direction of edges (if directed graph); • run Dijkstra from s
We need to adapt the estimator b˜(v ).
157/200

New estimator

For a vertex v , deﬁne



σut (v ) d(u,v )

gv (u, d) = 

t∈V ,t=u,v σut d(v ,t)

σut (v ) t∈V ,t=u,v σut

1

−

d(u,v ) d(v ,t)

if d = forward if d = backward

The new estimator for b(v ) is

b˜(v ) = 2 k

k

gv (ui , di )

i =1

The factor 2 corrects for the reduced sampling probabilities (1/2n)

Theorem If

k

≥

1 2ε2

1 ln 2 + ln n + ln δ )

,

then the output is a (ε, δ)-approximation:

Pr ∃v ∈ V s.t. |b˜(v ) − bv | > ε ≤ δ

158/200

10−2

10−2

10−3

Euclidean distance 10−3

Experiments
Brandes bisection (unit) bisection (sh.path) linear
16 32 64 128 256 512 1024 2048 4096 8192
Sample size Euclidean distance between the vector of exact centralities and the vector of estimated centralities.
159/200

10−4

10−4

3

3

2

2

ns

Fast Approximation of Betweenness Centrality through Sampling
M. Riondato, E. M. Kornaropoulos
DMKD: Data Mining and Knowledge Discovery (2015)
160/200

What is wrong with this sampling approach?

1) The algorithm needs

k

≥

1 2ε2

1 ln n + ln 2 + ln
δ

• This is loose due to the union bound, and does not scale well (experiments)
• The sample size depends on ln n. This is not the right quantity: not all graphs of n nodes are equally “diﬃcult”: e.g., the n-star is “easier” than a random graph
The sample size k should depend on a more speciﬁc characteristic quantity of the graph
2) At each iteration, the algorithm performs a SSSP computation Full exploration of the graph, no locality
161/200

How can we improve the sample size?
[R. and Kornaropoulos, 2015] present an algorithm that:
1) uses a sample size which depends on the vertex-diameter, a characteristic quantity of the graph.
The derivation uses the VC-dimension of the problem;
2) samples SPs according to a speciﬁc, non-uniform distribution over the set SG of all SPs in the graph. For each sample, it performs a single s − t SP computation
• More locality: fewer edges touched than single-source SP • Can use bidirectional search / A*, . . .
162/200

What is the algorithm?

VD(G) ← vertex-diameter of G // stay tuned!

k

←

1 2ε2

(

log2(VD(G) − 2

) + 1 + ln(1/δ))

//

sample

size

b˜(v ) ← 0, for all v ∈ V

for i ← 1 . . . , k do

(u, v ) ← random pair of diﬀerent vertices, chosen uniformly

Suv ← all SPs from u to v // Dijkstra, trunc. BFS, ... p ← random element of Suv , chosen uniformly // not

uniform over SG b˜(w ) ← b˜(w ) + 1/k, for all w ∈ Int(p) // update only

nodes along p

end

Output {b˜(v ), v ∈ V }

Theorem The output {b˜(v ), v ∈ V } is an (ε, δ)-approximation.

163/200

VC-dimension
• The Vapnik-Chervonkenkis (VC) dimension is a combinatorial quantity that allows to study the sample complexity of a learning problem;
• It allows to obtain uniform guarantees on sample-based approximations of expectations of all functions in a family F;
• Not easy to compute exactly, somewhat easier to give upper bounds;
164/200

Theorem (VC ε-sample)

• Let F be a family of functions from a domain D into {0, 1}; • Let d be an upper bound to the VC-dimension of F; • Let ε ∈ (0, 1) and δ ∈ (0, 1) • Let S be a random sample of D of size

|S |

≥

1 ε2

1 d + ln δ

obtained by sampling D according to a prob. distribution π • Then

Pr

∃f ∈ F s.t.

1 |S |

f (s) − Eπ[f ] > ε < δ .

s ∈S

In other words: if we sample proportionally to the VC-dimension, we can approximate all expectations with their sample averages. 165/200

How can we prove the correctness?
We want to prove that the output {b˜(v ), v ∈ V } is an (ε, δ)-approximation
Roadmap: 1 Deﬁne betweenness centrality computation as a expectation estimation problem (domain D, family F, distribution π) 2 Show that the algorithm eﬃciently samples according to π 3 Show how to eﬃciently compute an upper bound to the VC-dimension Bonus: show tightness of bound 4 Apply the VC-dimension sampling theorem
166/200

How do we bound the VC-dimension?
Deﬁnition (Vertex-diameter) The vertex-diameter VD(G) of G is the maximum number of vertices in a SP of G:
VD(G) = max{|p|, p ∈ SG } .

If G is unweighted, VD(G) = ∆(G) + 1. Otherwise no relationship Very small in social networks, even huge ones (shrinking diameter eﬀect)

Computing VD(G):

2

max. min.

edge edge

weight weight

-approximation via

single-source SP

Theorem The VC-dimension of (SG , F ) is at most log2 VD(G) − 2 + 1

167/200

Is the bound to the VC-dimension tight?

Yes! There is a class of graphs with VC-dimension exactly log2 VD(G) − 2 + 1
The Concertina Graph Class (Gi )i∈N:

G 1

v

v

l

r

G

G

2

3

v

vv

v

l

rl

r

G 4

v

v

l

r

Theorem The VC-dimension of (SGi , F ) is log2 VD(G) − 2 + 1 = i

168/200

How well does the algorithm perform in practice?
It performs very well!
We tested the algorithm on real graphs (SNAP) and on artiﬁcial Barabasi-Albert graphs, to evalue its accuracy, speed, and scalability
Results: It blows away the exact algorithm and the union-bound-based sampling algorithm
169/200

How accurate is the algorithm?
In O(103) runs of the algorithm on diﬀerent graphs and with diﬀerent parameters, we always had |b˜(v ) − b(v )| < ε for all nodes
Actually, on average |b˜(v ) − b(v )| < ε/8

Absolute estimation error

email−Enron−u,|V|=36,692,|E|=367,662,δ=0.1,runs= 5 10−1

10−2

10−3

10−4 0

Avg (diam−2approx) Avg+Stddev (diam−2approx) Max (diam−2approx)
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 epsilon

170/200

How fast is the algorithm?
Approximately 8 times faster than the simple sampling algorithm
Variable speedup w.r.t. exact algorithm (200x – 4x), depending on ε
email−Enron−u, |V|=36,692, |E|=367,662, δ= 0.1, runs= 5
103 VC (diam−2approx) BP Exact
102

Running Time (seconds)

101

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
epsilon

171/200

How scalable is the algorithm?
Much more scalable than the simple sampling algorithm, because the sample size does not depend on n

Running Time (seconds)

Undirected Random Barabasi−Albert Graphs, ε=0.02, δ=0.1, runs=5
1400 VC (diam−2approx)
1200 BP

1000

800

600

400

200

00

1

2

3

4

5

6

7

8

9 10

Number of Vertices

x 104

172/200

ABRA: Approximating Betweennes Centrality in Static and Dynamic Graphs with Rademacher Averages
M. Riondato, E. Upfal arXiv (2016)
173/200

Issues with RK approach
• For each s − t SP computation, we only use a single SP • a lot of wasted work!
• Must compute (upper bound to) the vertex-diameter before we can start sampling • Exact computation cannot be done (would be equivalent to obtain exact betweenness) • Approximate computation leads to larger-than-necessary sample size
174/200

How to solve these issues
• Design a sample scheme that uses all SPs between a sampled pair of vertices
• Use progressive sampling, rather than static sampling • Start from small sample size • Check stopping condition to verify whether we sampled enough to get a (ε, δ)-approximation • If yes, stop, otherwise keep sampling.
How to achieve this: using Rademacher averages (VC-dimension on steroids)
175/200

Key ideas

• When backtracking from t to s, follow all SPs, not just one of them, and increase the estimation of all vertices found along the way: no wasted work;
• The stopping condition depends on:
• the richness of the vectors representing the current estimates of the betweenness of all vertices
• the current sample size • Formulas like this:

1

1 −

α

min
s ∈R+

1 s

ln

v∈VS

exp(s 2

v

2/(2

2))+ 2

ln α(1

2 δ
−

α)

+

ln 2/δ 2

• But it works!

176/200

Experiments

Graph
Soc-Epinions1 Directed
|V | = 75, 879 |E| = 508, 837
P2p-Gnutella31 Directed
|V | = 62, 586 |E| = 147, 892
Email-Enron Undirected |V | = 36, 682 |E| = 183, 831
Cit-HepPh Undirected |V | = 34, 546 |E| = 421, 578

Á
0.005 0.010 0.015 0.020 0.025 0.030
0.005 0.010 0.015 0.020 0.025 0.030
0.010 0.015 0.020 0.025 0.030
0.010 0.015 0.020 0.025 0.030

Runtime (sec.)
483.06 124.60
57.16 32.90 21.88 16.05
100.06 26.05 11.91 7.11 4.84 3.41
202.43 91.36 53.50 31.99 24.06
215.98 98.27 58.38 37.79 27.13

Speedup w.r.t.

BA
1.36 5.28 11.50 19.98 30.05 40.95
1.78 6.85 14.98 25.09 36.85 52.38
1.18 2.63 4.48 7.50 9.97
2.36 5.19 8.74 13.50 18.80

RK
2.90 3.31 4.04 5.07 6.27 7.52
4.27 4.13 4.03 3.87 3.62 3.66
1.10 1.09 1.05 1.11 1.03
2.21 2.16 2.05 2.02 1.95

Runtime Breakdown (%)

Sampling
99.983 99.956 99.927 99.895 99.862 99.827
99.949 99.861 99.772 99.688 99.607 99.495
99.984 99.970 99.955 99.932 99.918
99.966 99.938 99.914 99.891 99.869

Stop Cond.
0.014 0.035 0.054 0.074 0.092 0.111
0.041 0.103 0.154 0.191 0.220 0.262
0.013 0.024 0.035 0.052 0.061
0.030 0.054 0.073 0.091 0.108

Other
0.002 0.009 0.018 0.031 0.046 0.062
0.010 0.036 0.074 0.121 0.174 0.243
0.003 0.006 0.010 0.016 0.021
0.004 0.008 0.013 0.018 0.023

Sample Size
110,705 28,601 13,114 7,614 5,034 3,668
81,507 21,315
9,975 5,840 3,905 2,810
66,882 30,236 17,676 10,589
7,923
32,469 14,747
8,760 5,672 4,076

Reduction w.r.t. RK
2.64 2.55 2.47 2.40 2.32 2.21
4.07 3.90 3.70 3.55 3.40 3.28
1.09 1.07 1.03 1.10 1.02
2.25 2.20 2.08 2.06 1.99

Absolute Error (◊105)

max
70.84 129.60 198.90 303.86 223.63 382.24
38.43 65.76 109.10 130.33 171.93 236.36
145.51 253.06 290.30 548.22 477.32
129.08 226.18 246.14 289.21 359.45

avg
0.35 0.69 0.97 1.22 1.41 1.58
0.58 1.15 1.63 2.15 2.52 2.86
0.48 0.71 0.93 1.21 1.38
1.72 2.49 3.17 3.89 4.45

stddev
1.14 2.22 3.17 4.31 5.24 6.37
1.60 3.13 4.51 6.12 7.43 8.70
2.46 3.62 4.83 6.48 7.34
3.40 5.00 6.39 7.97 9.53

Table 2: Runtime, speedup, breakdown of runtime, sample size, reduction, and absolute error

• Smaller
between two nodes

hsaas mmanpylepatshisz, eABsRtAh-sadnoesRmKore

work

per

sample

than

RK

(which

only

explore

a

sin•gle SMP oun cthhe DfAaGs)t,ehrenc(enthoetspjeuedsutp ibs semcaalleur.se using smaller sample, also

Runtimbeebcraeuaksdeownno Tnheeemdaintcohalcleongme ipn udetseigntinhgea vsteoprptinegxc-odndiaitimonefotreprr)ogressive sampling • Very accurate algorithm is striking the right balance between the strictness of the condition (i.e., it should stop early)
and the e ciency in evaluating it. We now comment on the e ciency, and will report about the strictness in Sect. 6.2 and 6.3. In columns 6 to 8 of Table 2 we report the breakdown of the runtime into the main

177/200

1E-02 1E-03

Experiments
max avg+3stddev avg

absolute error

1E-04

1E-05

1E-06 0

0.005 0.01 0.015 0.02 0.025 0.03 epsilon

• More than 10x more accurate than guaranteed, on average; • More than 100x more accurate than guaranteed, in the best
case; • Close to the guarantee in the worst case: this is good.

178/200

Approximation Algorithms for Dynamic Graphs
179/200

Fully-Dynamic Approximation of Betweenness Centrality
E. Bergamini, H. Meyerhenke
ESA: European Symposium on Algorithms (2015)
180/200

Key ideas
This algorithm builds on: • the RK sampling-based approximation algorithm; • existing algorithms to update the SP DAG after an insertion/removal of a batch of edges;
It keeps track of potential modiﬁcations to the vertex diameter to understand whether to increase the sample size;
Theorem After each batch update, the output is an (ε, δ)-approximation.
181/200

Updating the DAGs
• Never change the set of sampled pairs of vertices, unless a sample was removed or more samples are needed
• What can change is which SP is sampled: if an edge is added, the path we sampled before may no longer be a SP.
• In any case, must save all the SP DAGs between the sampled pair of nodes
• Requires a lot of memory, but is needed in order to be able to update the estimation after the batch update
• The update computation builds on existing algorithms
182/200

Keeping track of the vertex diameter
• An edge is removed: the VD may decrease, but no need to change the sample size;
• An edge is added between two existing vertices in the same connected component: no change in the VD, hence no change in sample size
• An edge is added between two existing vertices in two diﬀerent connected components: the VD may have changed, recomputation is necessary
• An edge is added between an existing vertex and a new vertex: the VD may have increased by one, recomputation is necessary (the model used in this paper does not actually consider the insertion and removal of vertices)
Relying on the vertex diameter is not a great idea, that’s why we developed ABRA, the Rademacher Averages-based algorithm.
183/200

EXPERIMENTS

Experiments

104 reSlLesDLgg

ePDLl6lDshdoW

103

ePDLlLLnXx

fDFeEook3osWs

102

ePDLlEnron

fDFeEookFrLends

101

DrXLvCLWDWLons englLshWLkLSedLD

6SeedXS

100 0

21

22

23

24

25

26

27

28

29

210

BDWFh sLze

g. 1: SpSepeedeudupps oofveDrARKon RK in real unweighted networks using real dynamic

Graph

Real

Random

Time [s]

Speedups

Time [s]

Speedups

| | = 1 | | = 1024 | | = 1 | | = 1024 | | = 1 | | = 1024 | | = 1 | | =1841/020204

Fully Dynamic Betweenness Centrality Maintenance on Massive
Networks
T. Hayashi, T. Akiba, Y. Yoshida
VLDB: Very Large Databases (2016)
185/200

Key ideas
• Still a sampling-based approximation algorithm, but samples pair of vertices;
• This similar to RU16, but analysis use the union bound, so O(ε−2 log n) samples, which is a lot;
• Presents a new data structure called hypergraph sketch to keep track of the SP DAGS.
• An additional data structure, called the Two-ball Index, allows to identify the parts of hypergraph sketches that require updates
186/200

The Hypergraph Sketch
(eﬀectively a hypergraph) • For each sampled pair (s, t) of vertices, an hyperedge is added to the hypergraph: est = {(v , σsv , σv,t ) : v is on a SP from s to t}
• The estimations b˜(v ) can be obtained from the sketch; • Handling insertion and removal of edges is straightforward,
but must be done eﬃciently • Handling insertion and removal of nodes requires to change
the set of sampled pair of vertices, i.e., to potentially remove a hyperedge and insert another one;
187/200

Vertex Operations

Algorithm 1 Vertex operations

h

1: procedure AddVertex(H, v)

i

2: Let G⌧ be obtained from G⌧ 1 by adding v. 3: for each est 2 E(H) do

5

4:

continue with probability |V⌧ 1|2/|V⌧ |2.

5: 6:

Sample (s0, t0) 2 (V⌧ ⇥ V⌧ ) \ (V⌧ 1 ⇥ V⌧ 1). Replace est by the hyperedge es0t0 made from (s0, t0).

m t

7: procedure RemoveVertex(H, v)

t

8: Let G⌧ be obtained from G⌧ 1 by deleting v.

e

9: for each est 2 E(H) do

10: 11: 12:

if s 6= v and t 6= v then continue. Sample (s0, t0) 2 V⌧ ⇥ V⌧ uniformly at random. Replace est by the hyperedge es0t0 made from (s0, t0).

p s f

p

In words, for any ⌧ and a vertex v 2 V , the probability that

t

CH (v) is far apart from C⌧ (v) can be made arbitrarily small r 188/200

The Two-Ball Index
• For each sampled pair (s, t), maintain a triplet (∆st , β+, β−), where • ∆st = {d(s, v ), v is on a SP from s to t} • The ball β+ is the set of vertices at distance less than some ds from s, with their distances • The ball β+ is the set of vertices at distance less than some dt from t, with their distances
• The radiuses of the balls are such that they do not touch and are small.
• The triplets can be built with a bidirectional SP computation from s to t
189/200

Update Mechanism (for insertion)

! s,
nds orks allveralls the alls. s is

Algorithm 2 Update !B (s, ds) after edge (u, v) is inserted

1:

procedure

InsertEdgeIntoBall(u,

v,

!
s

)

2: 3: t) 4:

Q if

!!sAs[v[nv] ]>em!p!tsy[suF[]uI+]F+O1 1tq;huQeenu.peu. sh(v).

5: while not Q.empty() do

6:

v Q.pop().

7:

if

!
s

[v]

=

ds

then

continue.

8:

for each (v, c) 2 E do

9: 10:

!

!

if

!s[sc[]c>]

!s[vs[]v+] +1

then 1; Q.push(c).

(mu5c.h3 moIrenccormempleexnftoar ldeUleptidona)te

In each

ttrhipislestu(bsect,i!on,,

we

present how to e ciently ) in the TB-index as well as

update the hy-

190/200

1
BMS EI0 EI1
103

Insertion time per edge (ms)

E(bxp)erPimoeknetsc

6

BMS

5

EI0

4

EI1

3

2

1

0100

101

102

103

Batch size

(d) in-2004

191/200

Summary on approximation algorithms for betweenness
• Sampling Rules Everything Around Me; • Work on pushing down the amount of needed sampling is
important; • Progressive sampling frees us from many worries, but it is
challenging; • Fast and memory eﬃcient data structures are needed to be
able to update the estimations fast in dynamic graphs, where approximation is most useful; • Developing hybrid estimators?
192/200

Conclusions
193/200

What we presented
• Brief survey of the most common measures of centrality • Axioms for centrality • Focusing on closeness and betweenness centrality:
• exact algorithms on static graphs (GPU-based) • exact algorithms on dynamic graphs (streaming, distributed) • approximation algorithms for static graphs • approximation algorithms for dynamic graphs In each of the above, there are important open questions and directions for future work.
194/200

Big Graphs
• “Big Data” is a lot of hype and refers to very diﬀerent things depending on the context.
• However, the unprecedented volume, velocity, and variety pose real algorithmic challenges, especially when dealing with expressive and complex representations such as graphs.
• Challenges are opportunities for researchers! • Big graphs require new algorithms
195/200

Volume requires new algorithms
• Classic computational complexity: • Is there a polynomial time exact algorithm →? Go for it! • Your problem is NP-Hard → better think about approximation algorithms. . .
• Classic computational complexity: polynomial = feasible • But is polynomial time really feasible?
• E.g., Brandes algorithm not feasible for n = 109 • On big graphs quadratic time is as bad as NP-Hard
• New, ﬁner-grain, complexity theory needed (?) • Need for massively parallel algorithms, out-of-core algorithms,
sublinear algorithms, approximated algorithms, randomized algorithms, etc.
196/200

Velocity requires new algorithms
• The velocity with which new data keeps arriving. . . • . . . and the velocity with which the information of interest
keeps changing.
• In the case of graphs new edges are formed and old edges might disappear at very high speed. • How to maintain the centrality score of all vertices continuously updated?
• Velocity requires streaming algorithms that only read each data point once (or a few time), specialized small-space data structures (sketches) that maintain basic statistics and can be updated on-the-ﬂy, algorithms which are robust to changes in the data, etc.
197/200

Variety requires new algorithms

• Variety refers to the richness of diﬀerent information types to

be mixed in the analysis. • Examples in graphs:

• Vertices have attributes; • Vertices are spatio-temporally localized and keeps moving; • Edges have types (colors); • Edges have multiple types (a.k.a. multigraphs, multiplex
networks, multidimensional networks, etc.); • Each edge has associated a time series representing the amount
of communication (or activity) along the edge per time unit; • ...

• Semantic richness in the data implies complexity in the

knowledge we can extract.

• Applications involving “multi-structured” data require the

deﬁnition of new, ad-hoc, model and patterns . . .

• . . . and of course, the algorithms to extract them,

• and these new algorithms need to be able to deal with the

volume and the velocity!

198/200

Big Graphs
• The computational complexity of most existing graph algorithms makes them impractical in today’s networks, which are: • massive, • information-rich, and • dynamic.
• In order to scale graph analysis to real-world applications and to keep up with their highly dynamic nature, we need to devise new approaches speciﬁcally tailored for modern parallel stream processing engines that run on clusters of shared-nothing commodity hardware.
199/200

Thank you!
Francesco Bonchi http://francescobonchi.com
@FrancescoBonchi
Gianmarco De Francisci Morales http://gdfm.me @gdfm7
Matteo Riondato http://matteo.rionda.to
@teorionda

Slides available at http://matteo.rionda.to/centrtutorial/

200/200

