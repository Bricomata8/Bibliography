CS109/Stat121/AC209/E-109 Data Science
Bias and Regression
Hanspeter Pﬁster, Joe Blitzstein, and Verena Kaynig
y
residual
yˆ
column space of X

This Week
• HW1 due tonight at11:59 pm (Eastern Time) • HW2 posted soon

Census Data from the Current Population Survey (CPS)
“It is important to note that the CPS counts students living in dormitories as living in their parents’ home.”
– Census Bureau, http://www.census.gov/prod/2013pubs/p20-570.pdf

Some Forms of Bias

•

selection bias

•

publication bias (ﬁle drawer problem)

•

non-response bias

•

length bias

1936 Presidential Election, Landon vs. FDR

1936 Presidential Election, Landon vs. FDR Literary Digest predicted Landon would win with 370 electoral votes,
based on sample size of 2.4 million.
source: https://en.wikipedia.org/wiki/United_States_presidential_election,_1936

1936 Presidential Election, Landon vs. FDR

Literary Digest got responses from 2.3 million out of 10 million people surveyed.

To collect their sample, they used 3 readily available lists:

• readers of their magazine

• car registration list

•

phone directory

Wald and the Bullet Holes

What about the unobserved planes? Missing data!
???

What about the unobserved planes? Missing data!

Longevity Study from Lombard (1835)

Profession

Average Longevity

chocolate maker

73.6

professors

66.6

clocksmiths

55.3

locksmiths

47.2

students

20.2

Sources: Lombard (1835), Wainer (1999), Stigler (2002)

Class Size Paradox
Why do so many schools boast small average class size but then so many students
end up in huge classes?
Simple example: each student takes one course; suppose there is one course with 100 students,
ﬁfty courses with 2 students.
Dean calculates: (100+50*2)/51 = 3.92
Students calculate: (100*100+100*2)/200 = 51

“About 10 percent of the 1.6 million inmates in America’s prisons are serving life sentences; another 11 percent are serving over 20 years.”
source: http://www.nytimes.com/2012/02/26/health/dealing-with-dementia-among-aging-criminals.html? pagewanted=all

Length-Biasing Paradox
How would you measure the average prison sentence?

Bias of an Estimator

The bias of an estimator is how far off it is on average:

bias(✓ˆ) = E(✓ˆ)

✓

So why not just subtract off the bias?

one form:

Bias-Variance Tradeoff
MSE(✓ˆ) = Var(✓ˆ) + bias2(✓ˆ)
often a little bit of bias can make it possible to have much lower MSE

http://scott.fortmann-roe.com/docs/BiasVariance.html

Unbiased Estimation: Poisson Example

X⇠ Pois( )

e2 Goal: estimate

X

e2

( 1) is the best (and only) unbiased estimator of

sensible?

Fisher Weighting

How should we combine independent, unbiased estimators for a parameter into one estimator?

Xk ✓ˆ = wi✓ˆi
i=1
The weights should sum to one, but how should they be chosen?

wi

/

1 Var(✓ˆi)

(Inversely proportional to variance; why not SD?)

Nate Silver Weighting Method

• Exponential decay based on recency of poll

•

Sample size of poll

•

Pollster rating

http://ﬁvethirtyeight.com/features/howthe-ﬁvethirtyeight-senate-forecastmodel-works/

Multiple Testing, Bonferroni
How should we handle p-values when testing multiple hypotheses?
For example, what if we are looking at diet (with 10 kinds of food) and
disease (with 10 diseases)? A simple, conservative approach is Bonferroni: divide signiﬁcance level by number of hypotheses being tested.
https://en.wikipedia.org/wiki/Bonferroni_correction

80

78

76

74

SON’S HEIGHT (INCHES)

72

70

68

66

64

62

60

58

58

60

62

64

66

68

70

72

74

76

78

80

FATHER’S HEIGHT (INCHES)

plot from Freedman, data from Pearson-Lee

Regression Toward the Mean (RTTM)
Examples are everywhere...
Test scores Sports
Inherited characteristics, e.g., heights Trafﬁc accidents at various sites

Daniel Kahneman Quote on RTTM
I had the most satisfying Eureka experience of my career while attempting to teach ﬂight instructors that praise is more effective than punishment for promoting skill-learning....
[A ﬂight instructor objected:] “On many occasions I have praised ﬂight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not...”
This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.

Regression Paradox
y: child’s height (standardized) x: parent’s height (standardized)
Regression line: predict y = rx; think of this as a weighted average of
the parent’s height and the mean
Now, what about predicting the parent’s height from the child’s height? Use x = y/r?
Regression line is x = ry, the r stays the same!

Linear Model
often called “OLS” (ordinary least squares), but that puts the focus on the procedure rather than the model.
|{yz} = |X{z} |{z} + |{✏z}
n⇥1 n⇥k k⇥1 n⇥1

What’s linear about it?
|{yz} = |X{z} |{z} + |{✏z}
n⇥1 n⇥k k⇥1 n⇥1
Linear refers to the fact that we’re taking linear combinations of the predictors. Still linear if, e.g., use both x and its square and its cube as predictors.

Sample Quantities vs. Population Quantities

sample version (think of x and y as
data vectors)

ˆ0 ˆ1

= =

y¯Pni=Pˆ11(nx¯xi
i=1

x¯)(yi

(xi

¯)2 x

y¯)

population version (think of x and y
as r.v.s)

y = 0 + 1x + ✏ E(y) = 0 + 1E(x) cov(y, x) = 1cov(x, x)

visualize regression as a projection
y
residual
yˆ
column space of X

or as a conditional expectation
Y Y-E(Y|X)
E(Y|X)
space of all functions of X

= n=ne nnnpe2⇡nnp2⇡n

Gauss-Markov Theorem wwhheererethtehepepneunltuimltiamtealtineeliunseesutsheasttehxapt exp(x n()x2 isns)m2 alilsifsxmiasllfairf fxroims fna.r f⇤rom

1199 GGaauusss-sM-MarakrokvoTvhTeohreeomrem

Consider a linear model
Consider a linear model

y=X +✏

y=X +✏
where y is n by 1, X is an n by k matrix of covariates, is a k by 1 vector of

pwarhaemreetyersi,sandbtyhe1e,rrXorsi✏sj aanre nunbcoyrrkelamteadtwriixthoefqucoavl avrairaiatnesc,e, ✏j i⇠s [a0,k 2]b.yT1heve

eprraorrasmdeotnerost,naenedd ttohebeerarsosursm✏ejd atroebuenNcoorrmreallalyteddiswtritbhuteeqdu. al variance, ✏j ⇠ [0, 2

Terhreoorrsedmo 1n9o.t1.neUenddetro tbhee aabsosvuemaesdsutmopbtieonNs,ormally distributed.
Then it follows that...
Theorem 19.1. Under the abˆo⌘ve(Xas0sXu)m1pXti0oyns,

is BLUE (the Best Linear Unbiased ˆE⌘stim(Xat0oXr)). 1X0y

his 1B9L.2U. WE h(atthedoBweestmLeianneabry Ubenstb?iaWsehdicEhsltoisms afutnocrt)i.on should we minimize? In
this case, the “best” estimator is the one that minimizes the sum of squares error.
Thha1t’9s .w2h.yWwhe actalldiot twhee mordeainnarbyylebasetsts?quaWrehsicehstilmoasstofru. nction should we minimi
PtTrhohiosaft.c’asLsewet,h˜ythFbweoeer“abncNyaelsolltin”ritemaetrsahtlueimneobrariradtosienrrdasir,seysttthihlmieseaasiotstonrsae,qlisut.oeha.aret˜hs=meesiAMntiyimmLfEaoizr.toessro.mtheemsautmrixoAf .squares
˜

Residuals
y =Xˆ+e
mirrors
y=X +✏
The residual vector e is orthogonal to all the columns of X.

Residual Plots 7.5. RESIDUAL PLOTS

81

yshoouucldanbemsaykme.mAIeftraliwcll viaesrywtiscealllp,lyylooabutosuhttoh0u.ledThsrieneegsscoitdonslutoaoanktlsvfoa!rri(aarPneclheoeittnertorhseecesvdeiardsticuciatayl l((sεnˆ)ovnd-iscreo.cnﬁtsiottantnetanvdadritahnecsec)aatntedr nonlinearity (whicvhailnudiecaste,sasnomde rcheasngide uinathlse mvosd.eleias cnehcespsarrey)d. iInctFoigurrev7a.5r,itahebseleth)ree cases are
illustrated.

No problem

Heteroscedascity

Nonlinear

Residual −0.3 −0.2 −0.1 0.0 0.1 0.2 0.3

1.0

1

0.5

−1.5 −1.0 −0.5 0.0

Residual

0

Residual

−1

−2

0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.2 0.4 0.6 0.8 1.0

−0.8 −0.6 −0.4 −0.2 0.0 0.2

Fitted

Fitted

Fitted

Figure 7.5: RFeasirdauwalsayv,shFtitttped:/p/clortsan- .trh-epﬁrrostjesucgtg.eostrsgn/odochca/ncgoe ntotrthibe /cFuarrrenatwmaoyd-ePl RwAhil.eptdhfe second
shows non-constant variance and the third indicates some nonlinearity which should prompt some change in the structural form of the model

“Explained” Variance

var(y) = var(X ˆ) + var(e)

R2

=

var(X ˆ) var(y)

=

P Pnini==11((yyˆii

y¯)2 y¯)2

R2 measures goodness of ﬁt, but it does not validate the model. Adding more predictors can only increase R2.

