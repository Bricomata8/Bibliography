CS109/Stat121/AC209/E-109 Data Science
Bayesian Methods
Hanspeter Pﬁster, Joe Blitzstein,Verena Kaynig

Freq

Bayes

FB

This Week
• Form project team if you haven’t already.Try your best to have your team formed by next Tuesday, but in any case it is required to ﬁll in the Google form by Tuesday Nov 3, 11:59 pm: http://goo.gl/forms/CzVRluCZk6
• HW4 is due Thursday Nov 5, 11:59 pm.

The Theory That Would Not Die

Think Bayes
http://greenteapress.com/thinkbayes/

Probabilistic Programming and Bayesian Methods for Hackers
http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methodsfor-Hackers/master/Prologue/Prologue.ipynb

Doing Bayesian Data Analysis
https://sites.google.com/site/doingbayesiandataanalysis/

Bayesian Data Analysis

Bayes’ rule

Bayes’ rule
prior probability for A
P (B|A)P (A) posterior probability for A P (A|B) =
P (B)

Bayes’ rule, likelihood version
p(✓|y) = p(y|✓)p(✓) p(y)
Treating the data y as ﬁxed,
p(✓|y) / L(✓)p(✓)
Bayes’ rule says the posterior density is proportional to the likelihood function times the prior density.

Discriminative vs. Generative Classiﬁers
What to model and what not to model? discriminative: just model p(y|x)
generative: give a full probability model p(x,y)=p(x)p(y|x)=p(y)p(x|y)

Generative Models

P (Y = 1|X = x) =

(|

( | = 1) ( = 1) f xY P Y = 1) ( = 1) + ( | = 0) (

= 0)

f xY P Y

f xY P Y

(by Bayes’ Rule)

Then can model the densities f(x|Y=1), f(x|Y=0).

Naive Bayes Spam Filter

Consider 10 words that occur frequently in spam, and let Wj be the event that the jth word appears in the email.

A certain email uses the 1st and 10th words but not the rest. What’s the probability that it is spam?

P (spam|W1, W2c, W3c, . . . , W9c, W10)

=

P (W1, W2c, W3c, . . . , W9c, W10|spam)P (spam) P (W1, W2c, W3c, . . . , W9c, W10)

Expand denominator with law of total probability

P (W )

=

P (W |spam)P (spam)

+

P

(W

| not

spam)P (not

spam)

Naive Bayes Spam Filter

P (spam|W1, W2c, W3c, . . . , W9c, W10)

=

P (W1, W2c, W3c, . . . , W9c, W10|spam)P (spam) P (W1, W2c, W3c, . . . , W9c, W10)

Naive Bayes assumption: conditional independence given spam, and also conditional independence given not spam.
P (W1, W2c, W3c, . . . , W9c, W10|spam) = P (W1|spam)P (W2c|spam) . . . P (W10|spam)
P (W1, W2c, W3c, . . . , W9c, W10|not spam) = P (W1|not spam)P (W2c|not spam) . . . P (W10|not spam)
Huge assumption but huge simpliﬁcation in statistical and computational complexity.

Naive Bayes
Naive conditional independence assumption:

fj (x1, . . . , xd) = fj1(x1)fj2(x2) . . . fjd(xd)

Often unrealistic, but still may be useful esp. since it leads to a drastic reduction in the number of parameters to estimate.

help to combat overﬁtting, for example e the best size of decision tree to learn. a, since if we use it to make too many can itself start to overﬁt [17].

80 Bayes
75 C4.5

Test-Set Accuracy (%)

tion, there are many methods to combat

70

ost popular one is adding a regulariza-

aluation function. This can, for exam-

65

rs with more structure, thereby favoring

ss room to overﬁt. Another option is to

60

l signiﬁcance test like chi-square before

e, to decide whether the distribution of

55

ﬀerent with and without this structure. e particularly useful when data is very , you should be skeptical of claims that

50 10

100

1000

10000

ue “solves” the overﬁtting problem. It’s

Number of Examples

tting (variance) by falling into the op-

rﬁtting (bias). Simultaneously avoiding

Figure 2: Naive Bayes can outperform a state-of-

ng a perfect classiﬁer, and short of know-

the-art rule learner (C4.5rules) even when the true

re is no single technique that will always

classiﬁer is a set of rules.

ch).

Domingos, http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf

ption about overﬁtting is that it is caused

trillion examples, the latter covers only a fraction of about
−18

Full Probability Modeling
“The process of Bayesian data analysis can be idealized by dividing it into the following three steps:
1.Setting up a full probability model – a joint probability distribution for all observable and unobservable quantities in a problem...
2.Conditioning on observed data: calculating and interpreting the appropriate posterior distribution – the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.
3.Evaluating the ﬁt of the model and the implications of the resulting posterior distribution...”
-- Gelman et al, Bayesian Data Analysis

Bayes-Frequency Reconciliation

Freq

Bayes

FB

Think like a Bayesian, check like a frequentist.

Conjugate Priors: Beta-Binomial
X|p ⇠ Bin(n, p) p ⇠ Beta(a, b)
f (p) / pa 1(1 p)b 1
https://en.wikipedia.org/wiki/Beta_distribution

Conjugate Priors: Beta-Binomial
X|p ⇠ Bin(n, p) p ⇠ Beta(a, b)

|

⇠

Posterior is then p X = x Beta(a + x, b + n x)

Conjugate Priors: Normal-Normal

y|µ ⇠ N (µ, 2) µ ⇠ N (µ0, ⌧ 2)

✓ Then µ|y ⇠ N (1

◆

1

B)y + Bµ0,

1
2

+

1 ⌧2

2
where B = 2+⌧2 is the shrinkage factor

Conjugate Priors
http://www.johndcook.com/conjugate_prior_diagram.html

Ranking Reddit Comments:
Example from Probabilistic Programming and Bayesian Methods for Hackers
http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-BayesianMethods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/LawOfLargeNumbers.ipynb

Ranking Reddit Comments: A Simple Model

number of upvotes ⇠ Bin(n, p)

p⇠

a, b

/ pa 1

pb 1

conjugate prior: Beta( ), pdf

(1 )

p| ⇠

a

,b

posterior: data Beta( + #upvotes + #downvotes)

Ranking Reddit Comments
Why not just add “pseudocounts” and then use proportion? Why bother with Bayes?
For example, the Agresti-Coull method adds 2 successes and 2 failures.

Posterior Distributions for Reddit Comments
http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/ Chapter4_TheGreatestTheoremNeverTold/LawOfLargeNumbers.ipynb

Ranking Reddit Comments by Posterior Quantiles
http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/ Chapter4_TheGreatestTheoremNeverTold/LawOfLargeNumbers.ipynb

