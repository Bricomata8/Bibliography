Radar and Vision Sensor Fusion for Object Detection in Autonomous Vehicle Surroundings

Jihun Kim, Dong Seog Han
School of Electronics Engineering Kyungpook National University
Daegu, Republic of Korea dshan@knu.ac.kr
Abstract— Multi-sensor data fusion for advanced driver assistance systems (ADAS) in the automotive industry has received much attention recently due to the emergence of self-driving vehicles and road traffic safety applications. Accurate surroundings recognition through sensors is critical to achieving efficient advanced driver assistance systems (ADAS). In this paper, we use radar and vision sensors for accurate object recognition. However, since sensorspecific data have different coordinates, the data coordinate calibrate is essential. In this paper, we introduce the coordinate calibration algorithms between radar and vision images and perform sensor calibrating using data obtained from actual sensors.
Keywords—Autonomous Vehicle; Sensor fusion; radar; Vision; Sensor calibration

Benaoumeur Senouci
Central Electronic Engineering School ECE-Paris France
senouci@ece.fr
II. SENSOR CHARACTERISTICS
This section describes the characteristics of the radar and vision sensors used to detect surrounding obstacles. The radar sensor has a vertical recognition distance of up to 50 m, and its azimuth angle is ± 75 ° horizontal and 12 ° vertical. Fig. 1 shows the target information displayed in the radar coordinate system. Basically, it shows distance information of x and y axes. In this paper, a virtual target was implemented using a total of seven corner-reflectors to obtain target information. However, the radar input information is input together with erroneous information other than the target.

I. INTRODUCTION
Of recent, there are great research efforts and interests in the field of intelligent transportation systems (ITS). ITS enhances driver safety and convenience and are being commercialized as intelligent vehicle and advanced driver assistance systems (ADAS). The first step in intelligent vehicles and advanced driver assistance systems is to understand the environment through road vehicle detection and tracking. Based on environmental conditions, response cruise control systems, front vehicle collision warning and collision mitigation systems, and pedestrian collision warning systems have already been commercialized. However, it is necessary to achieve high reliability and improve on the perception technology of surrounding environments for successful autonomous driving.
Multi-sensor fusion is one methodology used to achieve vehicle surrounding perception. Such sensors include radar, camera, lidar, and ultrasonic wave. Multi-sensor fusion is a highly desired strategy because different sensors have their own capabilities.
In this paper, we introduce the utilization of radar and vision sensors to improve the object recognition accuracy in a vehicle surrounding. In the Section. II, we introduce the characteristics of the sensors used for the experiment. In Section. III, we introduce the algorithm for the sensor calibrating experiment, and in Section. IV, we analyze the calibrating result of the experimental data.
This research was supported by The Leading Human Resource Training Program of Regional Neo industry through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT and future Planning(grant number)(NRF-2016H1D5A1909920)

Fig. 1. Target information in radar coordinate system.
The vision sensor has a resolution of 1280 × 720 and an azimuth angle of ± 65 °. Fig 2 shows the target information of the vision sensor. There is a total of 7 targets set for the experiment and they are corner reflectors with different reflectance.
Fig. 2. Target information in vision sensor.

978-1-5386-4646-5/18/$31.00 ©2018 IEEE

76

ICUFN 2018

III. RADAR AND VISION SENSOR CALIBRATION
The data of the radar sensor and the vision sensor are expressed in different coordinate systems. For sensor fusion, two coordinate systems must be represented. This is often referred to as sensor calibration. There are various algorithms for sensor calibrating such as pseudo inverse based homography estimation method, SVD based homography estimation method, and extrinsic parameter estimation method. Each algorithm shows a difference in implementation complexity and matching performance. In this paper, we use a pseudo inverse based on point alignment method [1]. The point alignment method represents the coordinates of the radar target data in the image plane. At this time, the coordinate values of each sensor are represented by a point on the image plane through the transformation matrix. The linear least square (LS) method is used to find the transformation matrix. At least four experimental data are required to get the transformation matrix. The following equation (1) is a process for obtaining a transformation matrix:

u

 xr  t11 t12 t13   xr 

=  v  T= IR  yr  t21

t22

t23

 

 

yr

 

(1)

 1 

 1  t31 t32 t33   1 

where TIR is a 3 by 3 transformation matrix. Using equation
(1), radar coordinates can be converted into image coordinates. The six parameters of the transformation matrix can be obtained by the following calculation:

  Ti = ti1 ti2 ti3 

U = u1 u2

un 

V = v1 v2

vn 

 In1 = 1 1

1

(2)

P

=

  

x1r

 xrn

y1r

1 



yrn 1

where n is the number of aligned points, and xrn , = yrn (n 1, 2, 3, 4, ... , n  4) is position of the target point in
radar coordinates. The transformation matrix

TIR = [T1 T2 T3 ] can be obtained using the linear LS method as follows:

( ) T1 = PT P −1 PTU

( ) T2 = PT P −1 PTV

(3)

( ) T3 = PT P −1 PT I

The actual radar target data and the vision target data are matched using TIR obtained from equation (3). In this paper,
the transformation matrix are obtained using the radar and

vision target data obtained from the experiment and calculated for the calibration accuracy.
IV. CALIBRATION EXPERIMENT RESULT
A coordinate calibration method was used to simultaneously process the data of the radar and vision sensor used for lateral rearward monitoring of the autonomous vehicle. The static target radar target data and vision target data obtained from the actual experiment were matched in the manner described above. Table 1 shows the coordinate data for the radar and vision sensors for the 7 targets obtained through the experiment. xr , yr are distance information of radar sensor
targets and xc , yc are location information of vision sensor targets.

TABLE I.
Target Number
1 2 3 4 5 6 7

TARGET COORDIATE DATA OF RADAR AND VISION SENSOR

xr

yr

xc

yc

3.00

0.10

604

516

5.00

-1.10

1010

404

8.00

-2.10

1120

415

9.00

2.10

305

425

11.00

-0.10

705

420

12.80

-2.10

975

425

15.00

1.10

595

425

The target data obtained through the experiment does not coincide with the coordinates of each sensor. For the coordinate matching, the transformation matrix obtained using the experimental data is as follows:

 0.8 −175.2 698.7 

TIR =

 

−4.6

6.0

476.7

 

(4)

 0

0

1 

In this experiment, we used a total of 7 targets to obtain the transformation matrix. As a result, the transformation matrix (4) was calculated. We transformed each target coordinate of the radar into image coordinates by using Equation (1). Table 2 shows the transformed image coordinates and the error rate. The matching results of the two sensors show about an accuracy of 95%.

TABLE II.
Target Number
1 2 3 4 5 6 7

TARGET COORDIATE DATA OF IMAGE PLANE

xI
683 895 1073 338 725 1007 518

yI

Accuracy (%)

463

93.23

447

92.52

427

97.33

447

97.18

425

98.87

404

94.56

414

96.23

77

The coordinates of the target in the image plane are expressed as xI , yI in pixels. In the experiment, the target information is represented by one point. However, since the actual target is a vehicle or an object, it must be expressed as an area, not a point. If the calculated accuracy is about 95%, the calibrating result is reliable.
V. CONCLUSION In this paper, we described the sensor fusion process for radar and vision for autonomous vehicles. Sensor fusion was performed using the data obtained from actual experiments. The algorithm used for sensor fusion is easy to implement and superior in performance, so it can be used to detect the surrounding obstacle of an autonomous vehicle.
ACKNOWLEDGMENT This research was supported by The Leading Human Resource Training Program of Regional Neo industry through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT and future Planning (grant number) (NRF-2016H1D5A1909920)
REFERENCES
[1] G. Alessandretti A. Broggi P. Cerri "Vehicle and guard rail detection using radar and vision data fusion" IEEE Trans. Intell. Transp. Syst. vol. 8 no. 1 pp. 95-105 Mar. 2007.
[2] U. Hofmann, A. Rieder, and E. D. Dickmanns, “Radar and vision data fusion for hybrid adaptive cruise control on highways,” in Proc. Int. Conf. Comput. Vis. Syst., Vancouver, BC, Canada, pp. 125–138, 2001.
[3] T. Wang, N. Zhengm J. Xin, Z. Ma, "Intergrating Millimeter Wave Radar with a Monocular Vision Sensor for On-Road Obstacle Detection Applications," Sensor vol. 11, pp. 8992-9008, 2011.
78

