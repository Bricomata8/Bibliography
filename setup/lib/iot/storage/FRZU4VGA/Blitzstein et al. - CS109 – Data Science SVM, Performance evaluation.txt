CS109 ‚Äì Data Science SVM, Performance evaluation
Joe Blitzstein, Hanspeter Pfister, Verena Kaynig-Fittkau
http://i.stack.imgur.com/1gvce.png

Announcements
‚Ä¢ HW1 grades went out yesterday ‚Ä¢ They are looking really good, well done
everyone!
‚Ä¢ HW2 is due this Thursday!
‚Ä¢ You should submit an executed notebook ‚Ä¢ But please without pages of test output

Recap K-NN
‚Ä¢ Keeps all training data ‚Ä¢ Training is fast ‚Ä¢ Prediction is slow

Separating Hyperplane
‚Ä¢ x: data point ‚Ä¢ y: label ‚Ä¢ w: weight vector
w

Separating Hyperplane
‚Ä¢ x: data point ‚Ä¢ y: label ‚Ä¢ w: weight vector
w

Separating Hyperplane

‚Ä¢ x: data point ‚Ä¢ y: label ‚Ä¢ w: weight vector ‚Ä¢ b: bias

w b

Separating Hyperplane

‚Ä¢ x: data point

‚Ä¢ y: label

‚Ä¢ w: weight vector

‚Ä¢ b: bias

+1

-1

w

Perceptron

x1

w1

w2 x2

w3

x3

b

-1

Perceptron

x1

w1

w2 x2

w3

x3

b

-1

Perceptron History
‚Ä¢ invented 1957 ‚Ä¢ by Frank Rosenblatt
‚Ä¢ the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence. (NYT 1958)
(http://en.wikipedia.org/wiki/Perceptron

https://www.youtube.com/watch?v=cNxadbrN_aI&list=PLdVOMWcqwwIlaygvb9ZteZ1r4Br6kR uBO

Side Note: Step vs Sigmoid Activation
1 Ì†µÌ±† Ì†µÌ±• = 1 + Ì†µÌ±í‚àíÌ†µÌ±êÌ†µÌ±•

The Critics
‚Ä¢ 1969: Minsky and Papert publish their book ‚ÄúPerceptrons‚Äù
‚Ä¢ Very controversial book, some blame the book for causing the whole research area to stagnate.
https://en.wikipedia.org/wiki/Perceptrons_(book)

The XOR Problem
x2
x1

The XOR Problem
xx23
x1

Support Vector Machine
‚Ä¢ Widely used for all sorts of classification problems
‚Ä¢ Some people say it is the best of the shelf classifier out there

Maximum Margin Classification

x2

x2

x1

x1

Solution depends only on the support vectors!

Maximum Margin Classification

margin:

w

Maximum Margin Classification
non-convex

This Is Kind of Odd
‚Ä¢ Which data points do we care the most about?
‚Ä¢ What would those samples look like?

Two Very Similar Problems

What about outliers?
: slack variables
x2
subject to:
x1

Two Very Similar Problems
Slack

Hard Margin (C = Infinity)
http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf

Soft Margin (C = 10)
http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf

XOR problem revised
x=0
Did we add information to make the problem seperable?

Non-Linear Decision Boundary

Polynomial Kernel in 3D

Quadratic Kernel

Kernel Functions
‚Ä¢ Polynomial: ‚Ä¢ Radial basis function (RBF):

So what is the excitement?

So what is the excitement?

Prediction
‚Ä¢ Again we can use the kernel trick! ‚Ä¢ Prediction speed depends on number of
support vectors

The Miracle Explained
‚Ä¢ Andrew Ng does this really well
‚Ä¢ http://cs229.stanford.edu/notes/cs229notes3.pdf
‚Ä¢ Course is also on Youtube, ItunesU, etc.

Kernel Trick for SVMs
‚Ä¢ Arbitrary many dimensions ‚Ä¢ Little computational cost ‚Ä¢ Maximal margin helps with curse of
dimensionality

Face Recognition

Face Recognition
‚Ä¢ Load image data ‚Ä¢ Put your test data aside ‚Ä¢ Extract Eigenfaces ‚Ä¢ Train SVM ‚Ä¢ Evaluate performance
‚Ä¢ Red are cross validation steps
http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html#exampleapplications-face-recognition-py

Jhon Gonzalez https://www.youtube.com/watch?v=cxHMgl2_5zg

http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html

Tips and Tricks
‚Ä¢ SVMs are not scale invariant ‚Ä¢ Check if your library normalizes by default ‚Ä¢ Normalize your data
‚Äì mean: 0 , std: 1 ‚Äì map to [0,1] or [-1,1]
‚Ä¢ Normalize test set in same way!

Tips and Tricks
‚Ä¢ RBF kernel is a good default ‚Ä¢ For parameters try exponential sequences ‚Ä¢ Read:
Chih-Wei Hsu et al., ‚ÄúA Practical Guide to Support Vector Classification‚Äù, Bioinformatics (2010)

SVM vs KNN
‚Ä¢ What are the main key differences?

Parameter Tuning
‚Ä¢ Given a classification task
‚Ä¢ Which kernel ? ‚Ä¢ Which kernel parameter values? ‚Ä¢ Which value for C?
Try different combinations and take the best.

Train vs. Test Error

error

testing

degree of freedom

training

Where is KNN on this graph for K=1, or for K=Inf?

Grid Search
Zang et al., ‚ÄúIdentification of heparin samples that contain impurities or contaminants by chemometric pattern recognition analysis of proton NMR spectral data‚Äù, Anal Bioanal Chem (2011)

Error Measures

‚Ä¢ True positive (tp) ‚Ä¢ True negative (tn) ‚Ä¢ False positive (fp) ‚Ä¢ False negative (fn)

true

predicted

1

-1

1 tp

fn

-1 fp

tn

TPR and FPR

‚Ä¢ True Positive Rate:

predicted

1

-1

1 tp

fn

true

‚Ä¢ False Positive Rate:

-1 fp

tn

true positive rate

Reciever Operating Characteristic
1
1
false positive rate

true positive rate

ROC Example
false positive rate
https://inclass.kaggle.com/c/ca-2015/details/evaluation

‚Ä¢ Recall:

Precision Recall

predicted

1

-1

1 tp

fn

true

‚Ä¢ Precision:

-1 fp

tn

Precision Recall
‚Ä¢ Recall: If I pick a random positive example, what is the probability of making the right prediction?
‚Ä¢ Precision: If I take a positive prediction example, what is the probability that it is indeed a positive example?

precision

Precision Recall Curve
1
1
recall

Comparison
J. Davis & M. Goadrich, ‚ÄúThe Relationship Between Precision-Recall and ROC Curves.‚Äù, ICML (2006)

F-measure
‚Ä¢ Weighted average of precision and recall
‚Ä¢ Usual case: ‚Ä¢ Increasing allocates weight to recall

Multi Class
One vs. All

One vs All
‚Ä¢ Train n classifier for n classes ‚Ä¢ Take classification with greatest margin ‚Ä¢ Slow training

Multi Class
One vs. One

One vs One
‚Ä¢ Train n(n-1)/2 classifiers ‚Ä¢ Take majority vote ‚Ä¢ Fast training

Confusion Matrix

24

1

1

1

28

5

1

3

19

http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

Recap
‚Ä¢ Perceptrons are great ‚Ä¢ But really just a separating hyperplane ‚Ä¢ So is SVM ‚Ä¢ Kernels are neat ‚Ä¢ Evaluation metrics are important

