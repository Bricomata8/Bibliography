CS109/Stat121/AC209/E-109 Data Science
Bayesian Methods Continued
Hanspeter Pﬁster, Joe Blitzstein,Verena Kaynig

Freq

Bayes

FB

This Week
• Make sure HW3 Google presentation is public ASAP if you haven’t already. See Rahul’s Piazza post on this (@1159).
• Form project team if you haven’t already.Try your best to have your team formed by next Tuesday, but in any case it is required to ﬁll in the Google form by Tuesday Nov 3, 11:59 pm: http://goo.gl/forms/CzVRluCZk6
• HW4 is due Thursday Nov 5, 11:59 pm.

Which proportion is bigger? A: 1 out of 2
B: 40 out of 100
Which baseball player would you prefer to have on your team? Player A: 1 hit out of 2 at-bats
Player B: 40 hits out of 100 at-bats
Which cab driver would you prefer to have drive you somewhere?
Driver A: 1 of 2 reaching destination Driver B: 40 out of 100 reaching destination

Bayes and Shrinkage Estimation
http://sas-and-r.blogspot.com/2012/04/example-927-baseball-and-shrinkage.html

Empirical Bayes
Consider overall distribution across baseball players, estimate the prior from the data.
http://varianceexplained.org/r/empirical_bayes_baseball/

Empirical Bayes
Beta(79,225)
http://varianceexplained.org/r/empirical_bayes_baseball/

Empirical Bayes
http://varianceexplained.org/r/credible_intervals_baseball/

Empirical Bayes

Best batters according to MLE

name Jeff Banister Doc Bass Steve Biras C. B. Burns Jackie Gallagher

H AB

average

1

1

1

1

1

1

2

2

1

1

1

1

1

1

1

http://varianceexplained.org/r/empirical_bayes_baseball/

Empirical Bayes

Best batters according to EB estimates.

name Rogers Hornsby Shoeless Joe Jackson Ed Delahanty Billy Hamilton Harry Heilmann

H

AB average eb_estimate

2930 8173

0.358

0.355

1772 4981

0.356

0.350

2596 7505

0.346

0.343

2158 6268

0.344

0.340

2660 7787

0.342

0.339

// We hired a Data Scientist to analyze our Big Data // and all we got was this lousy line of code.
ﬂoat estimate = (successes + 78.7) / (total + 303.5);
http://varianceexplained.org/r/empirical_bayes_baseball/

Empirical Bayes
Posterior density for Derek Jeter, with 95% credible interval
http://varianceexplained.org/r/credible_intervals_baseball/

Conﬁdence Intervals vs. Credible Intervals
95% conﬁdence interval: P (a(Y )  ✓  b(Y )) = 0.95
parameter is ﬁxed, data is random
P (3  ✓  7) is 0 or 1 (we just don’t know which), from non-Bayesian perspective
95% credible interval: P (a(Y )  ✓  b(Y )|Y ) = 0.95
parameter is random, data is ﬁxed
But we can often get the best of both worlds: we can study the coverage probabilities of credible intervals; we can study the frequentist
performance of Bayesian methods.

http://varianceexplained.org/r/credible_intervals_baseball/

Bayesian Bandits
Example from Probabilistic Programming and Bayesian Methods for Hackers
http://research.microsoft.com/en-us/projects/bandits/
N slot machines, each with its own unknown probability distribution for rewards. Exploration-exploitation tradeoff.

Bayesian Bandits
Example from Probabilistic Programming and Bayesian Methods for Hackers
A fast, simple Bayesian algorithm:
1. sample from the prior of each bandit 2. select the bandit with the largest sampled value 3. update the prior for that bandit (the posterior
becomes the new prior) 4. repeat.

Bayesian Bandits
http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/ Chapter6_Priorities/Priors.ipynb

Kidney Cancer Example from Bayesian Data Analysis (Gelman et al) U.S. counties with the highest 10% of
Figure 2.7 Tkhidenceoyunctaiensceorf dtheeatUhnirtaedtesSta(ategse-waidthjutshteedh)ighest 10% age-
standardized death rates for cancer of kidney/ureter for U.S. white

Kidney Cancer Example from Bayesian Data Analysis (Gelman et al)
58 Bayesian Data Analysis The issue is sample size. Consider a county of population 1000. Kidney cancer is a rare
disease, and, in any ten-year period, a county of 1000 will probably have zero kidney cancer deaths, so that it will be tied for the lowest rate in the country and will be shaded in Figure 2.8. However, there is a chance the county will have one kidney cancer
Figure 2.7 The counties of the United States with the highest 10% agestandardized death rates for cancer of kidney/ureter for U.S. white males, 1980–1989. Why are most of the shaded counties in the middle of the country? See Section 2.8 for discussion.
each based on different data but with a common prior distribution. In addition to illustrating the role of the prior distribution, this example introduces hierarchical modeling, to which we return in Chapter 5.
A puzzling pattern in a map Figure 2.7 shows the counties in the United States with the highest kidney cancer death

lowest rates: blue Figure 2. A cursory glance at the distribution of the U.S. counties with the lowest rates of kidney cancer (teal) might lead one to conclude that
something about the rural lifestyle reduces the risk of that cancer. After all, the counties with the lowest 10 percent of risk are mainly Midwest-
highest rates: orange ern, Southern and Westem counties. When one examines the distribution of counties with the highest rates of kidney cancer (red), however,
it becomes clear that some other factor is at play Knowledge of de Moivre's equation leads to the conclusion that what the counties with the lowest and highest kidney-cancer rates have in common is low population—and therefore high variation in kidney-cancer rates.
H. Wainer, The Most Dangerous Equation

Kidney Cancer Example from Bayesian Data Analysis (Gelman et al)

Figure 2.7 The counties of the United States with the highest 10% age-

simple model: y ⇠ Pois(10n ✓ ) smtaanledsa,r1d9iz8e0d–1d9e8aj9th.

rates for cancer of Why are most of the

skhijaddneedjy/cuoruentetriesfoirn

U.S. white the middle

✓ ⇠ Gamma(↵, ) of the country? See Section 2.8 for discussion.
j

each based on different data but with a common prior distribution. In addition to

y illustrating the role of the prior distributjion, this example introduces hierarchical

E(✓ |y ) = w + (1 w)E(✓ ) modeling, to which we jreturjn in Chapter 5.

j

10nj

weighted combinatiAopnuzozlifngthpaettedrnaintaa maanpd the prior mean
Figure 2.7 shows the counties in the United States with the highest kidney cancer death

ay, 319 were 0’s, 141 were 1’s, 33 were 2’s, and 5
Kidney Cancer Example from Bayesian Data Analysis (Gelman et al)
death ratersawyj/(d1a0tan:j)smvsa.llpcoopuunltaiteisonacsciozeunntj.fo(rb)almost le of log10 poaplluolaf ttihoen htioghsaenedthloewddaetathmroartees come from the discreteness of the data (nj=0, 1,

clearly. The patterns come from the discreteness o
Kidney Cancer Examp2le, …fro).m Bayesian Data Analysis (Gelman et al)
BayeFsiegsutriem2a.t1e0s:(aau)toBmayaetsi-ceasltliymaactecdoupnotsstefroiorr mean kidney regression toward the meanvs. logarithm of popula
counties in the U.S. (b) Posterior medians and for a sample of 100 counties j. The scales on th

Kidney Cancer Example from Bayesian Data Analysis (Gelman et al)
ed pBosatyeersiioarn pmoesatnerikoirdnmeeydicaannscaenrdd5e0a%thprroabteasb,ility intervals vs. logarithm of population size nj, the 3071
b) Posterior medians and 50% intervals for ș

Decision Theory: Nature vs. Data Scientist
Nature picks the parameter, data scientist gets data and then chooses an action
(estimate the parameter, predict a future observation, give an interval estimate, accept or reject a hypothesis, ….)
Then some loss is incurred, based on a loss function.

Decision Theory: Nature vs. Data Scientist

Most common loss functions for estimation:

L2 (mean square error) : L(✓, ✓ˆ) = (✓ˆ

✓2 )

L1 (mean absolute error) : L(✓, ✓ˆ) = |✓ˆ ✓|

Bayesian can try to minimize the expected risk, given the data. Mean square error: use posterior mean
Mean absolute error: use posterior median

Decision Theory: Geometry of Admissibility

10

8

6

R(theta_{2},d)

4

d1

d8

d7 d2

d3 d4

d5 d6

0

2

4

6

8

10

R(theta_{1},d)

2

0

Complete Class Theorem
Under mild technical conditions:
Any Bayesian procedure is admissible. Any admissible procedure is Bayesian (or a limit of such).

Freq

Bayes

FB

drawn from course work of Stanford students Marc Coram and Phil Beineke.
Example 1 (CrMypatrogkroapvhyC).hSatiannfMordo’snStteatCistaicrsloDe(pMartCmMentCh)a:s a drop-in consulting service. One dayD, aiapcsoycnhiosl-oCgisot rfraomm tEhxe asmtatpe lperison system showed up
with a collection of coded messages. Figure 1 shows part of a typical example.
Figure 1: The problem was to decode these messages. Marc guessed that the code was a simple substitution cipher, each symbol standing for a letter, number, punctuation mark or space. Thus, there is an unknown function f
f : {code space} −→ {usual alphabet}.

PERSI DIACONIS
MCMCryptography

stGateitstaictrsa,nMsitiaorncmdaotwrixnlMoa(xd,ye)dfoar Estnagnlisdha(rtdhetperxotba(be.ilgit.y, Wa

the ﬁrst-ordeorftgroaingsiftrioomnsl:ettthere xptroopleotrtetrioyn) of consecutive

This gives a matrix M (x, y) of transitions. One may th

f via

Deﬁne “plausibility”

Pl(f ) = M (f (si), f (si+1)) ,
i
ueo“svTeorryf”cPtooln(fsswe)caauprteotiwvngeotohrsoeaydnmrdacotbaimoonldosleifdtiptnaelatruteshssiienbfioltcihrtoieeddsdee.edccroymdpeitnsigso,anbga.esMe. dFaxuinmc y running the following Markov chain Monte Carlo alg

with a preliminary guess, say f . ute Pl(f ). ge to f by making a random transposition of the value

Figure 2:

The text was scrambled at random and the Monte Carlo algorithm was run.

Figure 3 shows sample output.

Figure 2:

The text was scrambled at random and the Monte Carlo algorithm was ru igure 3 shows sample output.

Figure 3:

