Deep Learning

Deep Learning
Why do you want to know about deep learning?

Motivation
‚Ä¢ It works! ‚Ä¢ State of the art in machine learning ‚Ä¢ Google, Facebook, Twitter, Microsoft are all
using it.
‚Ä¢ It is fun! ‚Ä¢ Need to know what you are doing to do it
well.

Google Trends

Google Trends
https://www.google.com/trends/explore#q=deep%20learning%2C%20%2Fm%2F0hc2 f&cmpt=q&tz=Etc%2FGMT%2B5

Scene recognition
http://places.csail.mit.edu/demo.html

Google Brain - 2012
16 000 Cores

What it learned
http://www.nytimes.com/2012/06/26/technology/in-a-big-network-ofcomputers-evidence-of-machine-learning.html?pagewanted=all

Google DeepMind
https://www.youtube.com/watch?v=V1eYniJ0Rnk

What is different?
‚Ä¢ We have seen ML methods:
‚Äì SVM, decision trees, boosting, random forest
‚Ä¢ We needed to hand design the input ‚Ä¢ ML algorithm learns the decision boundary

Feature Design

Yes No

classification

hand designed features

input image patch

Learned Feature Hierarchy

Yes No

classification
high level features

medium level features

low level features

[Honglak Lee]

input image patch

Scaling with Data Size

Performance

Deep Learning
Most ML algorithms

Amount of data

[Andrew Ng]

Deep Learning Techniques
‚Ä¢ Artifical neural network
‚Äì Introduced in the 60s
‚Ä¢ Convolutional neural network
‚Äì Introduced in the 80s
‚Ä¢ Recurrent neural network
‚Äì Introduced in the 80s

What Changed since the 80s?
‚Ä¢ MLPs and CNNs have been around since the 80s and earlier
‚Ä¢ Why did people even bother with SVMs, boosting and co?
‚Ä¢ And why do we still care about those methods?

Brain or Rocket
https://www.youtube.com/watch?v=EczYSl-ei9g

What Changed -Computational Power

What Changed ‚Äì Data Size

I don‚Äôt Have a Cluster at Home
http://www.geforce.com/whats-new/articles/introducing-nvidia-geforce-gtx-titan-z

Deep Learning
What is deep learning?

Perceptron

x1

w1

w2 x2

w3

x3

b

-1

Perceptron

x1

w1

w2 x2

w3

x3

b

-1

Ì†µÌ±† Ì†µÌ±è + Ì†µÌ±§Ì†µÌ±áÌ†µÌ±•

Separating Hyperplane

‚Ä¢ x: data point ‚Ä¢ y: label ‚Ä¢ w: weight vector ‚Ä¢ b: bias

w b

Side Note: Step vs Sigmoid Activation
1 Ì†µÌ±† Ì†µÌ±• = 1 + Ì†µÌ±í‚àíÌ†µÌ±êÌ†µÌ±•

The XOR Problem
x2
x1

The XOR Problem
xx23
x1

Perceptron
x2
Ì†µÌ±§Ì†µÌ±• = 0
x1

Multi-Perceptron

x2
Ì†µÌ±§‚Ä≤1Ì†µÌ±• = 0

Ì†µÌ±§‚Ä≤2Ì†µÌ±• = 0

Ì†µÌ±§‚Ä≤3Ì†µÌ±• = 0

Ì†µÌ±§11 Ì†µÌ±§12 Ì†µÌ±ä = Ì†µÌ±§21 Ì†µÌ±§22
Ì†µÌ±§31 Ì†µÌ±§32
Ì†µÌ±äÌ†µÌ±• = ?

x1

x2

Ì†µÌ±§‚Ä≤1Ì†µÌ±• = 0

a

Xor Problem

Ì†µÌ±§‚Ä≤2Ì†µÌ±• = 0

Ì†µÌ±äÌ†µÌ±é = [+, ‚àí] Ì†µÌ±äÌ†µÌ±è = [+, +] Ì†µÌ±äÌ†µÌ±ê = [‚àí, ‚àí] Ì†µÌ±äÌ†µÌ±ë = [+, ‚àí]

b

c

d

x1

Xor Problem

h2
b

Ì†µÌ±äÌ†µÌ±é = [+, ‚àí] Ì†µÌ±äÌ†µÌ±è = [+, +] Ì†µÌ±äÌ†µÌ±ê = [‚àí, ‚àí] Ì†µÌ±äÌ†µÌ±ë = [+, ‚àí]

h1

c

a,d

Multi-Layer Perceptron
Ì†µÌ±ä Ì†µÌ±• Ì†µÌ±† Ì†µÌ±è(1) + Ì†µÌ±ä(1)Ì†µÌ±•
http://deeplearning.net/tutorial/mlp.html

Multi-Layer Perceptron
Ì†µÌ±ì Ì†µÌ±• = Ì†µÌ∞∫(Ì†µÌ±è(2) + Ì†µÌ±ä(2) Ì†µÌ±† Ì†µÌ±è(1) + Ì†µÌ±ä(1)Ì†µÌ±• ) Ì†µÌ∞∫: logistic function, softmax for multiclass
http://deeplearning.net/tutorial/mlp.html

Yes No

Classification high level features medium level features
low level features

Autoencoder
‚Ä¢ This is what Google used for their Google brain
‚Ä¢ Basically just a MLP ‚Ä¢ Output size is equal to input size
‚Ä¢ Popular for pre-training a network on unlabeled data

Autoencoder
Decoder
WT Encoder
W

Deep Autoencoder
‚Ä¢ Reconstruct image from learned low dimensional code
‚Ä¢ Weights are tied ‚Ä¢ Learned features are often
useful for classification ‚Ä¢ Can add noise to input
image to prevent overfitting
Salakhutdinov & Hinton, NIPS 2007

From MLP to CNN
‚Ä¢ So far no notion of neighborhood ‚Ä¢ Invariant to permutation of input ‚Ä¢ A lot of data is structured:
‚Äì Images ‚Äì Speech ‚Äì‚Ä¶
‚Ä¢ Convolutional neural networks preserve neighborhood

http://www.amolgmahurkar.com/classifySTLusingCNN.html

http://www.amolgmahurkar.com/classifySTLusingCNN.html

Convolution

Convolutional Network
http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg

CNN Advantages
‚Ä¢ neighborhood preserved ‚Ä¢ translation invariant ‚Ä¢ tied weights

DNNs are hard to train
‚Ä¢ backpropagation ‚Äì gradient descent ‚Ä¢ many local minima ‚Ä¢ prone to overfitting ‚Ä¢ many parameters to tune ‚Ä¢ SLOW

Stochastic Gradient Decent
https://www.youtube.com/watch?v=HvLJUsEc6dw

Development
‚Ä¢ Computers got faster! ‚Ä¢ Data got bigger. ‚Ä¢ Initialization got better.

2006 Breakthrough
‚Ä¢ Ability to train deep architectures by using layer-wise unsupervised learning, whereas previous purely supervised attempts had failed
‚Ä¢ Unsupervised feature learners:
‚Äì RBMs ‚Äì Auto-encoder variants ‚Äì Sparse coding variants

Unsupervised Pretraining
http://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf

http://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf

Dropout
X XX X X X Ì†µÌ±ä
Ì†µÌ±•
‚Ä¢ Helps with overfitting ‚Ä¢ Typically used with random initialization ‚Ä¢ Training is slower than without dropout

Deep Learning for Sequences
‚Ä¢ MLPs and CNNs have fixed input size ‚Ä¢ How would you handle sequences?
‚Ä¢ Example: Complete a sentence
‚Äì‚Ä¶ ‚Äì are ‚Ä¶ ‚Äì How are ‚Ä¶

Slide from G. Hinton

Meaning of Life
https://www.youtube.com/watch?v=vShMxxqtDDs

Slide from G. Hinton

Recurrent Neural Network
http://blog.josephwilk.net/ruby/recurrentneural-networks-in-ruby.html

Recurrent Neural Network
http://karpathy.github.io/2015/05/21/rnn-effectiveness/

Intriguing properties of neural networks
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,Ian Goodfellow, Rob Fergus International Conference on Learning Representations (2014) http://www.datascienceassn.org/sites/default/files/Intriguing%20Properties%20of%20Neural% 20Networks_0.pdf

‚Ä¢ Theano ‚Ä¢ Torch ‚Ä¢ Caffe

Libraries

‚Ä¢ TensorFlow ‚Ä¢‚Ä¶

Theano
‚Ä¢ Full disclosure: My favorite ‚Ä¢ Python ‚Ä¢ Transparent GPU integration ‚Ä¢ Symbolical Graphs ‚Ä¢ Auto-gradient ‚Ä¢ Low level ‚Äì in a good way! ‚Ä¢ If you want high-level on top:
‚Äì Pylearn2 ‚Äì Keras, Lasagne, Blocks ‚Äì‚Ä¶

Torch
‚Ä¢ Lua (and no Python interface) ‚Ä¢ Very fast convolutions ‚Ä¢ Used by Google Deep Mind, Facebook AI, IBM ‚Ä¢ Layer instead of graph based
https://en.wikipedia.org/wiki/Torch_(machine_learning)

Caffe
‚Ä¢ C++ based ‚Ä¢ Higher abstraction than Theano or Torch ‚Ä¢ Good for training standard models ‚Ä¢ Model zoo for pre-trained models

Tensorflow
‚Ä¢ Symbolic graph and auto-gradient ‚Ä¢ Python interface ‚Ä¢ Visualization tools ‚Ä¢ Some performance issues regarding speed and
memory
https://github.com/soumith/convnet-benchmarks/issues/66

Tips and Tricks

Number of Layers / Size of Layers
‚Ä¢ If data is unlimited larger and deeper should be better
‚Ä¢ Larger networks can overfit more easily ‚Ä¢ Take computational cost into account

Learning Rate
‚Ä¢ One of the most important parameters ‚Ä¢ If network diverges most probably learning
rate is too large ‚Ä¢ Smaller works better ‚Ä¢ Can slowly decay over time ‚Ä¢ Can have one learning rate per layer
Other tips for SGD: http://leon.bottou.org/publications/pdf/tricks-2012.pdf

Momentum
‚Ä¢ Helps to escape local minima ‚Ä¢ Crucial to achieve high performance
More about Momentum: http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf

Convergence
‚Ä¢ Monitor validation error ‚Ä¢ Stop when it doesn‚Äôt improve within n
iterations
‚Ä¢ If learning rate decays you might want to adjust number of iterations

Initialization of W
‚Ä¢ Need randomization to break symmetry
‚Ä¢ Bad initializations are untrainable
‚Ä¢ Most heuristics depend on the number of input (and output) units
‚Ä¢ Sometimes W is rescaled during training
‚Äì Weight decay (L2 regularization) ‚Äì Normalization

Data Augmentation
‚Ä¢ Exploit invariances of the data ‚Ä¢ Rotation, translation ‚Ä¢ Nonlinear transformation ‚Ä¢ Adding Noise
http://en.wikipedia.org/wiki/MNIST_database

Data Normalization
‚Ä¢ We have seen std and mean normalization
‚Ä¢ Whitening
‚Äì Neighbored pixels often are redundant ‚Äì Remove correlation between features
More about preprocessing: http://deeplearning.stanford.edu/wiki/index.php/Data_Preproc essing

Non-Linear Activation Function
‚Ä¢ Sigmoid
‚Äì Traditional choice
‚Ä¢ Tanh
‚Äì Symmetric around the origin ‚Äì Better gradient propagation than Sigmoid
‚Ä¢ Rectified Linear
‚Äì max(x,0) ‚Äì State of the art ‚Äì Good gradient propagation ‚Äì Can ‚Äúdie‚Äù

L1 and L2 Regularization
‚Ä¢ Most pictures of nice filters involve some regularization
‚Ä¢ L2 regularization corresponds to weight decay ‚Ä¢ L2 and early stopping have similar effects ‚Ä¢ L1 leads to sparsity ‚Ä¢ Might not be needed anymore (more data,
dropout)

Monitoring Training
‚Ä¢ Monitor training and validation performance ‚Ä¢ Can monitor hidden units ‚Ä¢ Good: Uncorrelated and high variance

Further Resources
‚Ä¢ More about theory:
‚Äì Yoshua Bengio‚Äôs book:http://www.iro.umontreal.ca/~bengioy/dlbook/
‚Äì Deep learning reading list: http://deeplearning.net/reading-list/
‚Ä¢ More about Theano:
‚Äì http://deeplearning.net/software/theano/ ‚Äì http://deeplearning.net/tutorial/

