CS109 ‚Äì Data Science
Joe Blitzstein, Hanspeter Pfister, Verena Kaynig-Fittkau vkaynig@seas.harvard.edu staff@cs109.org

Announcements
‚Ä¢ HW2 is due today! ‚Ä¢ Please execute your notebooks, but without
test output.
‚Ä¢ Help with lecture material

Books
‚Ä¢ ‚ÄúElements of Statistical Learning‚Äù ‚Ä¢ http://statweb.stanford.edu/~tibs/ElemStatLe
arn/
‚Ä¢ ‚ÄúPattern Recognition and Machine Learning‚Äù ‚Ä¢ http://research.microsoft.com/en-
us/um/people/cmbishop/PRML/

Next Topics
‚Ä¢ Tree classifier ‚Ä¢ Bagging ‚Ä¢ Random Forest

Decision Tree

after 10

no

pm?

yes

call friend

got electricity?

read book

got new dvd?

play computer

watch tv

Decision Trees
‚Ä¢ Fast training ‚Ä¢ Fast prediciton ‚Ä¢ Easy to understand ‚Ä¢ Easy to interpret
http://en.akinator.com/personnages/jeu

Decision Tree - Idea

A

B

C

D

E

E B

C

D

A

Bishop, ‚ÄúPattern Recognition and Machine Learning‚Äù, Springer, 2006

Decision Tree - Idea
‚Ä¢ What is a the benefit of using only one feature at a time?
‚Ä¢ What is the drawback?

Decision Tree - Prediction

A

B

C

D

E

E B

C

D

A

Decision Tree -Training
‚Ä¢ Learn the tree structure:
‚Äì which feature to query ‚Äì which threshold to choose

A

B

C

D

E

Node Purity

10 7

35

72

B

32

A

A

B

C

D

E

E

C

D

Gini Impurity
‚Ä¢ Expected error ‚Ä¢ if you randomly choose a sample ‚Ä¢ and predict the class of the entire node based
on it.

Gini Impurity
Example: 4 red, 3 green, 3 blue data points

‚Ä¢ Class probabilities:
‚Äì red: 4/10 green: 3/10

blue: 3/10

‚Ä¢ misclassification:
‚Äì red: 4/10 * (3/10 + 3/10)

Picking red

Making an error

Gini Impurity
‚Ä¢ misclassification:
‚Äì red: 4/10 * (3/10 + 3/10) = 0.24
‚Äì green and blue: 3/10 * (4/10 + 3/10) = 0.21
‚Ä¢ gini impurity: 0.24 + 0.21 + 0.21 = 0.66

Gini Impurity
‚Ä¢ Number of classes: ‚Ä¢ Number of data points: ‚Ä¢ Number of data points of class i:

true

wrong

class

prediction

Gini Impurity
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Node Purity Gain

‚Ä¢ Compare:
‚Äì Gini impurity of parent node ‚Äì Gini impurity of child nodes

A

B

C

Misclassification

‚Ä¢

1 Ì†µÌ±Å

Ì†µÌ±Å Ì†µÌ±ñ

Ì†µÌøè(≈∑Ì†µÌ±ñ

‚â†

Ì†µÌ±¶Ì†µÌ±ñ )

‚Ä¢ not differentiable

Comparison Gini vs Misclassification

‚Ä¢ Binary problem: 400 samples per class

400 | 400

400 | 400

100|300 300|100
Misclassification: 0.25 Gini gain: 0.125

200|400 200|0
Misclassification: 0.25 Gini gain: 0.166

Pseudocode
‚Ä¢ Check if already finished ‚Ä¢ For each feature xi
‚Äì Calculate the gain from splitting on xi ‚Äì Let xbest be the feature with highest gain
‚Ä¢ Create a decision node that splits on xbest ‚Ä¢ Repeat on the sub-nodes
‚Ä¢ Does this produce an optimal tree? ‚Ä¢ What does optimal tree mean?
http://en.wikipedia.org/wiki/C4.5_algorithm

When to Stop
‚Ä¢ node contains only one class ‚Ä¢ node contains less than x data points ‚Ä¢ max depth is reached ‚Ä¢ node purity is sufficient ‚Ä¢ you start to overfit => cross-validation

Tree Pruning

10 7

35

72

32

A

B

C

D

E

E B

C

D

A

How do you make a prediction for the merged cell?

Pruning and Complexity
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Decision Trees - Disadvantages
‚Ä¢ Sensitive to small changes in the data ‚Ä¢ Overfitting ‚Ä¢ Only axis aligned splits

Decision Trees vs SVM
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Wisdom of Crowds
The collective knowledge of a diverse and independent body of people typically exceeds the knowledge of any single individual, and can be harnessed by voting.
James Surowiecki
http://socialmedia4srm.wordpress.com/

Netflix Prize
‚Ä¢ Take home messages:

Ensemble Methods
‚Ä¢ A single decision tree does not perform well ‚Ä¢ But, it is super fast ‚Ä¢ What if we learn multiple trees?
We need to make sure they do not all just learn the same.

Bootstrap

Bootstrap
‚Ä¢ Resampling method from statistics ‚Ä¢ Useful to get error bars on estimates
‚Ä¢ Take N data points ‚Ä¢ Draw N times with replacement
‚Ä¢ Get estimate from each bootstrapped sample

Bootstrap
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Bootstrap
‚Ä¢ I can generate more data! ‚Ä¢ Can I do cross validation on this?

Bootstrap vs Cross-validation
‚Ä¢ Bootstrap has overlap in data sets Do not use simple bootstrap to generate train and test data from same data set.
Probability of choosing n

Bootstrap vs Cross-validation
‚Ä¢ Bootstrap has overlap in data sets Do not use simple bootstrap to generate train and test data from same data set.
Probability of not choosing n

Bootstrap vs Cross-validation
‚Ä¢ Bootstrap has overlap in data sets Do not use simple bootstrap to generate train and test data from same data set.
Probability of not choosing n in N draws

Bootstrap vs Cross-validation
‚Ä¢ Bootstrap has overlap in data sets Do not use simple bootstrap to generate train and test data from same data set.
Probability of (not not) choosing n in N draws

Bootstrap vs Cross-validation
‚Ä¢ Bootstrap has overlap in data sets
Do not use simple bootstrap to generate train and test data from same data set.
Ì†µÌ±í‚àí1
This number is important later

Bagging
‚Ä¢ Bootstrap aggregating
‚Ä¢ Sample with replacement from your data set ‚Ä¢ Learn a classifier for each bootstrap sample ‚Ä¢ Average the results

Bagging Example
x2
x1

Bias-Variance Trade-off

Bagging Decision Trees
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Bagging Decision Trees
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Bagging
‚Ä¢ Reduces overfitting (variance) ‚Ä¢ Normally uses one type of classifier ‚Ä¢ Decision trees are popular ‚Ä¢ Not helping with linear models ‚Ä¢ Easy to parallelize

Random Forest
‚Ä¢ Builds upon the idea of bagging ‚Ä¢ Each tree build from bootstrap sample ‚Ä¢ Node splits calculated from random feature
subsets
http://www.andrewbuntine.com/articles/about/fun

Random Forest ‚Äì Fun Fact

http://research.microsoft.com/enus/projects/handpose/

Random Forest
‚Ä¢ All trees are fully grown ‚Ä¢ No pruning
‚Ä¢ Two parameters
‚Äì Number of trees ‚Äì Number of features

Random Forest Error Rate
‚Ä¢ Error depends on:
‚Äì Correlation between trees (higher is worse) ‚Äì Strength of single trees (higher is better)
‚Ä¢ Increasing number of features for each split:
‚Äì Increases correlation ‚Äì Increases strength of single trees

Out of Bag Error
‚Ä¢ Each tree is trained on a bootstrapped sample ‚Ä¢ About 1/3 of data points not used for training
‚Ä¢ Predict unseen points with each tree ‚Ä¢ Measure error

Out of Bag Error

sample
bootstrap sample

data points

filter
unused data points

train

test

Out of Bag Error
‚Ä¢ Very similar to cross-validation ‚Ä¢ Measured during training ‚Ä¢ Can be too optimistic

Variable Importance - 1
‚Ä¢ Again use out of bag samples ‚Ä¢ Predict class for these samples ‚Ä¢ Randomly permute values of one feature ‚Ä¢ Predict classes again ‚Ä¢ Measure decrease in accuracy

Variable Importance - 1
shape
color

Variable Importance - 2
‚Ä¢ Measure split criterion improvement ‚Ä¢ Record improvements for each feature ‚Ä¢ Accumulate over whole ensemble

Example: Spam classification
Randomization tends to spread out the variable importance more uniformly.
Hastie et al.,‚ÄùThe Elements of Statistical Learning: Data Mining, Inference, and Prediction‚Äù, Springer (2009)

Unbalanced Classes
‚Ä¢ The Problem: ‚Ä¢ Oversample: ‚Ä¢ Subsample: ‚Ä¢ Subsample for each tree!

Random Forest Subsampling
sample
train

Random Forest
‚Ä¢ Similar to Bagging ‚Ä¢ Easy to parallelize ‚Ä¢ Packaged with some neat functions:
‚Äì Out of bag error ‚Äì Feature importance measure ‚Äì Proximity estimation

