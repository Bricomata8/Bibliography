CS109 Data Science
Classiﬁcation & PCA
Hanspeter Pﬁster, Joe Blitzstein, and Verena Kaynig

This Week
• HW2 due on Thursday • HW1 grades will be out Sunday night • HW1 solution is available next Tuesday

Classiﬁcation

Classiﬁcation
Alex Krizhevsky, Stanford

Problems
Alex Krizhevsky, Stanford

Training Data
Alex Krizhevsky, Stanford

Machine Learning
• Input: A training set of N data points, each labeled with one of K different classes.
• Learning: Use the training set to learn what every one of the classes looks like.
• Evaluation: Predict labels for a test set of data and compare the true labels (ground truth) to the ones predicted by the classiﬁer.

Machine Learning
• Make predictions for new data points
• data has labels • supervised learning: kNN, SVM, Decision Trees,
Random Forests, Bagging, Boosting, etc.
• Find patterns in the data
• data has no labels • unsupervised learning: PCA, MDS, Clustering

Classiﬁcation
x: data points y: labels
features decision boundary

Classiﬁcation
x: data points y: labels
features decision boundary

Feature Selection

Feature Selection

Nearest Neighbor Classiﬁer

Nearest Neighbor
Predict class of new data point by majority vote of K nearest neighbors

1-Nearest Neighbor

1-Nearest Neighbor

1-Nearest Neighbor

1-Nearest Neighbor

1-NN Properties
• Simple and quite good for low dimensional data • “Rough” decision boundary, may have “islands” • Training complexity for N data points? • Test complexity for M data points? • Error on training set? • Variance? Bias?

k=1 k=5

k=3 k=9

k-NN Properties
• Gets rid of “islands” • If k is too large, the boundary may become too
smooth
• Lower variance, but increased bias • How do we choose the ideal k?

Validation
• Train	  on	  training	  data,	  test	  on	  test	  data	   • Pick	  the	  k	  with	  the	  lowest	  test	  error	  
• Training	  data:	  train	  classifier	   • Test	  data:	  measure	  performance

Cross-Validation

• Training	  data:	  train	  classifier	  

validation	  	   data

• Validation	  data:	  estimate	  (hyper)	  parameters	  (k)	  

• Test	  data:	  measure	  performance

Cross-Validation
1. Iterate over choice of validation fold 2. For all parameter values:
a. Train with training data b. Validate with validation data
3. Average the parameters with best performance on validation data The test data is NOT used to determine the parameters!

Cross-Validation
• Take best parameters • Train on training data and validation data
together
• Test performance on test data
Evaluate on the test set only a single time, at the very end!

CIFAR-10 Data Set
60,000 images 32x32 pixels
10 classes
Training set: 50,000 images
Test set: 10,000 images
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton

High-Dimensional Data

Each image is a vector in 32 x 32 x 3 = 3,072 dimensional space

32 32

3,072

Unroll

Image Comparison
p: pixel I1: image 1
Fei-Fei Li & Andrej Karpathy, Stanford

Distance Metrics
L1 (Manhattan) distance: L2 (Euclidean) distance: More general Lp norms:
Fei-Fei Li & Andrej Karpathy, Stanford

k-NN Classiﬁers
https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm

5-fold Cross Validation

k=7

Alex Krizhevsky, Stanford

10 Nearest Neighbors
Accuracy: L1 distance: 38% / L2 distance 35%
Alex Krizhevsky, Stanford

Pixel-based L2 Distances
These images have the same L2 pixel distance
Alex Krizhevsky, Stanford

Image Features
Fei-Fei Li, Alex Krizhevsky, Stanford

Self-Driving Cars

Car Features

Laser scan

Intensity model Elevation model

Camera vision 2D stationary map Lane model

Why don’t we just use more and more features?

Curse of Dimensionality
• When dimensionality increases, the volume of the space increases so fast that the available data becomes sparse
• Statistically sound result requires the sample size N to grow exponentially with d

Dimensionality Reduction

High-dimensional Data
Based on slide from P. Liang

Basic Idea
Project the high-dimensional data onto a lowerdimensional subspace that best “ﬁts” the data
Based on slide from P. Liang

Linear Methods
• Does the data lie mostly in a hyperplane? • If so, what is its intrinsic dimensionality d?
Based on slide from F. Sha

Uses
• Dimensionality reduction for supervised learning • Compression • Visualization
MusicBox, Anita Lillie, MIT

Principal Components Analysis (PCA)

Example

Is the intrinsic dimensionality of this data:

a)

1D

b)

2D

c)

1.5 D

Based on slide from A. Ihler

Example
v: chosen to minimize orthogonal distances Equivalent: v is the direction of maximum variance (max. the spread along v)
Based on slide from A. Ihler

Linear Regression vs. PCA

Linear Regression

PCA

http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues

PCA Algorithm

• Subtract mean from data (center X)

• (Typically) scale each dimension by its variance

• Helps to pay less attention to magnitude of dimensions

• Compute covariance matrix S

S = 1 X|X N

• Compute k largest eigenvectors of S

• These eigenvectors are the k principal components

Based on slide from A. Ihler

How many PC vectors?
Enough PC vectors to cover 80-90% of the variance
Screeplot
Based on slide from J. Leskovec

Dimensionality Reduction
Has?e	  et	  al.,”The	  Elements	  of	  Sta?s?cal	  Learning:	  Data	  Mining,	  Inference,	  and	  Predic?on”,	  Springer	  (2009)

PCA for Handwritten Digits
Has?e	  et	  al.,”The	  Elements	  of	  Sta?s?cal	  Learning:	  Data	  Mining,	  Inference,	  and	  Predic?on”,	  Springer	  (2009)

PCA for Handwritten Digits
Has?e	  et	  al.,”The	  Elements	  of	  Sta?s?cal	  Learning:	  Data	  Mining,	  Inference,	  and	  Predic?on”,	  Springer	  (2009)

PCA for Face Images
Based on slide from T. Yang

PCA for Face Images
• 64x64 images of faces = 4096 dimensional data
Based on slide from A. Ihler

“Eigenfaces”
We can reconstruct each face as a linear combination of “basis” PC vectors, or Eigenfaces [M.Turk and A. Pentland (1991)]

+
Average Face

Eigenfaces

Based on slide from T. Yang

Reconstruction
90% variance is captured by the ﬁrst 50 eigenvectors

V0
for n lights

∑

x

Based on slide from T. Yang

Multidimensional Scaling (MDS)

Multi-Dimensional Scaling

• A	  diﬀerent	  goal	  :	  
–Find	  a	  set	  of	  points	  whose	  pairwise	  distances	  match	  a	   given	  distance	  matrix

p1 p2 p3 p4 p5 p1 0 1 2 3 1 p2 1 0 2 4 1 p3 2 2 0 1 3 p4 3 4 1 0 1 p5 1 1 3 1 0

p5
1 1
p1 1
p2

2

3

2

4
p3

p4

1

Classical MDS
• Given n x n matrix of pairwise distances between data points
• Compute n x k matrix X with coordinates of distances with some linear algebra magic
• Perform PCA on this matrix X

European Cities Data
• Distances between European cities:

Result of MDS

!

!

Based on slide from T. Yang

Color Images
N. Bonneel

Facebook Friends

– Distance	  =	  1	  for	  friends	   – Distance	  =	  2	  for	  friends	  of	  friends	  ;	  etc.

N. Bonneel

Nonlinear Methods

Data-Driven BRDFs
• Bi-Directional Reﬂectance Distribution Functions

Data-Driven BRDFs
• Measure light reﬂected off a sphere • 20-80 million measurements (6000 images) per
material

Data-Driven BRDFs

• Each tabulated BRDF is a vector in 90x90x180x3 =4,374,000 dimensional space

180 90

Unroll

90

4,374,000

Eigenvalue magnitude

PCA

0

20

40

60

80

100

120

Dimension

mean

5

10

20

30

45

60

all

PCA
• First 11 PCA components

PCA Interpolation

Then, one day...

Why do linear models fail?
PCA

Why do linear models fail?
• Classic “Swiss Roll” example

xi

PCA
Based on slide from F. Sha

Non-Linear Manifold Methods

Back-projection

Projection

Linear Embedding Space

Non-Linear Manifold Methods
• Intuition: Distortion in local areas, but faithful in the global structure
Based on slide from F. Sha

Non-Linear BRDF Model
• 15-dimensional space (instead of 45 PCs) • More robust - allows extrapolations

Linear Model Extrapolation

Non-linear Model Extrapolation

Dimensionality Reduction
• Linear methods:
• Principal Component Analysis (PCA) – Hotelling[33] • Singular Value Decomposition (SVD) – Eckart/Young[36] • Multidimensional Scaling (MDS) – Young[38]
• Nonlinear methods:
• IsoMap – Tenenbaum[00] • Locally Linear Embeddings (LLE) – Roweis[00]

Further Reading
http://cs231n.github.io

