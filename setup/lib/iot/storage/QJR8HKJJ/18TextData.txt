CS109/Stat121/AC209/E-109 Data Science
Bayesian Methods Continued,Text Data
Hanspeter Pﬁster, Joe Blitzstein,Verena Kaynig

Topics

gene dna genetic .,,

0.04 0.02 0.01

life

0.02

evolve 0.01

organism 0.01

.,,

brain neuron nerve ...

0.04 0.02 0.01

data

0.02

number 0.02

computer 0.01

.,,

Documents

Topic proportions and assignments

Blei, https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf Figure 1: The intuitions behind latent Dirichlet allocation. We assume that some number of “topics,” which are distributions over words, exist for the whole collection (far left). Each document is assumed to be generated as follows. First choose a distribution over the topics (the histogram at right); then, for each word, choose a topic assignment (the colored coins) and choose the word from the corresponding topic. The topics and topic assignments

This Week
• Project team info is due tonight at 11:59 pm via the Google form: http://goo.gl/forms/CzVRluCZk6
• HW4 is due this Thursday (Nov 5) at 11:59 pm • Before this Thursday’s lecture on interactive
visualizations:
• Download/install Tableau Public at https://public.tableau.com/
• Download data ﬁle (.zip) from http://bit.ly/cs109data

MCMC as mountain exploration vs.
http://healthyalgorithms.com/2010/03/12/a-useful-metaphor-for-explaining-mcmc/

Bayesian Hierarchical Models: Radon Example
Example from Gelman http://www.eecs.berkeley.edu/~russell/ classes/cs294/f05/papers/gelman-2005.pdf Python-based exposition at
http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/

Complete Pooling vs. No pooling complete pooling: radoni,c = ↵ + · ﬂoori,c + ✏

no pooling:

radoni,c = ↵c + c · ﬂoori,c + ✏c

no pooling:

Partial Pooling

partial pooling/ hierarchical model:

Partial Pooling
radoni,c = ↵c + c · ﬂoori,c + ✏c ↵c ⇠ N (µ↵, ↵2 ) c ⇠ N (µ , 2 )

http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/

Hierarchical Models Provide:

• a compromise between no pooling and complete pooling

•

regularization and shrinkage

•

give sensible estimates even for small groups

•

organize the parameters in an interpretable way

• incorporate information at different levels in the hierarchy

(e.g., individual level, county level, state level)

• predictions at various levels of the hierarchy (e.g., for

new house or for new county)

Gibbs Sampler
Explore space by updating one coordinate at a time.
2D parameter space version:
Draw new ✓1 from conditional distribution of ✓1|✓2 Draw new ✓2 from conditional distribution of ✓2|✓1
Repeat
http://zoonek.free.fr/blosxom//R/2006-06-22_useR2006_rbiNormGiggs.png

Gibbs sampler animation http://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/

Metropolis-Hastings Algorithm

Modify a Markov chain on a state space of interest to obtain

a new chain with any desired stationary distribution!

2

CHAPTER 1. MARKOV CHAIN MONTE CARLO

1. If = , propose a new state using the transition probabilities of the

Xn i

j

pij

original Markov chain.

2. Compute an acceptance probability,

✓

◆

= min sjpji 1

aij

,.

sipij

3. Flip a coin that lands Heads with probability , independently of the Markov aij
chain.

4. If the coin lands Heads, accept the proposal and set Xn+1 = j. Otherwise, stay

in

state

; i

set

Xn+1

=

. i

In other words, the modiﬁed Markov chain uses the original transition probabilities

to propose where to go next, then accepts the proposal with probability ,

pij

aij

https://www.siam.org/pdf/news/637.pdf

Metropolis-Hastings animation http://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/

MCMC in Python
• Stan: http://mc-stan.org • PyMC: https://pymc-devs.github.io/pymc/

Mosteller-Wallace, Federalist Papers Authorship

Mosteller-Wallace, Federalist Papers Authorship
https://www.stat.cmu.edu/Exams/mosteller.pdf

onian. In combination with a similar treatment of other “non-contextual” words in these
writings, Uthissaepprooafc“h upropvoidend”strobnyg eHvidaenmce ithltatoMnadivsosn.wMas athde aiustohonr of all twelve
of the disputed papers, essentially settling the authorship debate.

Rate/1000 Words Exactly 0 (0.0, 0.4) [0.4, 0.8) [0.8, 1.2) [1.2, 1.6) [1.6, 2.0) [2.0, 3.0) [3.0, 4.0) [4.0, 5.0) [5.0, 6.0) [6.0, 7.0) [7.0, 8.0) Totals:

Authored by Hamilton 0 0 0 2 3 6 11 11 10 3 1 1 48

Authored by Madison 41 2 4 1 2 0 0 0 0 0 0 0 50

12 Disputed Papers 11 0 0 1 0 0 0 0 0 0 0 0 12

Table froTmablSea1m.2a.1n.iFergeoqu, eSntcoycdhiastsrtiibcutMioondoef ltihnegwaonrdd “Mupaothn”emina1t1ic0aelsSsatyast.istics

But what is the probability that Madison authored a Exercises 1.2.particular disputed document, and how conﬁdent
1. Sfapirecciofyint,haendsasmppecleifyspsaahcsoetoufcolhdrasthtwieceemxbopdeeerilmafboenrottuhciotsnoesxiusptreinraigmnoesfnwtt.heUrerse?incgonthseact umtiovdeetlo, scsoems pouftae

Poisson Model
ey f (y| ) =
y! y
is the number of occurrences of a speciﬁc word is the rate parameter
p / a 1e b Gamma prior is conjugate: ( )

Likelihood and Posterior for Madison’s use of “from”
Posterior Likelihood

6

5

4

3

dgamma(x, shape = 331.6, rate = 270.3)

2

1

0

0.9

1.0

1.1

1.2

1.3

1.4

1.5

1.6

x

g. 12.2 Posterior and the likelihood function for the rate of using the word fr

n-grams
Data science is fun.
Unigrams: look at individual words. “data”, “science”, “is”, “fun”
Bigrams: look at word pairs. “data science”, “science is”, “is fun”
Trigrams: look at word triplets. “data science is”, “science is fun”

n-grams: Randomized Hobbit
into trees, and then bore to the Mountain to go through?” groaned the hobbit. “Well, are you doing, And where are you doing, And where are you?” it squeaked, as it was no answer. They were surly and angry and puzzled at ﬁnding them here in their holes
Karl Broman, Randomized Hobbit http://www.r-bloggers.com/randomized-hobbit/

n-grams: Hobbit/Cat in the Hat Mixture
“I am Gandalf,” said the ﬁsh. This is no way at all! already off his horse and among the goblin and the dragon, who had remained behind to guard the door. “Something is outside!” Bilbo’s heart jumped into his boat on to sharp rocks below; but there was a good game, Said our ﬁsh No! No! Those Things should not ﬂy.
Karl Broman, Randomized Hobbit http://www.r-bloggers.com/randomized-hobbit/

if current == ".": return " ".join(result) # if "." we're done
The sentences it produces are gibbernis-hg, rbuatmthsey’re the kind of gibberish you might
put on your website if you were trying to sound data-sciencey. For example:
If you may know which are you want to data sort the data feeds web friend someone on trending topics as the data in Hadoop is the data science requires a book demonstrates why visualizations are but we do massive correlations across many commercial disk drives in Python language and creates more tractable form making connections then use and uses it to solve a data.
—Bigram Model
We can make the sentences less gibberishy by looking at trigrams, triplets of consecu‐ tive words. (More generally, you might look at n-grams consisting of n consecutive owuosIirnndtswsihg,oihbnwtudsotsiirntgdhthosrt:eheMowwapilRelcebodenuopcmeleinseetsyemwfoosrrkliukTse.h)aaNnt’soewnpoittdheaemqtirucaensatsniiodtnioifnwsesowcoidluloleddseeptvheenantdhgoaivnveethauseskpendreewavi‐
fetwriygeraarmssth=erzeihp(adsobceuemnenitn,strduomcuemnetnetd[.1:], document[2:]) trigram_transitions = defaultdict(list) —Trigram Model
starts = []
Of course, they sound better because at each step the generation process has fewe choicefso,rapnrdeva,t cmurarnenyts,tenpexstoinnlytraigsrinamgsl:e choice. This means that you frequently gen erate sentiefncpersev(o=r= a"t.l"e:aJsot elol nGgrupsh,rDasa#etasi)fSthcthiaeetnwpcreeervfeiroosumesenS"wcvoreradrtb"cahwtaims ainpetrhieodoriginal data Having morestdaarttas.wapopuenldd(chuerlrpe;nti)t wo#utldhenaltshoiswiosrka bsteatrtetrwiofrdyou collected n-gram rom multiple essays about data science.
trigram_transitions[(prev, current)].append(next)

Topics

gene dna genetic .,,

0.04 0.02 0.01

life

0.02

evolve 0.01

organism 0.01

.,,

brain neuron nerve ...

0.04 0.02 0.01

data

0.02

number 0.02

computer 0.01

.,,

Topic Modeling
Documents

Topic proportions and assignments

BFnuilgmeubri,eerh1o:ftt“Tpthospe:ic/is/n,w”tuwwithiwiochn.csarsbe.edphisritnirnidbcultaeitotenons ntov.Deeridrwiocuhr/dl~se,tbexalilesltoi/fcopartatiohpene.wrhWsole/eBacoslesllueimc2tei0ont1h(1afat.rpsloedmftf)e.
Each document is assumed to be generated as follows. First choose a distribution over the

Topic Modeling

genetics evolution
brain computing

Topics

gene dna genetic .,,

0.04 0.02 0.01

life

0.02

evolve 0.01

organism 0.01

.,,

brain neuron nerve ...

0.04 0.02 0.01

data

0.02

number 0.02

computer 0.01

.,,

Documents

Topic proportions and assignments

Figure 1: The intuitions behind latent Dirichlet allocation. We assume that some number of “topics,” which are distributions over words, exist for the whole collection (far left). Each document is assumed to be generated as follows. First choose a distribution over the topics (the histogram at right); then, for each word, choose a topic assignment (the colored coins) and choose the word from the corresponding topic. The topics and topic assignments in this ﬁgure are illustrative—they are not ﬁt from real data. See Figure 2 for topics ﬁt from data.
Blei, https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf model assumes the documents arose. (The interpretation of LDA as a probabilistic model is ﬂeshed out below in Section 2.1.)

17,000 articles from Science, 100 topics

0.4

0.3

0.2

Probability

1 8 16 26 36 46 56 66 76 86 96 Topics

“Genetics” human genome dna genetic genes sequence gene
molecular sequencing
map information
genetics mapping project sequences

“Evolution” evolution
evolutionary species
organisms life
origin biology groups phylogenetic living diversity group
new two common

“Disease” disease host bacteria diseases
resistance bacterial
new strains control infectious malaria parasite parasites united tuberculosis

“Computers” computer models information data computers system network systems model parallel methods networks software new simulations

0.1

0.0

Figure 2:

We ﬁt a 100-topic LDA model to 17,000 articles

Real inference with LDA.

fFriogBmurleteh1ie,. jAhouttrtrnpigahlstS:ac//irewentcwhee.wtAo.tpcl1es5ft.mpisortsihtnefcrieneqfueterornentdw.etoordpdisuc f/pr~orombpotlhreteiio/mnposasftoprfreethqreuseen/xBtatmloeppiil2ces0afro1tuic1nlde.piinndf

this article.

is drawn from one of the topics (step #2b), where the selected topic is chosen from the per-document distribution over topics (step #2a).2

Latent Dirichlet Allocation (LDA): Generation and Estimation
http://mcburton.net/blog/joy-of-tm/

Dirichlet Distribution
http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/

Latent Dirichlet Allocation (LDA): Generative Model
https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation

Latent Dirichlet Allocation (LDA): Generative Model Example
• Pick 5 to be the number of words in D. • Decide that D will be 1/2 about food and 1/2 about cute animals. • Pick the first word to come from the food topic, which then gives you the word “broccoli”. • Pick the second word to come from the cute animals topic, which gives you “panda”. • Pick the third word to come from the cute animals topic, giving you “adorable”. • Pick the fourth word to come from the food topic, giving you “cherries”. • Pick the fifth word to come from the food topic, giving you “eating”.
http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/

Latent Dirichlet Allocation (LDA): Generative Model
http://mcburton.net/blog/joy-of-tm/

Recommendation Systems and LDA in the NY Times
http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-timesrecommendation-engine/?_r=2

Recommendation Systems and LDA in the NY Times
http://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-timesrecommendation-engine/?_r=2

LDA Visualization
http://cpsievert.github.io/LDAvis/reviews/reviews.html pyLDAvis: https://pypi.python.org/pypi/pyLDAvis

