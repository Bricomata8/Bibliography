CS109/Stat121/AC209/E-109 Data Science
Regression Continued
Hanspeter Pﬁster, Joe Blitzstein, and Verena Kaynig
y
residual
yˆ
column space of X

This Week
• HW2 due next Thursday (Oct 8) at 11:59 pm (Eastern Time)
• See updated Piazza posting guidelines (pinned note) and follow the format described there

http://www.phdcomics.com/comics.php?f=1823

NYC Housing Example

NYC Housing Example
Lander, R for Everyone; NYC Open Data

NYC Housing Example
Lander, R for Everyone; NYC Open Data

NYC Housing Example
• Response variable (y): price per square foot • Predictor variables (x’s): number of units in
complex, number of square feet, borough indicators
• Try linear regression; it may help to take logs of some of the continuous variables ﬁrst
Lander, R for Everyone; NYC Open Data

NYC Housing Example
Lander, R for Everyone; NYC Open Data

NYC Housing Example
Lander, R for Everyone; NYC Open Data

Where did the Bronx go? Why are SqFT and Units so close to 0?
Lander, R for Everyone; NYC Open Data

Why are SqFT and Units so close to 0? Where did the Bronx go?
Lander, R for Everyone; NYC Open Data

Collinearity
• Should avoid having predictor variables that are highly correlated with each other (collinearity results in instability, high variances in estimates, and worse interpretability)
• An extreme case of collinearity would be also including a Bronx indicator in the NYC Housing example. Instead, use one borough as a baseline.

Predicting a Binary Response
4.3 Logistic Regression 131

|| |

| || | ||||| || ||||||||||||||||||| |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |||| | | | | | |

|| |

| || | ||||| || ||||||||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |||| | | | | | |

Probability of Default
0.0 0.2 0.4 0.6 0.8 1.0
Probability of Default
0.0 0.2 0.4 0.6 0.8 1.0

|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |||||||| ||||||| | | ||

|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |||||||| ||||||| | | ||

0

500 1000 1500 2000 2500

Balance

0

500 1000 1500 2000 2500

Balance

FIGURE 4.2. sCourlcae:sInstrioﬁducctaiotnitooSntatiusticsailnLegarntinhg,eJamDees,fWaittuenl, Htastdiea, Ttibash.iraLni,ehftttp:://wEwws-btcifm.usca.etdeu/d~gaprertho/ISbLa/ bility of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default(No or Yes). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1.

Fen-Phen Case Study
On July 8, 1997 Mayo Clinic investigators described 24 cases of valvular heart disease in patients taking the recently released appetite suppressant combination fen/phen (fenfluramine plus phentermine). The FDA issued an advisory to encourage reporting of similar cases.
Example from Stat S105 with Xiao-Li Meng, Yves Chretien

Strong Association?
Recall we obtained the following sample of patients in a follow-up study:

Heart disease No heart disease

Total

Fen/phen

53

Control

3

180

233

230

233

•So do you think there is strong association between heart disease and fen/phen usage?

•How would you defend your assertion scientifically? Can you just say “Well, 53/3=17.7 is very large to me”?

How about this one then?
Now suppose that instead of heart disease, you wanted to test whether fen/phen increased the risk of a rare type of cancer. Using the same patients, you observe that:

cancer

No cancer

Total

Fen/phen

1

Control

0

232

233

233

233

Is that the strongest evidence of association one can ever get, since 1/0 is infinite?

Measures of Association:   Odds Ratio
If someone’s probability of experiencing an outcome is p ,
then that person’s odds of the outcome are p/(1-p)

The odds ratio is the ratio of two different people’s odds of some outcome. If people in group A have probability pA of disease, and people in group B have probability pB, then the odds ratio of group A vs. group B is

Odds Ratio = pA 1− pA

pB 1− pB

=

(1 − (1 −

pB ) pA pA ) pB

Crude Odds Ratio Estimate

The data in one study were as follows:

Aortic

+

Regurgitation
-

Fen/phen

+

-

6

162

13

2343

19

2505

168 2356 2524

A crude estimate of the odds ratio is

6× 2343 13×162

=

6.7

Palmieri V, Arnett DK, Roman MJ, Liu JE, Bella JN, Oberman A, Kitzman DW, Hopkins PN, Morgan D, de Simone G, Devereux RB. Appetite suppressants and valvular heart disease in a population-based sample: the HyperGEN study. Am J Med. 2002 Jun 15;112(9):710-5. 35

What about confounding factors?
But what if there are confounding factors? For example, what if fen/phen users are more likely to be obese, and obesity increases the risk of heart disease?
We can set up a logistic regression model to predict a person’s odds of heart disease, given the predictor variables.
We can also use this to compare fen/phen users vs. nonfen/phen users, controlling for the other predictors.
Then we can use the data to estimate the parameters, using Maximum Likelihood Estimation (MLE).

Variables in the model

Y

=

!1, "#0,

if if

cardiac not

valve

abnormality

!1, if taking fen/phen X fen = "#0, if not X age = subject's age
!1, male X sex = "#0, if female (plus other X -variables...)

( ) p = P Y = 1| X fen , X age , X sex ,..., X k

So, how is p related to all of these factors?

A logistic regression model

✓◆

p

logit(p) = ln

p

=

0+

X
f en f en

+

age

X
age

+

X
sex sex

+

·

·

·

+

X
kk

1

The parameters of the model (the β’s) are unknown, and are estimate from the data using MLE.

This gave 1.84 as an estimate for the fen/phen parameter. How can that be interpreted?

Two patients, A and B, are the same age, same gender, and similarly identical on all other variables. Patient A has taken fen/phen and Patient B has not. The model predicts that

✓◆

p

p logit( A) = ln

p

=

0+

X age age +

X ··· sex sex + +

X k k+

X
f en f en

1

✓◆

p

p logit( B) = ln

p

=

0+

X age age +

X ··· sex sex + +

X
kk

1

fen = logit(pA)

logit(pB) = ln

pA !
1 pA pB
1 pB

Using this model we can estimate an “adjusted” odds ratio that’s the odds ratio for two people with all other known factors held constant:

e ˆfen = e1.84 ⇡ 6.3

his formula came I noted immediately that the val- expands faster. In fact, the curse is far

Curse of Dimensionality cally and histori- ues for one, two and three dimensions more damning: At the same time the
rely note that the agreed with the results I already knew. cube inflates exponentially, the ball

ula that ventures (This kind of confirmation is always shrinks to insignificance. In a space of

etic is the gamma an elaboration on

Forfoerar stashueurfinnirgisftowtirmhmeen.)lyyI oarulsaornuodnbosaemrpvreopdgortahimantt

i1vn0o0laudmibmeoefixnllseiwdonbisyt, htthheesfbirdaacleltiholanesondfegtchtliehnceu2db,tioc

For positive inte- wthehvaotluims ethweas pslorwolbyainbcirleiatsyintghwaitththe1.8p×o10in–7t0. Tishiisnistfharesumnalilterbthaalln? the

×2×3 ×...× n. But n, as I had expected.

volume of an atom in relation to the

2-ball in 2-cube

3-ball in 3-cube

r=1 1.0

r=1

r=1

s=2 volume ratio = 0.79

s=2 volume ratio = 0.52

ple system for studying geometry across a series of spatial dimensions. A ball is the solid object bounded by a sphere; sides of length 2, which msoaukrecset:hAemn AjudsvtelanrtguereeninouthgehntothaDcciommemnsoiodnat,eBaribaanllHoafyreads,iuAsm1.eIrnicoanneSdciimenetnistio2n0(1l1eft) the e same shape: a line segment of length 2. In two dimensions (middle) and three dimensions (right) the ball and cube
s dimension increases, the ball fills a smaller and smaller fraction of the cube’s internal volume. In three dimensions t half; in 100-dimensional space, the ball has all but vanished, filling only 1.8 × 10–70 of the cube’s volume.

Curse of Dimensionality
For a uniformly random point in the box in d dimensions with length 2 in each dimension, what is the probability that
the random vector is in the unit ball in d dimensions?

d

probability

2

0.79

3

0.52

6

0.08

10

0.002

15

0.00001

100

1.87 · 10-70

In many high-dimensional settings, the vast majority of data will be near the boundaries, not in the center.

Interpolation vs. Extrapolation
source: https://xkcd.com/605/
In high dimensions, nearest neighbor point tends to be very far away. May be very hard to interpolate well, even with a lot of data points.

Blessing of Dimensionality
In statistics, “curse of dimensionality” is often used to refer to the difficulty of ﬁtting a model when many possible predictors are available. But this expression bothers me, because more predictors is more data, and it should not be a “curse” to have more data....
With multilevel modeling, there is no curse of dimensionality. When many measurements are taken on each observation, these measurements can themselves be grouped. Having more measurements in a group gives us more data to estimate group-level parameters (such as the standard deviation of the group effects and also coefficients for group-level predictors, if available).
In all the realistic “curse of dimensionality” problems I’ve seen, the dimensions–the predictors–have a structure. The data don’t sit in an abstract K-dimensional space; they are units with K measurements that have names, orderings, etc.
Andrew Gelman, http://andrewgelman.com/2004/10/27/the_blessing_of/

Tall data vs. wide data
vs.
n rows (individuals), p columns (variables)

p measurements
n people
Wide data are increasingly common in applications, e.g., neuroimaging, microarrays, MOOC data. But many traditional
statistical methods assume n greater than p.

Ridge RegresX snio0n and ShrXpinkage12

RSS = @yi 0

j xij A .

i=1

j=1

In a linear regression model, in place of minimizing the sum

of sq•uaInrecdonrterassitd,utahles,rrididgegerergeregsrseiosnsicoone saciyesnttoestmiminaitmesizˆeR

are the values that minimize

0 Xn
@yi

Xp

12

Xp

Xp

0

j xij A +

2 j

=

RSS

+

2
j,

i=1

j=1

j=1

j=1

where 0 is a

, to be determined

tuning parameter

separately.

27 / 57

Stein’s

Paradox

and

Shrinkage

Estimation

1
1

Let

Let

y1

⇠

N

(✓1,

1) ,

y2

eys1tim⇠atNe

(th✓1e, v1e)c,tyo2r

⇠,
✓

u⇠Nnd(Ne✓r2(,✓s12u,)m1, ).,o..f..,s.yq,kuyak⇠re⇠dNNer(r✓(o✓krk,,1l1o))ssw?wiitthh

k
k

estimate the vector , under sum of squared error loss?

✓

3. How should we
3. How should we

A ceSrtteaiinn: ctohuervseechtoars a ifsreshmen, b so;puhnoimfoormrelsy, cbejautneinorbsy, athned Jdasmeensi-oSrtse.inLeesttXimabteorthe

number of freshmen yandinsoapdmhoismsiobrlees (✓total), Y b◆e the number of juniors, and Z be

the is

number of seniors in replacement (all

saturdaenndtosm✓ˆajrs=eameqp1ulealolP ykf sliikz22eelyny,tjwo. hbeerechfoorsepnaretac(ha)ttimhee,saamndpltinhge

with

y

ii

same student can be chosen more than once) and for parts (b),(c) the sampling is

withAoucterrteapinlaccoeumrseentha(saall

fsrteushdmenetns,

arseopehqoumaollryesl,ikejluynitoorsb, eancdhosseennieoarsc.hLteitme,beextcheept

b

c

d

X

thatnaumstbuedreonftfrceasnh’mt ebne acnhdosseonphmomoroeretsha(tnotoanl)c,e). be the number of juniors, and be

Y

Z

(a) FtishinewdintuhtmhrebepejorlaioncfetmsPeenMniotFr(saoilnfl sXatu,rdaYen,ndZtos,maforseramseaqpmuleaplollyifnslgiikzweelyint,htworhebeperleachcfooermsepneanretta(c(shai)mttimphleeif,syaa)mn. dpltinhge

(b) Fsainmde tshtuedjeonitntcaPnMbFe cohfoXse,nYm, Zor,efotrhasnamonpclien)ganwditfhooruptarretps l(abc)e,m(c)entthe(ssiammpplilfiyn)g. is

(cca)lcFuwtihlinaatdhttiooEauntsstX2ru(edspieflmoanrctpeslcmiaafmynen)’pt.tlbi(neaglcl hwsotiustehdnoenumttsorraeerpetlhaecaqenumaolenlnyctel,)i.kperleyfetroabbleycwhoitshenouetaachnyticmoem, pelxicceaptted

(a) Find the joint PMF of X, Y, Z, for sampling with replacement (simplify).

S(hbo)wFitnhdatthe joint PMF of

, for sampling without replacement (simplify).

X, Y, Z

(cca)lcFuilnadtiX oE=nnsX2✓(snkifmo◆rpxslkiaf(my1)p. linxg)wn itkh=ouZt 0rexp(lkacem1e)nn!(t!n, prefke)r!atbkly1w(1ithotu)tnankydtc,omplicated

kj

Show that

.

withLoeuttXu1s,inXg2,cX a. n.lc. u,✓lXunsn◆bke(i1.i.d.

Z
)cnonkti=nuouxs

r.v.s

wni!th

PDFkf ,1(a1nd

le)nt

Rk

=

[X(1),

X(n)].

Let

N

be

the

nu=mbekr

oxf

additxional

i.i.d0 .

d(kraw1s)!f(rnom

k)!nteeded f

tot

getdat, value

not

in

. R

TMwhioteshntoeuEllte(urNs“i)nOkg=pjctanimlScauoanlluudsLr.Pcene(gN:tEh>froofkn)P-=lMay(onf+ornkrr()in(asn,+B1Sk)icn1ioe)m.nitaiﬁl Gc aAmme”erican

1977

Let X1, X2, . . . , Xn be i.i.d. continuous r.v.s with PDF f , and let R = [X(1), X(n)].

that involve jusLt Aa SsuSbOsetanodf tShpe avrasriitaybles, ridge regression
will include all p predictors in the ﬁnal model
•InTrcaeohgeleirneLesocsaaifiersonssnrqtoesut,giahsrrˆaeeaLtsd,sroiemrovleaenisrnticimdivomueomalidyzlesees,rletL,thcihAneeiSnspqStlduOaaicaslenatsedtaoriyvtnfsayamnttotiiavngmeimeti.nioziTmirnhigidzegetlehaesssoum

0 Xn
@yi
i=1

Xp

12

Xp

Xp

0

j xij A +

| j| = RSS +

|

j

| .

j=1

j=1

j=1

• In statistical parlance, the lasso uses an `1 (pronounced

“ell coe

1c”i)enpntTeunvmhaeibsclttehyorerilonpfsstvieisanargddiiauvobcefleneassnbpoya`nr2kesipthyke,a1nsra=etltodP yu.dceT|inahjlge|w.t`hi1tehn. orm

of

a

33 / 57

LASSO vs. Ridge Constraints
222 6. Linear Model Selection and Regularization

β2

β^

β2

β^

β1

β1

FIGURE 6.7. Contours of the error and constraint functions for the lasso (lefts)ouracne:dIntroiddugcetiornetgorSetsastisioticnal L(erairgnhintg),.JaTmhese, Wsoittleind, Hbalsutie, Taibreshaisranai,rhettpt:/h/wewwco-bncfs.utrsac.iendtu/~rgea-reth/ISL/ gions, |β1| + |β2| ≤ s and β12 + β22 ≤ s, while the red ellipses are the contours of the RSS.
circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is suﬃciently large, then the constraint regions will con-

