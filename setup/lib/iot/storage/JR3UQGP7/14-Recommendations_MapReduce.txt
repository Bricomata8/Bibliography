CS109 – Data Science
Joe Blitzstein, Hanspeter Pfister, Verena Kaynig-Fittkau vkaynig@seas.harvard.edu staff@cs109.org

Announcements
• Grades for HW2 are getting out tonight • Final Projects:
– 3-4 persons per team

Next Topics
• ML best practices
– imbalanced data – missing values
• Recommender systems
– collaborative filtering – content-based filtering
• Map Reduce

Cross Validation

training validation test

data

data

data

• Training data: train classifier • Validation data: estimate hyper parameters • Test data: estimate performance

• Be mindful of validation and test set, validation set might refer to test set in some papers.

5 – Fold Cross Validation

5 – Fold Cross Validation

Last Step of Each Fold
1. Take best parameters 2. Train on training data and validation data
together 3. Test performance on test data
This is the final result of your method.

Things to Keep in Mind
• How do you aggregate the parameters?
• What if the hyperparameters are all over the place?
• What if the hyperparameters are at the border of your grid search window?

Scenario - 1
• 1. Screen the predictors: find a subset of “good” predictors that show fairly strong (univariate) correlation with the class labels
• 2. Using just this subset of predictors, build a multivariate classifier.
• 3. Use cross-validation to estimate the unknown tuning parameters and to estimate the prediction error of the final model.
Hastie-Tibsherani_Friedman, “The Elements of Statistical Learning”

Scenario - 2
• 1. Divide the samples into K cross-validation folds (groups) at random.
• 2. For each fold k = 1, 2, . . . ,K
– Find a subset of “good” predictors that show fairly strong (uni-variate) correlation with the class labels, using all of the samples except those in fold k.
– Using just this subset of predictors, build a multivariate classifier, using all of the samples except those in fold k.
– Use the classifier to predict the class labels for the samples in fold k.
Hastie-Tibsherani_Friedman, “The Elements of Statistical Learning”

Effect of Sample Size
5-fold cross validation: • n=200 => 160 samples • n=50 => 40 samples
Hastie et al.,”The Elements of Statistical Learning: Data Mining, Inference, and Prediction”

Cross Validation Over Estimates Error
Hastie et al.,”The Elements of Statistical Learning: Data Mining, Inference, and Prediction”

Normalization
• Be very careful. • Do not leak into the test data. • Think about what is useful.

Example PCA on MNIST
standard PCA

Example PCA on MNIST
PCA with normalized std dev

Normalization - 1

training

validation

test

Estimate mean values and normalize.

Estimate mean values and normalize.

Estimate mean values and normalize.

Normalization - 2

training

training

Estimate mean

values

normalize validation

test

Know Your Data

Imbalanced Data
• subsample • oversample • re-weight sample points • use clustering to reduce majority class
• re-calibrate classifier output
• Beware the easy true negatives

Imbalanced Classes
• The Problem: • Oversample: • Subsample: • Subsample for each tree in a random forest

Example: Random Forest Subsampling
sample
train

Class Weights
http://scikitlearn.org/stable/_images/plot_separating_hyperplane_unbalanced_0011.png

Cross Validation with Imbalanced Classes
• Think about using stratified sampling to generate the folds
• The goal is to have the same class ratio in training, validation and test set.

Missing data
• Delete data points
– Can cause sample size to be way too small
• Use the mean of the feature
– Does not change the sample mean, but is independent of the other features.
• Use regression to estimate the value
– Values will be deterministic

Recommender Systems
• We are already surrounded by them

Good Resources (also for this lecture)
Survey on recommender systems by Michael D. Ekstrand et al. • http://files.grouplens.org/papers/FnT%20CF%
20Recsys%20Survey.pdf
Good slides from Stanford lecture by Lester Mackey • http://web.stanford.edu/~lmackey/papers/cf_
slides-pml09.pdf

Rating Matrix Completion Problem
https://en.wikipedia.org/wiki/Collaborative_filtering

Collaborative Filtering
Insight: Personal preferences are correlated • If Jack loves A and B, and Jill loves A, B, and C,
then Jack is more likely to love C
• Does not rely on item or user attributes (e.g. demographic info, author, genre)

Content-based Filtering
• Each item is described by a set of features • Measure similarity between items • Recommend items that are similar to the
items the User liked

Comparison
• Collaborative filtering:
– Items entirely described by user ratings – Good for new discoveries – People who like SciFi maybe also like Fantasy
• Content-based filtering:
– Predictions are in users comfort zone – Can start with a single item
• Can do a hybrid approach

User Based Collaborative Filtering
Intuition: • I like what people similar to me like • Users give ratings • People with similar ratings in the past
assumed to have similar ratings in the future

Item-based Collaborative Filtering
• Similar to user-based, but looks at the items instead of the users
• Useful if the user base is way larger than the number of items.
• More useful: Items are relatively stable in their rating, users vary more.

We Could Use Missing Data Strategies
All that we talked about earlier: • Omitting samples • Using the mean rating of an item • Doing regression

CF as Regression
• Choose favorite regression algorithm • Train a predictor for each item • Each user who rated that item provides one
sample • To predict rating of an item A, apply predictor
for A to the user’s incomplete ratings vector.

Recommendation by Regression
• Pros:
– Reduces recommendations to a well-studied problem
– Many good prediction algorithms available
• Cons:
– Have to handle tons of missing data – Training M predictors is expensive

KNN
https://en.wikipedia.org/wiki/Collaborative_filtering

KNN for Collaborative Filtering
• Widely used • Item-based and User-based focus • Represent each user as incomplete vector of item
ratings • Compute similarity between query user and all
other users • Find K most similar users who rated the query
item • Predict weighted average of ratings

Similarity Measures
• Pearson Correlation Coefficient
– bound between 1 and -1 – suffers from computing high similarity between
users with few ratings in common – set threshold for minimum number of co-rated
itemssuffers from computing high similarity between users with few ratings in common

Similarity Measures
• Cosine similarity
– vector-space approach based on linear algebra – Unknown ratings are considered to be 0 – this causes them to effectively drop out of the
numerator

Netflix Prize
• Remember when we saw the Netflix prize video they mentioned SVD
• SimonFunk did this publicly on his blog with the title “Try this at home”
• http://sifter.org/~simon/journal/20061027.2. html

Singular Value Decomposition
A
• If we know the SVD, we could compute the missing values in R.
• Try to infer SVD from matrix with missing data, and reconstruct full matrix R

Best SVD Explanation I have seen!
• Leskovec, Rajaraman, Ullman
• https://www.youtube.com/watch?v=YKmkAoI UxkU

https://www.youtube.com/watch?v=YKmkAoIUxkU

U: Users x Topics

Σ: Topics x Topics

https://www.youtube.com/watch?v=YKmkAoIUxkU

VT: Topics x Movies

SVD for Recommender Systems
• Not only good for estimating missing data • We might actually care about the topics more

What is Map Reduce
• programming model • addressing large data sets • parallel and distributed algorithms • cluster framework
• It also is a way of thinking!

Map Reduce Background
• Originally developed by Google • Apache Hadoop is open source
implementation in Java • MrJob is a Python interface to Hadoop

The Map and the Reduce
• Map:
– performs filtering and sorting
• Reduce:
– summary operation

k1 v1 k2 v2 k3 v3 k4 v4 k5 v5 k6 v6

map

map

map

map

Shuffle and Sort: aggregate values by keys

a 15

b 27

c 298

reduce
a6

reduce
b9

reduce
c 19

The Famous Word Count Example

Green Eggs and Ham
• Result of a bet: • Can Dr. Seuss write a book
using only 50 words? • Bennett Cerf (Dr. Seuss's
publisher) lost. • It is the fourth best
selling English-language children's hardcover book of all time.
http://en.wikipedia.org/wiki/Green_Eggs_and_Ham

Example Input File
…

Launching the Job

Output File
50 words in total

Culturomics

Anagram Finder
• Anagram: Words or phrases consisting of the same letters
• Examples:
– Dormitory – Dirty room – Astronomer – Moon starer – Election results – Lies let’s recount
• Verifying anagrams with map reduce • Input: file with one word per line
http://www.fun-with-words.com/anag_example.html

k1 v1 k2 v2 k3 v3 k4 v4 k5 v5 k6 v6

map

map

map

map

Shuffle and Sort: aggregate values by keys

a 15

b 27

c 298

reduce
a6

reduce
b9

reduce
c 19

Importance of Local Aggregation
• Ideal scaling characteristics:
– Twice the data, twice the running time – Twice the resources, half the running time
• Why can’t we achieve this?
– Synchronization requires communication – Communication kills performance
• Thus… avoid communication!
– Reduce intermediate data via local aggregation – Two possibilities:
• Combiners • In-mapper combining

k1 v1 k2 v2 k3 v3 k4 v4 k5 v5 k6 v6

map

map

map

map

a1 b2 combine

c3 c6 combine

a5 c2 combine

b7 c8 combine

a1 b2

c9

a5 c2

b7 c8

partition

partition

partition

partition

Shuffle and Sort: aggregate values by keys

a 15

b 27

c 298

reduce
r1 s1

reduce
r2 s2

reduce
r3 s3

Combiner
• “mini-reducers” • Takes mapper output before shuffle and sort • Can significantly reduce network traffic • No access to other mappers • Not guaranteed to get all values for a key • Not guaranteed to run at all! • Key and value output must match mapper
Why does the key and value output have to match the mapper output?

Word Count with Combiner

Combiner Design
• Combiners and reducers share same method signature
– Sometimes, reducers can serve as combiners – Often, not…
• Remember: combiners are optional optimizations
– Should not affect algorithm correctness – May be run 0, 1, or multiple times
• Example: find average of all integers associated with the same key

Computing the Mean: Version 1
Why can’t we use reducer as combiner?

Computing the Mean: Version 2
Why doesn’t this work?

Computing the Mean: Version 3
Fixed? What if combiner does not run?

In-Mapper Combining
• “Fold the functionality of the combiner into the mapper by preserving state across multiple map calls

In-Mapper Combining
• Advantages
– Speed – Why is this faster than actual combiners?
• Disadvantages
– Explicit memory management required – Potential for order-dependent bugs

Word Count with In-Mapper-Comb.

Which is better?
• For large dictionaries?
– Combiner has no memory problems
• For skewed word distributions (“the”)?
– In-mapper reduces load on reducer

Word of Caution
1!!

Pairs and Stripes:
• Term co-occurrence matrix for a text collection
– M = N x N matrix (N = vocabulary size) – Mij: number of times i and j co-occur in some
context – Context can be a sentence, sequence of m words,
etc. – In this case co-occurrence matrix is symmetric

MapReduce: Large Counting Problems
• Term co-occurrence matrix for a text collection = specific instance of a large counting problem
– A large event space (number of terms) – A large number of observations (the collection itself) – Goal: keep track of interesting statistics about the
events
• Basic approach
– Mappers generate partial counts – Reducers aggregate partial counts

First Try: “Pairs”
• Each mapper takes a sentence:
– Generate all co-occurring term pairs – For all pairs, emit (a, b) → count
• Reducers sum up counts associated with these pairs
• Use combiners!

Pairs: Pseudo-Code

“Pairs” Analysis
• Advantages
– Easy to implement, easy to understand
• Disadvantages
– Lots of pairs to sort and shuffle around – Not many opportunities for combiners to work

Another Try: “Stripes”

 Idea: group together pairs into an associative array

(a, b) → 1 (a, c) → 2 (a, d) → 5 (a, e) → 3 (a, f) → 2

a → { b: 1, c: 2, d: 5, e: 3, f: 2 }

 Each mapper takes a sentence:

 Generate all co-occurring term pairs  For each term, emit a → { b: countb, c: countc, d: countd … }
 Reducers perform element-wise sum of associative arrays

a → { b: 1, d: 5, e: 3 }

+ a → { b: 1, c: 2, d: 2,

f: 2 }

a → { b: 2, c: 2, d: 7, e: 3, f: 2 }

Stripes: Pseudo-Code

“Stripes” Analysis
• Advantages
– Far less sorting and shuffling of key-value pairs – Keys are less unique than in pairs approach – Can make better use of combiners
• Disadvantages
– More difficult to implement – Underlying object more heavyweight – Fundamental limitation in terms of size of event
space

Cluster size: 38 cores Data Source: Associated Press Worldstream (APW) of the English Gigaword Corpus (v3), which contains 2.27 million documents (1.8 GB compressed, 5.7 GB uncompressed)

Map Reduce for Machine Learning
• Random Forest? • SVM?

