Extrinsic calibration of a camera-LIDAR multi sensor system using a planar chessboard

Eung-su Kim
School of Computer Science & Engineering Kyungpook National University Daegu, South Korea jsm80607@gmail.com

Soon-Yong Park
School of Electronics Engineering Kyungpook National University
Daegu, South Korea sypark@knu.ac.kr

Abstract— Extrinsic calibration of imaging systems containing multi-sensors have enabled unprecedented capabilities in data fusion in the fields of computer vision and robotics throughout the past few decades. In this paper, we propose a simple mutual rotation-andtranslation estimation method for a multi-sensor system containing six omnidirectional RGB cameras and a common 3D Light Detection and Ranging (LIDAR) sensor using a planar chessboard pattern. We mount the sensors on a specially designed hexagonal plate, while considering each camera-LIDAR combination as an independent multi-sensor unit. For each unit, we simultaneously capture chessboard images and their three-dimensional (3D) point data at a few different orientations. Twodimensional (2D) chess corners are reprojected into 3D space for plane fitting. RANSAC algorithm is applied on LIDAR points before they are used for plane fitting. The mutual rotation between the camera and LIDAR is calculated by aligning the normal vectors of plane fitting results. An arbitrary point from the camera plane is projected on to the LIDAR plane and the distance is minimized to estimate the mutual translation. The accuracy of this proposed method is evaluated through scene fusion experiments.
Keywords—3D LIDAR; omnidirectional camera system; calibration
I. INTRODUCTION
3D environment mapping and localization for autonomous vehicles is a subject that has recently been studied extensively. In order to create high accuracy 3D maps, visual information and distance data must be acquired and mapped correctly in the environment. High-resolution multiple cameras and 3D LIDAR sensors can be used to obtain color and distance information of the environment; however, their relative transformation must be known precisely. Accurate extrinsic calibration between cameras and LIDAR sensors increase the reliability of data fusions between color information and 3D point data. A vast collection of extrinsic calibration methods has been proposed for camera-LIDAR multi-sensor systems ([1-5]), however, these methods depend on using special calibration boards or objects. [1] used a calibration board having three circular holes. They followed a circle fitting approach by trying to align disparity maps with circle centers of corresponding LIDAR data. Pusztai and Hajder [2] proposed another extrinsic calibration method using ordinary boxes. Planes of these boxes are fitted using 3D points of the LIDAR and used to estimate 3D corners of boxes. The relative rotation between camera-LIDAR unit is estimated by solving 2D and 3D box corners.

In this paper, we propose a simple mutual rotation (pose) and translation estimation method for a camera-LIDAR unit without using such special calibration boards or objects. In our approach, we use a simple planar chessboard pattern. Our method is based on the theories mentioned in [6], however, instead of using a single energy function to find the rotation and translation, we divide the process into two different approaches to increase the reliability. The contents of this paper are as follows: section II gives a brief description to our multi-sensor device configuration. Section III is divided into three subsections. Section III.A contains the plane fitting method of the LIDAR sensor while section III.B gives a small description to the plane fitting approach of the camera using reprojected 2D chess corner locations. Mutual rotation and translation estimation between each camera-LIDAR unit is briefly described in section III.C. Section IV contains experimental results and a few performance analyses. Final conclusions and a discussions are briefly summarized in section V.
II. DESIGN OF THE MULTI-SENSOR IMAGER Fig. 1 is a vehicle that is mounting with our multi-sensor system. The design of our multi-sensor system is graphically shown as a CAD model in Fig. 2. Most state-of-the-art methods have used FLIR Ladybug cameras to capture omnidirectional images. A collection of five spherical cameras with high resolution are embedded together as a single hardware to design these multi-sensor cameras. However, these cameras are more expensive, and sometimes cause connectivity errors. Instead of using expensive camera systems, we used six RGB FLIR Balckfly cameras with a field-of-view of 103.60. These cameras
Fig. 1. Test vehicle that mounted with our multi-sensor system (red circle).

978-1-7281-1340-1/19/$31.00 ©2019 IEEE

89

ICUFN 2019

We can calculate a best-fit plane of the chessboard in the LIDAR coordinate system by employing RANSAC. This plane is used to estimate the mutual pose of the camera coordinate system.

Fig. 2. The design of our six omnidirectional camera-LIDAR imaging system.
are mounted on the top of a hexagonal plate to cover the full 3600 view horizontally. A 16-channel Velodyne VLP-16 LIDAR sensor is also mounted on a separate plate and integrated on above the camera tier. The VLP-16 sensor uses an array of 16 infrared (IR) lasers paired with IR detectors to measure distances to objects. The vertical field of view of the sensor is 300 and can acquire data at a rate of up to 20 Hz for 3600 degrees horizontally. We considered each camera-LIDAR combination as a separate multi-sensor unit, having partially overlapping field-of-views. This is shown in Fig. 3.
III. EXTRINSIC SENSOR CALIBRATION
Our research is carried out in the following sections. First, 2D images and 3D data are captured at different locations with different orientations simultaneously - creating an N number of views. 2D images of the chessboard captured by the camera are reprojected to find 3D points of 2D chess corner locations and are fitted into a plane. 3D data of the LIDAR corresponding to the area of the chessboard are manually selected and a plane is fitted accordingly. Then the relative rotation between each view of the single camera-LIDAR unit is estimated by finding the normal vector of planes. To find the relative translation, we arbitrarily select a 3D point from the rotated camera plane at all views and project it onto the LIDAR plane. The distance between two 3D points are iteratively minimized until they are properly aligned. This way we can find the relative translation between camera-LIDAR unit.
A. Plane fitting of the 3D LIDAR sensor
Only the 3D points relate to the chessboard area are extracted from the LIDAR scan. We use the RANSAC [8] based plane fitting method to estimate the best plane equation. The best-fit plane of 3D points  = {, , … ,  }(,…,) of the chessboard at i-th frame is estimated using the inliers determined with the RANSAC algorithm. This RANSAC-based plane fitting algorithm can be described as:

B. Omnidirectional camera system
Extrinsic calibration between the camera and the LIDAR requires accurate intrinsic parameters of the camera. To estimate the intrinsic parameters of each single camera, we employed the well-known Zhang's calibration method [7]. First, we find the chess corner points from the camera image with subpixel precision. Then these corner points are reprojected into 3D space using a real scale value. Assuming  denotes an arbitrary chess corner point in 3D space, we can represent all the reprojected point locations as a single reconstruction result in the camera coordinate system. Finally, the reconstructed 3D corner points are used for plane fitting.

C. Calculating mutual rotation and translation between camera-LIDAR unit

In our proposed extrinsic calibration, we first estimate the mutual rotation between two sensors. Plane  and  represent the plane fitting results of the chessboard area for the camera and the LIDAR, respectively. We consider the LIDAR coordinate system as the world coordinate system. The mutual rotation matrix R from the camera coordinate system to the world coordinate system can expressed as an energy term
mentioned in (1):



  

 ∙ 

(1)

where plane

 

=an[d, =, [],

represents , ]

the normal represents

vector of the the normal

vector of the plane . We minimum of the dot product between

two planes gives the mutual rotation matrix of the unit.

If the rotation matrix is calculated correctly, the rotated plane  is almost parallel to the . Then, the translation vector is calculated by minimizing the distance between two planes. First,

1. Randomly selecting three points from 3D points  of the chessboard.
2. Calculating the plane equation for the selected three points using a linear method.
3. Finding inliers using the distance between the plane and 3D points.
4. Repeat until finding the best plane with the highest inlier ratio.

Fig. 3. An illustration of set up the camera and the LIDAR on a specially designed plate. The field of view of the camera(blue) and the LIDAR (red) overlap.

90

Fig. 4. Back-projection of 3D points of LIDAR using the extrinsic parameters from our algorithm to the images of the camera using (a) 3.5 and (b) 8mm lenses.

a 3D point  = , ,  is selected randomly on the

plane . The translation vector is calculated by projecting the

selected point on the projected point. The

plane  distance

to 

minimize between

the the

distance from point  and

the the

plane  in i-th frame can be written as:

 =   +   +   +   +  + 

(2)

Twhhee, rpeo,tihnetplacnatenheabqteumawotirvoietntdeonffraosm:

is by

+  +  the translation

+  vector

= 

0. =

 

=

     

=

  

+

 

(3)

The translation vector is estimated that minimizes (2) on all frames. Finally, to remove any ambiguities in relative pose estimation, we require at least three frames. Only one or two planes cannot be determined the 6 degree of freedom pose as the energy function of rotation and translation does not converge when the plane  moves parallel to the plane .

IV. EXPERIMENTAL RESULT
We performed a few experiments to evaluate the accuracy of our proposed extrinsic calibration approach using the device configuration shown in Fig. 2. In our experiments we used a 3.5mm lens. The rotation and translation average between the camera and the LIDAR is tested according to concerning the number of frames used. Frames used in the experiment are randomly selected and repeated 100 times for each test. We found from our experimental results that our algorithm requires at least six frames for stable standard deviation and constant averaging. Fig. 4 shows the result of projecting 3D points of the chessboard using the extrinsic calibration information. Fig. 5 shows the result of colored 3D points of the LIDAR using the image of the omnidirectional camera (all six cameras) and the extrinsic calibration information.

Fig. 5. The result of the colored 3D point of the LIDAR (bottom) using the image of the six omnidirectional camera system (top) and the extrinsic calibration information
V. CONCLUSIONS
In this paper, we proposed an extrinsic calibration method for a multi-sensor system consist of a common LIDAR and six omnidirectional cameras. We described the plane fitting method using data of the camera and the LIDAR respectively, and how to estimate the relative pose of two sensor coordinates system using these planes. This relative pose information can provide fused data from both sensors. The fused data can be used to create a 3D map of the environment for navigation of autonomous vehicles. Moreover, the performance of featurebased or geometric-based localization can be enhanced. As future work, we plan to enhance our proposed extrinsic calibration method into all multiple cameras and the LIDAR instead of considering each camera-LIDAR unit sepaprately.
ACKNOWLEDGMENT
This research was supported by the National Research Foundation of Korea(NRF) funded by the Korea government(MSIT: Ministry of Science and ICT ) ((No. 2018M2A8A5083266)) and this work was supported by Electronics and Telecommunications Research Institute(ETRI) grant funded by the Korean government (19HR2500)
REFERENCES
[1] L. Zhou, Z. Li, M. Kaess, “Automatic Extrinsic Calibration of a Camera and a 3D LiDAR using Line and Plane Correspondences,” in Proc. IEEE/RSJ IROS, pp. 5562-5569, 2018.
[2] Z. Pusztai, L. Hajder, “Accurate Calibration of LiDAR-Camera Systems using Ordinary Boxes,” in Proc. IEEE ICCV, pp. 394-402, 2017.
[3] R. Song, Z. Jiang, Y. Li, Y. Shan, Kai. Huang, “Calibration of Eventbased Camera and 3D LiDAR,” in Proc. WRC Symposium on Advanced Robotics and Automation, pp. 289-295, 2018.
[4] Z. Chai, Y. Sun, Z. Xiong, “A Novel Method for LiDAR Camera Calibration by Plane Fitting,” in Proc. IEEE/ASME AIM, pp. 286-291, 2018.
[5] J. Sui, S. Wang, “Extrinsic Calibration of Camera and 3D Laser Sensor System,” in Proc. CCC, pp. 6881-6886, 2017.
[6] G. Pandey, J. McBride, S. Savarese, R. Eustice, “Extrinsic Calibration of a 3D Laser Scanner and an Omnidirectional Camera,” in Proc. IFAC, pp. 336-341, 2010.
[7] Z. Zhang, “A flexible new technique for camera calibration,” IEEE Trans. Pattern Analysis and Machine Intelligence. pp. 1330–1334, 2000.
[8] M. A. Fischler, R. C. Bolles, “Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography,” in Communications of the ACM, vol. 24, no. 6, pp 381-395, 1981.

91

