CS109/Stat121/AC209/E-109 Data Science
Ensemble Learning and Random Forests
Hanspeter Pﬁster, Joe Blitzstein, and Verena Kaynig

This Week
• HW3 due next Thursday (10/22) at 11:59 pm Eastern. Start early!

https://xkcd.com/627/

Decision Trees
• Easy to understand • Easy to visualize • Can be used both for classification and for
regression • Can handle many types of predictors
(continuous, categorical, etc.) • Poor predictive accuracy

Ensemble Learning
• Combine many learners, each of which may on its own be weak
• Weighted average, weighted majority voting • Wisdom of crowds? • Several methods such as bagging, random
forests, and boosting

Suppose you’re very indecisive, so whenever you want to watch a movie, you ask your friend Willow if she thinks you’ll like it. In order to answer, Willow ﬁrst needs to ﬁgure out what movies you like, so you give her a bunch of movies and tell her whether you liked each one or not (i.e., you give her a labeled training set). Then, when you ask her if she thinks you’ll like movie X or not, she plays a 20 questions-like game with IMDB, asking questions like “Is X a romantic movie?”, “Does Johnny Depp star in X?”, and so on. She asks more informative questions ﬁrst (i.e., she maximizes the information gain of each question), and gives you a yes/no answer at the end. Thus, Willow is a decision tree for your movie preferences.

But Willow is only human, so she doesn’t always generalize your preferences very well (i.e., she overﬁts). In order to get more accurate recommendations, you’d like to ask a bunch of your friends, and watch movie X if most of them say they think you’ll like it. That is, instead of asking only Willow, you want to ask Woody, Apple, and Cartman as well, and they vote on whether you’ll like a movie (i.e., you build an ensemble classiﬁer, aka a forest in this case).

Now you don’t want each of your friends to do the same thing and give you the same answer, so you ﬁrst give each of them slightly diﬀerent data…. You don’t change your love/hate decisions, you just say you love/hate some movies a little more or less (formally, you give each of your friends a bootstrapped version of your original training data). For example, whereas you told Willow that you liked Black Swan and Harry Potter and disliked Avatar, you tell Woody that you liked Black Swan so much you watched it twice, you disliked Avatar, and don’t mention Harry Potter at all….

There’s still one problem with your data, however. While you loved both Titanic and Inception, it wasn’t because you like movies that star Leonardo DiCaprio. Maybe you liked both movies for other reasons. Thus, you don’t want your friends to all base their recommendations on whether Leo is in a movie or not. So when each friend asks IMDB a question, only a random subset of the possible questions is allowed (i.e., when you’re building a decision tree, at each node you use some randomness in selecting the attribute to split on, say by randomly selecting an attribute or by selecting an attribute from a random subset). This means your friends aren’t allowed to ask whether Leonardo DiCaprio is in the movie whenever they want.… And so your friends now form a random forest.
http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/

Random Forests
• Extension of bagging of decision trees: generate bootstrap training samples, and a tree based on each, but also randomize set of predictors allowed each time a split is considered.
• Typically, # of random predictors chosen to be of order the square root of the total number of predictors.
• All trees are fully grown • No pruning • Two parameters
– Number of trees – Number of features

8.2 Bagging, Random Forests, Boosting
m=p m=p/2 m= p

0.5

0.4

Test Classification Error

0.3

0.2

0

100

200

300

400

500

Number of Trees

URE 8.10. Results fJraommes-Wirtteann-Hdasotiem-Tibshfioranrie, AsntIsntrofduocrtionttohSetatis1ti5ca-l Lcelaarnsinsg gene express set with p = 500 predictors. The test error is displayed as a function

This question of trying to ﬁgure out whether a book is good or bad by looking at it carefully or by taking the reports of a lot of people who looked at it carelessly is like this famous old problem: Nobody was permitted to see the Emperor of China, and the question was, What is the length of the Emperor of China’s nose?
To ﬁnd out, you go all over the country asking people what they think the length of the Emperor of China’s nose is, and you average it. And that would be very “accurate” because you averaged so many people. But it’s no way to ﬁnd anything out; when you have a very wide range of people who contribute without looking carefully at it, you don’t improve your knowledge of the situation by averaging.
- Richard Feynman

Boosting
• Also ensemble method like Bagging • But:
– weak learners evolve over time – votes are weighted
• Better than Bagging for many applications
Very popular method

Boosting
“Boosting is one of the most powerful learning ideas introduced in the last twenty years.”
Hastie-Tibshirani-Friedman,The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer (2009)

Boosting Schematic 338 10. Boosting and Additive Trees

Final Classifier

G(x) = sign

M m=1

αm

Gm

(x)

Weighted Sample

GM (x)

Weighted Sample

G3(x)

Weighted Sample

G2(x)

Training Sample

G1(x)

FIGURE 10.1. SchemaHtaisctieo-fTiAbsdhairBanoi-oFsrtie.dCmlaans,sEilﬁemeresntasroef Strtaatiisntiecdal oLenarwnienigghted versions of the dataset, and then combined to produce a ﬁnal prediction.

Tuning Parameters for Boosting
• number of trees • number of splits in each tree (often stumps
work well) • parameters controlling how weights evolve

Raschka, Python Machine Learning

AdaBoost10.1 Boosting Methods 339
Algorithm 10.1 AdaBoost.M1.

1. Initialize the observation weights wi = 1/N, i = 1, 2, . . . , N .

2. For m = 1 to M :

(a) Fit a classiﬁer Gm(x) to the training data using weights wi.

(b) Compute

errm =

N i=1

wiI

(yi ̸= Gm

N i=1

wi

(xi

))

.

(c) Compute αm = log((1 − errm)/errm).

(d) Set wi ← wi · exp[αm · I(yi ̸= Gm(xi))], i = 1, 2, . . . , N .

3. Output G(x) = sign

M m=1

αmGm(x)

.

to concentrate on those training observations that are missed by previous ones in the seqHuaesntiec-Teib.shirani-Friedman, Elements of Statistical Learning
Algorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The

0.5

0.4

Single Stump 244 Node Tree

0.3

Test Error

0.2

0.1

0.0

0

100

200

300

400

Boosting Iterations
Hastie-Tibshirani-Friedman, Elements of Statistical Learning

Wine Example ...

ha='center', va='center', fontsize=12)

>>> plt.show()

As we can see in the resulting plot, the piece-wise linear decision boundary of the three-node deep decision tree looks smoother in the bagging ensemble:

Chapter 7
By looking at the decision regions, we can see that the decision boundary of the AdaBoost model is substantially more complex than the decision boundary of the decision stump. In addition, we note that the AdaBoost model separates the feature space very similarly to the bagging classifier that we trained in the previous section.
[ 223 ]

As concluding remarks aboutReansscehmkbal,ePtyetchhonniqMuaecsh, iinteisLweaorrntihngnoting that ensemble learning increases the computational complexity compared to individual

“Competing in a data science contest without reading the data” http://blog.mrtz.org/2015/03/09/competition.html

“Competing in a data science contest without reading the data” http://blog.mrtz.org/2015/03/09/competition.html

