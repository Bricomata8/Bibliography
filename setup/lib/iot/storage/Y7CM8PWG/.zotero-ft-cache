CS109/Stat121/AC209/E-109 Data Science
Statistical Models
Hanspeter Pﬁster, Joe Blitzstein, and Verena Kaynig

data y (observed)

statistical inference probability

model parameter θ (unobserved)

This Week
• HW1 due next Thursday - start last week! • Section assignments coming soon • Please avoid posting duplicate questions on
Piazza (always search ﬁrst), and avoid posting code from your homework solutions (see Andrew’s post https://piazza.com/class/ icf0cypdc3243c?cid=310 for more)

Drowning in data, but starved for information
source: http://extensionengine.com/drowning-in-data-the-biggest-hurdle-for-mooc-proliferation/

What is a statistical model?
• a family of distributions, indexed by parameters • sharpens distinction between data and parameters,
and between estimators and estimands
• parametric (e.g., based on Normal, Binomial) vs.
nonparametric (e.g., methods like bootstrap, KDE)

data y (observed)

statistical inference probability

model parameter θ (unobserved)

What good is a statistical model?
“All models are wrong, but some models are useful.” – George Box (1919-2013)
“All models are wrong, but some models are useful.” – George Box

Jorge Luis Borges, “On Exactitude in Science”
In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City,
and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisﬁed, and the
Carto“gArlalpmhoedreslsGarue iwldrosntgr,ubcukt saomMeampodoeflstharee uEsmefpuli.r”e–wGheoorsgee Bsoizxe was that of the Empire, and which coincided point for point with it.
Borges Google Doodle

“Big Data vs. Pig Data”:
https://scensci.wordpress.com/2012/12/14/big-data-or-pigdata/

Statistical Models:Two Books

Parametric vs. Nonparametric

• parametric: ﬁnite-dimensional parameter space (e.g.,

mean and variance for a Normal)

• nonparametric: inﬁnite-dimensional parameter space

•

is there anything in between?

• nonparametric is very general, but no free lunch!

•

remember to plot and explore the data!

Parametric Model Example: Exponential Distribution

( )=

x

0

f x e ,x >

pdf 0.0 0.2 0.4 0.6 0.8 1.0 1.2
cdf 0.0 0.2 0.4 0.6 0.8 1.0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 x

0.0 0.5 1.0 1.5 2.0 2.5 3.0 x

Remember the memoryless property!

Exponential Distribution

( )=

x

0

f x e ,x >

• Exponential is characterized by memoryless property

•

all models are wrong, but some are useful...

• iterate between exploring, the data model-building,

model-ﬁtting, and model-checking

•

key building block for more realistic models

Remember the memoryless property!

The Weibull Distribution

•

Exponential has constant hazard function

• Weibull generalizes this to a hazard that is t to a power

• much more ﬂexible and realistic than Exponential

• representation: a Weibull is an Expo to a power

Family Tree of Parametric Distributions

Normal

HGeom

Limit

Conditioning

Bin
(Bern)

Limit

Conditioning

Conjugacy

Pois

Beta
(Unif)

Limit

Poisson process Conjugacy

Limit

Gamma
(Expo, Chi-Square)

Bank - post oﬃce

Limit

NBin
(Geom)

Limit
Student-t
(Cauchy)
Blitzstein-Hwang, Introduction to Probability

Binomial Distribution Figure 3.6 shows plots of the Binomial PMF for various values of n and p. Note that
the PMF of the Bin(10, 1/2) distribution is symmetric about 5, but when the success
probasbitlioty risyn:otX1/~2,BthienP(MnF,pis )skeiwsedt.hFeor anﬁuxemd nbumebrer oofftrsiauls cn,cXestesndesstoibne n
larger when the success probability is high and lower when the success probability is low,
independent Bernoulli(p) trials. as we would expect from the story of the Binomial distribution. Also recall that in any
PMF plot, the sum of the heights of the vertical bars must be 1.

pmf 0.00 0.05 0.10 0.15 0.20 0.25 0.30

Bin(10,1/2)

●

●

●

●

●

●

● ●

0

2

4

6

x

●

● ●

8

10

Bin(100,0.03)

pmf

0.0

0.1

0.2

0.3

0.4

Bin(10,1/8)
●
● ●

●

● ●●●●●●

0

2

4

6

8

10

x

Bin(9,4/5)

0.4

0.3

pmf 0.00 0.05 0.10 0.15 0.20 0.25 0.30

●●

● ●
●

●

●

●

●

●

●

0

2

4

6

8

10

x

pmf

0.0

0.1

0.2

●●

●

●

● ●●●●

0

2

4

6

x

●

8

10

Binomial Distribution
story: X~Bin(n,p) is the number of successes in n independent Bernoulli(p) trials.
Example: # votes for candidate A in election with n voters, where each independently votes for A with probability p
mean is np (by story and linearity of expectation: E(X+Y)=E(X)+E(Y))
variance is np(1-p) (by story and the fact that Var(X+Y)=Var(X)+Var(Y) if X,Y are uncorrelated)

(Doonesbury)

Poisson

Poisson Distribution

last h is

adnsistecoxrretyrt:eecmodueilnaslytrtrgnpiebuoumnptuubimoleanrbreotdrhfiasoettvfreiiwnbndeut’setlipltoheinnnattdfroeoorncdtcmuurcoare,drweeinlehinveetgnhntidtsshi.seccrhreaetpaerteedraatisa.tW hee

its PMExFa,mmpleeasn: #, amndutvaatiroiannscine,gaenndettihcse,n#doisfctursasfﬁictsacstcoidryenitns matoare deta

nition 4.7.1c(ePrtoaiisnsointedrisetrcitbiounti,o#n)o.f Aemnaril.sv.inXanhiansbotxhe Poisson dis parameter , wherme ea>n =0,viafrtiahnecePMfoFr tohfeXPoiissson

P (X

=

k)

=

e

k!

k
,

k = 0, 1, 2, . . . .

write this as X ⇠ Pois( ).

is

a

valid

PMF

because

of

the

Taylor

series

P1
k =0

k
k !

=e

.

mple 4.7.2 (Poisson expectation and variance). Let X ⇠ Pois( ).

w that the mean and variance are both equal to . For the mean, we h

Poisson PMF, CDF

1.0

CDF 0.0 0.2 0.4 0.6 0.8 1.0

PMF

0.6

0.8

Pois(2)

0.0

0.2

0.4

●

●

● ●
●
● ●

0

1

2

3

4

5

6

x

1.0

0.8

0.6

PMF

0.4

Pois(5)

0.2

0.0

●●

●

●

●
● ●

●

●

●

●

0

2

4

6

8

10

x

FIGURE 4.7

CDF 0.0 0.2 0.4 0.6 0.8 1.0

●

●●

●

●

●

●

●

●

●

●

●

●

●

0

1

2

3

4

5

6

x

●

●●

●●

●●

●●

●●

●●

●●

●●

●●

●●

●

0

2

4

6

x

8

10

dence of r.v.s) and of good approximations (there are various ways to m
ood an approximatPioon iiss)s.oAnreAmaprkparbolexthimeoraetmioisnthat, in the above n
Aj are independent, N ⇠ Pois( ), and B is any set of nonnegative i

|P (X 2 B)

P (N

2

B)|



min

✓ 1,

◆ 1

Xn

p2j .

j =1

X

i

provipdi,easifnadnNisu⇠tphepPoneiursm(bb)oe, ruwoniftdhevoenntthsehtaohvwaetraomgcecuuncru,hmwebhreerrreoortfheievsetnihntscetvuheranrtteohdcacsfuprror.ombabuilsiitny g a

ximation, not only for approximating the PMF of X, but also for appr

e d e

rbpeers:oubwltaebcEaiwxlniaatynmbtetphPslaehtnjo:n=wXion1npmmi2jsuaasatittonnccyghhbiseainsengtva.eapprAdyrpvlosarsobonm,lxceaieitmmldlm,a,toeattehrckhleeyanstpi1mqrl/euoeoaeb.rseatkbnpvoirelweirtcyynisoseamfshatohlwlecsSomtmeaip

d.

Poisson paradigm is also called the law of rare events. The interpret is that the pj are small, not that is small. For example, in the email e
w probability of getting an email from a speciﬁc person in a particular by the large number of people who could send you an email in that hou

Poisson Model Example
Poisson model empirical distribution

Pr(Â Yi = y|q = 1.83)
0.00 0.02 0.04 0.06 0.08

0.30

0.20

Pr(Y i = yi)

0.10

0.00

0

2

4

6

8

0 10 20

number of children

number

source: Hoff, A First Course in Bayesian Statistical Methods

Fig. 3.7. Poisson distributions. The ﬁrst panel shows a Poisso
Extensions:mZeeanroo-fIn1ﬂ.8a3t,eadlonPgoiwsisthonth,eNeemgpairtiicvael dBisitnriobmutiaonl, o…f the nu
women of age 40 from the GSS during the 1990s. The secon

distribution of the sum of 10 i.i.d. Poisson random variables w

Normal (Gaussian) Distribution

•

symmetry

•

central limit theorem

•

characterizations (e.g., via entropy)

•

68-95-99.7% rule

Wikipedia

Normal Approximation to Binomial
Wikipedia

The Magic of Statistics
source: Ramsey/Schafer, The Statistical Sleuth

The Evil Cauchy Distribution
http://www.etsy.com/shop/NausicaaDistribution

Bootstrap (Efron, 1979)

data: reps

3.142 2.718 1.414 0.693 1.618 1.414 2.718 0.693 0.693 2.718 1.618 3.142 1.618 1.414 3.142 1.618 0.693 2.718 2.718 1.414 0.693 1.414 3.142 1.618 3.142 2.718 1.618 3.142 2.718 0.693 1.414 0.693 1.618 3.142 3.142
resample with replacement, use empirical distribution to approximate true distribution

Bootstrap World
source: http://pubs.sciepub.com/ijefm/3/3/2/ which is based on diagram in Efron-Tibshirani book

