Expert Systems With Applications 86 (2017) 18–31
Contents lists available at ScienceDirect
Expert Systems With Applications
journal homepage: www.elsevier.com/locate/eswa

A privacy self-assessment framework for online social networks
Ruggero G. Pensa∗, Gianpiero Di Blasi
Dept. of Computer Science, University of Torino, C.So Svizzera, 185 – I-10149 Torino, Italy

article info
Article history: Received 19 December 2016 Revised 11 April 2017 Accepted 20 May 2017 Available online 22 May 2017
Keywords: Privacy measures Online social networks Active learning

abstract
During our digital social life, we share terabytes of information that can potentially reveal private facts and personality traits to unexpected strangers. Despite the research efforts aiming at providing eﬃcient solutions for the anonymization of huge databases (including networked data), in online social networks the most powerful privacy protection “weapons” are the users themselves. However, most users are not aware of the risks derived by the indiscriminate disclosure of their personal data. Moreover, even when social networking platforms allow their participants to control the privacy level of every published item, adopting a correct privacy policy is often an annoying and frustrating task and many users prefer to adopt simple but extreme strategies such as “visible-to-all” (exposing themselves to the highest risk), or “hidden-to-all” (wasting the positive social and economic potential of social networking websites). In this paper we propose a theoretical framework to i) measure the privacy risk of the users and alert them whenever their privacy is compromised and ii) help the users customize semi-automatically their privacy settings by limiting the number of manual operations. By investigating the relationship between the privacy measure and privacy preferences of real Facebook users, we show the effectiveness of our framework.
© 2017 Elsevier Ltd. All rights reserved.

1. Introduction
Social networks are one of the main traﬃc sources in the Internet. At the end of 2014, they attracted more than 31% of the worldwide Internet traﬃc towards the Web. Facebook, the most famous social networking platform, drives alone 25% of the whole traﬃc. As a comparison, Google search engine represents just over 37% of the global traﬃc. More than two billions people are estimated to be registered in at least one of the most popular social media platforms (Facebook hits the goal of one billion users in 2012). Overall, the number of active “social” accounts are more than two billions. In view of these numbers, the risks due to a more and more global and unaware diffusion of our sensitive and less sensitive personal data cannot be overlooked. If, on the one hand, many users are informed about the risks linked to the disclosure of personal facts (private life events, sexual preferences, diseases, political ideas, and so on), on the other hand the awareness of being exposed to privacy breaches each time we disclose facts that are apparently less sensitive is still insuﬃciently widespread. A GPS tag far from home or pictures taken during a journey, may alert potential thieves who may clean out the apartment. The disclosure of family relation-
∗ Corresponding author. E-mail addresses: ruggero.pensa@unito.it (R.G. Pensa), diblasi@di.unito.it (G. Di
Blasi).
http://dx.doi.org/10.1016/j.eswa.2017.05.054 0957-4174/© 2017 Elsevier Ltd. All rights reserved.

ships may expose our own or other family members’ privacy to the risks of stalking, slander and cyberbullying. Moreover, the research project myPersonality (Kosinski, Stillwell, & Graepel, 2013), carried out at the University of Cambridge, has shown that, by leveraging Facebook user’s activity (such as ”Likes” to posts or fan pages) it is possible to “guess” some very private traits of the user’s personality. According to another study, it is even possible to infer some user characteristics from the attributes of users who are part of the same communities (Mislove, Viswanath, Gummadi, & Druschel, 2010). As a consequence, privacy has become a primary concern among social network analysts and Web/data scientists. Also, in recent years, many companies are realizing the necessity to consider privacy at every stage of their business. In practice, they have been turning to the principle of Privacy by Design (Cavoukian, 2012) by integrating privacy requirements into their business model.
Despite the huge research efforts aiming at providing eﬃcient solutions to the anonymization of huge databases (including networked data) (Backstrom, Dwork, & Kleinberg, 2011; Xue, Karras, Raïssi, Kalnis, & Pung, 2012; Zhou & Pei, 2011; Zou, Chen, & Özsu, 2009), in online social networks the most powerful privacy protection is in the hands of the users: they, and only they, decide what to publish and to whom. Even though social networking sites (such as Facebook), notify their users about the risks of disclosing private information, most people are not aware of the dangers due to the indiscriminate disclosure of their personal data. Moreover, despite the fact that all social media provide some advanced tools for con-

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

19

trolling the privacy settings of the user’s proﬁle, such tools are not user-friendly and they are barely utilized, in practice. According to Facebook former CTO Bret Taylor, most people have modiﬁed their privacy settings, but in 2012, still “13 million users [in the United States] said they had never set, or didn’t know about, Facebook’s privacy tools”. Often the choices of many users are limited to two: i) make their own proﬁle completely public, being exposed to all the above mentioned risks, ii) make their own proﬁle completely private, preventing all opportunities offered by the social network sites. Some studies try to foster risk perception and awareness by “measuring” users’ proﬁle privacy according to their privacy settings (Liu & Terzi, 2010; Wang, Nepali, & Nikolai, 2014). These metrics usually require a separation-based policy conﬁguration: in other terms, the users decide “how distant” a published item may spread in the network. Typical separation-based privacy policies for proﬁle item/post visibility include: visible to no one, visible to friends, visible to friends of friends, public. However, this policy fails when the number of user friends becomes large. According to a well-known anthropological theory, in fact, the maximum number of people with whom one can maintain stable social (and cybersocial) relationships (known as Dunbar’s number) is around 150 (Dunbar, 2016; Roberts, Dunbar, Pollet, & Kuppens, 2009), but the average number of user friends in Facebook is more than double1. This means that many social links are weak (oﬄine and online interactions with them are sporadic), and a user who sets the privacy level of an item to “visible to friends” probably is not willing to make that item visible to all her friends. Other studies try to make the customization process of the privacy settings less frustrating (Fang & LeFevre, 2010). However, a consensus on how to identify a trade-off between privacy protection and exploitation of social network potentials is still far from being achieved.
With the ﬁnal goal of enhancing users’ privacy awareness in online social networks, in this paper we propose a theoretical framework to i) measure the privacy risk of the users and alert them whenever their privacy is compromised and ii) help the exposed users customize semi-automatically their privacy level by limiting the number of manual operations thanks to an active learning approach. Moreover, instead of using a separation-based policy for computing the privacy risk, in this paper we adopt a circle-based formulation of the privacy score proposed by Liu and Terzi (2010). We assume that a user may set the visibility of each action and proﬁle item separately for each other user in her friend list. For instance, a user u may decide to allow the access to all photo albums to friends f1 and f2, but not to friend f3. In our score, the sensitivity and visibility of proﬁle item i published by user u are computed according to the set of u’s friends that are allowed to access the information provided by i. We show experimentally that our circle-based deﬁnition of privacy score better capture the real privacy leakage risk. Moreover, by investigating the relationship between the privacy measure and the privacy preferences of real Facebook users, we show that our framework may effectively support a safer and more fruitful experience in social networking sites. Differently from other research works addressing the same problem, our framework takes into account both users’ preferences and the real sensitive information leakage risk in deciding how much visibility should be given to each proﬁle item.
Our contribution can be resumed as follows:
• we deﬁne a formal framework for privacy self-assessment in online social networks based on both sensitivity and visibility of user proﬁle items;
• we use a new privacy score leveraging more accurate circlebased policies;
1 http://www.pewresearch.org/fact- tank/2014/02/03/6- new- facts- about- facebook/

• we present a semi-supervised machine learning approach to support the conﬁguration of the visibility level of user proﬁle items;
• we report the results of several experiments on original data obtained from real Facebook users.
The remainder of the paper is organized as follows: we brieﬂy review the related literature in Section 2; the overview and the theoretical details of our framework are presented in Section 3; Section 4 provides the report of our experimental validation; ﬁnally, we draw some conclusions, discuss some limitations and propose some future research directions in Section 5.
2. Related work
With the unrestrained success of online social networks, there has been increasing research interests about privacy protection methods for individuals that participate in them. Most research efforts are devoted to the identiﬁcation and formalization of privacy breaches and to the anonymization of networked data. The goal is to modify data so that the probability of identifying an individual within the network is minimized. This objective is achieved by either anonymizing only the network structure or anonymizing both network structure and user attributes (Zheleva & Getoor, 2011).
Some of the most relevant contributions tackle the problem of graph anonymization by applying edge modiﬁcation (Liu & Terzi, 2008; Zhou & Pei, 2011; Zou, Chen, & Özsu, 2009), randomization (Vuokko & Terzi, 2010; Ying & Wu, 2011), generalization (Cormode, Srivastava, Bhagat, & Krishnamurthy, 2009; Hay, Miklau, Jensen, Towsley, & Weis, 2008) or differentially private mechanisms (Hay, Li, Miklau, & Jensen, 2009; Task & Clifton, 2012). Among the approaches that anonymize also the user attributes, Zhou and Pei (2011) adopt a greedy edge modiﬁcation and label generalization algorithm, Zheleva and Getoor (2008) anonymize nodes attribute ﬁrst and then tries to preserve the network structure, Campan and Truta (2009) optimize an utility function using the attribute and structural information simultaneously.
All these works focus on how to share social networks owned by companies or organizations masking the identities or the sensitive connections of the individuals involved. However, less attention has been given to the privacy risk of users caused by their information-sharing activities (e.g., posts, likes, shares). In fact, since disclosing information on the web is a voluntary activity, a common opinion is that users should care about their privacy and control it during their interaction with other social network users. Although multiple complex factors are involved in user privacy protection on social media (Litt, 2013), privacy controls for online social networking sites are not fully socially aware (Misra & Such, 2016) and are barely utilized in practice. This statement is conﬁrmed by a study of Liu, Gummadi, Krishnamurthy, and Mislove (2011) which shows that 36% of Facebook content is shared with the default privacy settings and exposed to more users than expected.
Thus, another branch of research has focused on investigating measures, strategies and tools to enhance the users’ privacy awareness and help them act more safely during their day-to-day social network activity. Liu and Terzi (2010) propose a framework to compute a privacy score measuring the users’ potential risk caused by their participation in the network. This score takes into account the sensitivity and the visibility of the disclosed information and leverages the item response theory as theoretical basis for the mathematical formulation of the score. Instead, Motahari, Ziavras, and Jones (2010) propose an information-theoretic estimation of the user anonymity level to help predict the identity inference risks according to both external knowledge and the correlation between user attributes. Cetto et al. (2014) present an online

20

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

game, called Friend Inspector, that allows Facebook users to check their knowledge of the visibility of their shared personal items and provides recommendations on how to improve privacy settings. Instead, Fang and LeFevre (2010) propose a social networking privacy wizard to help users customize their privacy settings. Similarly, Wang et al. (2015) present an interactive visualization tool that helps users conﬁgure the privacy according to their own personality traits derived from their social media data. Squicciarini, Lin, Sundareswaran, and Wede (2015) propose a framework which determines the best available privacy policy for user-uploaded images on content-sharing sites according to the user’s available history on the site. Becker and Chen (2009) present a tool to detect unintended information loss in online social networks by quantifying the privacy risk attributed to friend relationships in Facebook. The authors show that a majority of users’ personal attributes can be inferred from social circles. Talukder, Ouzzani, Elmagarmid, Elmeleegy, and Yakout (2010) present a privacy protection tool that measures the inference probability of sensitive attributes from friendship links. In addition, they suggest self-sanitization actions to regulate the amount of leakage. Squicciarini, Paci, and Sundareswaran (2014) propose an ontology-based privacy protection mechanism supporting semi-automated generation of access rules for users’ proﬁle information. Instead, Such and Rovatsos (2016) and Such and Criado (2016) suggest a computational mechanism that is able to negotiate conﬂicting privacy preferences of multiple users on any individual item and merge them into a single policy. Other approaches to privacy control in social networks investigate the problem of the risk perception. Akcora, Carminati, and Ferrari (2012a); 2012b), for instance, propose to provide users with a measure of how much it might be risky to have interactions with them, in terms of disclosure of private information. They use an active learning approach to estimate user risk from few required user interactions. Finally, the impact of user privacy policies on information diffusion processes has been studied as well (Bioglio & Pensa, 2017 ).
The positioning of our work is in this second branch of research, but differently from the above mentioned papers, our proposal considers all aspects usually involved in social network privacy issues. In fact, we take into account the real and perceived sensitiveness of proﬁle items, the preferences of social network users regarding the disclosure level of their activity and the position of the user within the network. In addition, to support our claims, we performed a social experiment involving real Facebook users.
3. Keeping privacy under control
In this section we introduce our theoretical framework aiming at supporting the users participating in a social network in ﬁnding a balanced tradeoff between privacy protection and visibility of the proﬁle. We assume that the social networking platform provides all required conﬁguration tools to set the privacy of users’ actions and proﬁle items properly. In particular, our desired property is that a user may set the visibility of each action and proﬁle item separately for each other user in her or his friend list. For instance, a user A may decide to allow the access to all photo albums to friends B and C, but not to friend D. Most social networking platforms (such as Facebook or Google+), provide an adequate ﬂexibility in conﬁguring privacy of proﬁle items and user’s actions. They offer some advanced facilities, such as the possibility of grouping friends into special lists or social circles. However, using them correctly is often an annoying and frustrating task and many users prefer to adopt simple but extreme strategies such as “visible-toall” (exposing themselves to the highest risk), or “hidden-to-all” (wasting the positive social and economic potential of social networking websites).

Furthermore, privacy is not just a matter of users’ preferences; it also relies on the context in which an individual is immersed: the attitude of her or his friends towards privacy (some users likes or share friends’ posts more often than the others, thus contributing to the rapid spread of information), the position within the network (very central users are more exposed than marginal users), her or his own attitude on disclosing very private facts, and so on.
The framework we propose in this paper takes into account both aspects: i) thanks to a semi-supervised learning approach that builds a model leveraging few user’s preferences, it allows to extend privacy settings to all users’ friends according to this model; ii) thanks to a score that quantify the privacy leakage of each user considering both individual and contextual parameters, it provides a constant feedback on the privacy protection level of each user. Moreover, our privacy score ﬁts the real user expectations about the visibility of proﬁle items. Before entering the technical details of framework, we brieﬂy introduce some basic mathematical notation required to formalize the problem.
3.1. Preliminaries and notation
Here we introduce the mathematical notation we will adopt in
the rest of our paper. We consider a set of n users U = {u1, . . . , un}
corresponding to the individuals participating in a social network. Each user is characterized by a set of m properties or proﬁle items
P = {p1, . . . , pm}, corresponding, for instance, to personal informa-
tion such as gender, age, political views, religion, workplace, birthplace and so on. Hence, each user ui is described by a vector pi = pi1, . . . , pim .
Users are part of a social network. Without loss of generality, we assume that the link between two users is always reciprocal (if there is a link from uj to uj then there is also a link from uj to ui). Hence, the social network here is represented as an undirected
graph G(V, E), where V is a set of n vertices {v1, . . . , vn} such that
each vertex vi ∈ V is the counterpart of user ui ∈ U and E is a set
of edges E = {(vi, vk )}. Given a pair of users (ui, uk ) ∈ U , (vi, vk) ∈
E iif users ui and uk are connected (e.g., by a friendship link).
For any given vertex vi ∈ V we deﬁne the neighborhood N (vi ) as the set of vertices vk directly connected to vertex vi, i.e., N (vi ) = {vk ∈ V | (vi, vk ) ∈ E}. Conversationally speaking, N (vi) is the set of friends (also known as friend-list) of user ui, hence we use N (vi ) or N (ui ) interchangeably. Given a user ui and its friend-list N (ui ),
we also deﬁne the ego network centred on user ui as the graph
Gi(Vi, Ei), where Vi = N (vi ) ∪ {vi} and Ei = {(vk, vl ) ∈ E | vk, vl ∈ Vi}.
Finally, for any user ui we introduce a privacy policy matrix Mi ∈
{0, 1}|N (ui)|×m deﬁned as follows: for any element mik j of Mi, mik j = 1 iif proﬁle item p j ∈ P is visible to user uk ∈ N (ui ) (0 otherwise,
i.e., iif user uk is not allowed to access proﬁle item pj). It is worth noting that our framework can be easily extended to
the case of directed social networks (such as Twitter): in this case, the privacy policies are deﬁned only on inbound links.
3.2. General framework
Let us now introduce the technical details of our framework that allows the users to actively control their own privacy leakage.
The framework consists of two distinct core parts: i) a score φp(ui)
that measures the privacy leakage of each user ui and ii) a set of
models {μp(ui)} of privacy preferences, one for each user ui. In a
nutshell, the framework is based on a routine (see Algorithm 1 ) that: i) computes the privacy policy matrix Mi according to the
privacy preference model μp(ui) of each user ui; ii) computes the privacy score φp(ui) of all users; iii) notify each user ui whose privacy score φp(ui) exceeds a given threshold τ . Even if there hasn’t

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

21

Algorithm 1: GenericPrivacyCheckRoutine({μp(ui)}, τ ): {μp(ui)} is the set of models of users’ preferences and τ is a privacy
leakage threshold.
forall the ui ∈ U do
use the preference model μp(ui ) to compute the policy
matrix Mi; end forall the ui ∈ U do
compute the privacy score φp(ui ); if φp(ui ) > τ then
notify user ui; end end
been any change in the privacy policies, the routine should be executed periodically, since other types of changes may have occurred in the social network (e.g., creation or removal of vertices/links in G, voluntary changes in the privacy policy by any user, and so on).
In the following, we will provide more details on the key as-
pects of Algorithm 1: how to compute the privacy score φp(ui) and the preference model μp(ui). Before entering the computational details, we describe here the desired intuitive properties of φp(ui) and μp(ui).
• Desired properties of φp(ui): The privacy score should satisfy
the following properties: i) the higher the sensitivity of the disclosed information, the higher the value of the score; ii) the higher the visibility of the disclosed information within the network, the higher the value of the score.
• Desired properties of μp(ui): The model describing users’ pri-
vacy preferences should meet the following intuitive requirements: i) since deciding the access level of any proﬁle item
for any individual friend is a long and frustrating task, μp(ui)
should minimize the user’s intervention; ii) despite this, the model should be as accurate as possible in predicting those privacy preferences not explicitly set by the users; iii) the model should by easily updatable when the user sets more privacy preferences or add new friends.
3.3. Privacy score
In our framework, the privacy score is inspired by the naive privacy score deﬁned by Liu and Terzi (2010). It measures the user’s potential risk caused by his or her participation in the network. A n × m response matrix R is associated to the set of n users U and the set of m proﬁle properties P2. Each element rij of R contains a privacy level that determines the willingness of user ui to disclose information associated with property pj. In the binomial case rij ∈ {0, 1}: ri j = 1 (resp. ri j = 0) means that user ui has made the information associated with proﬁle item pj publicly available (resp. private). Here we adopt the multinomial case, where entries in R take
any non-negative integer values in {0, 1, . . . , }, where ri j = h (with h ∈ {0, 1, . . . , }) means that user ui discloses information related to
item pj to users that are at most h links away in the social network G (e.g., if ri j = 0 user ui wants to keep pj private, if ri j = 1 user ui is willing to make pj available to all friends, if ri j = 2 user ui is willing to make pj available to the friends of her or his friends, and so on). For this reason, we call this policy separation-based. However, in this work, we use a circle-based deﬁnition of privacy score, ﬁrst introduced by Pensa and di Blasi (2016). A different meaning for the
2 In this work, we refer to P as a ﬁxed set of proﬁle properties or user actions. It is out of the scope of this paper to consider posted items individually. We address this point in the conclusions (see Section 5).

entries rij of R is adopted: in our framework rij is directly proportional to the number of friends to whom ui is willing to disclose the information of proﬁle property pj. Hence, we can compute R according to the circle-based privacy policies deﬁned by matrices

Mi’s using this formula:

ri j =

·

1
|N (ui)|

|N (ui )|
mik j
k =1

(1)

where N (ui ) is the set of friends of user ui, mik j denotes the visi-
bility of user ui’s proﬁle item pj for friend uk, and · is the ﬂoor
function. As a consequence, ri j = iif ∀uk ∈ N (ui ), mik j = 1. Our
deﬁnition is conceptually different from the original one, since the

latter does not take into account the possibility of disclosing per-

sonal items to just a part of friends.

In the following, we use RS when we refer to the response ma-

trix computed with the original separation-based policy approach

deﬁned by Liu and Terzi (2010). We use RC when we refer to our

circle-based deﬁnition of response matrix.

Using the response matrix, it is possible to compute the two

main components of the privacy score: the sensitivity βj of a pro-
ﬁle item pj, and the visibility Vij of a proﬁle item pj due to ui. The sensitivity of a proﬁle item pj depends on the item itself (attribute “sexual preferences” is usually considered more sensitive

than “age”). The visibility, instead, captures to what extent infor-

mation about proﬁle item pj of user ui spreads in the network. Liu and Terzi (2010) use a mathematical model based on item response

theory (a well known theory in psychometrics) to compute sensi-

tivity and visibility. However, we adopt the naive but still effective

formulation that, additionally, is more eﬃcient from the computa-

tional point of view.

In this framework, for h = {1, . . . , − 1} sensitivity is computed

as follows:

β jh

=

1 2

n−

n i =1

1(rij ≥h)

+

n−

n

n i =1

1(ri

j

≥h+1)

n

(2)

where 1A is the indicator function that returns 1 when condition A is true (0 otherwise). When h = 0 or h = , the sensitivity values
are respectively

n−
βj0 =

n i =1

1(rij ≥1)

n

(3)

and

n−
βj =

n i =1

1(ri j ≥

)

n

(4)

The meaning of Eqs. (2)–(4) is the following: the more users

adopt at least privacy level h for privacy item pj, the less sen-
sitive pj is w.r.t. level h. Moreover, for intermediate values of h
(h = {1, . . . , − 1}), the sensitivity values takes into account both level h and h + 1. This guarantees that β j0 < β j1 < . . . < β j (Liu &
Terzi, 2010).
The visibility, for h = {0, . . . , } is computed as follows:

Vi jh = Pr(ri j = h) · h

(5)

where Pr(ri j = h) is the probability that rij is equal to h. By assum-
ing independence between proﬁle properties and users, this prob-

ability can be computed as follows:

Pr(rij = h) =

n i =1

1(ri j =h)

·

n

m j=1

1(ri j =h)

m

(6)

Intuitively, visibility Vijh is higher when the sensitivity of proﬁle items is low and when users have the tendency to disclose lots of

their proﬁle items (Liu & Terzi, 2010). An alternative formulation

of Vijh is given by the following formula:

Vi jh

=

Pr(ri j

=

h)

·

f

i j

(h )

(7)

22

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

where

f

i j

(h )

is

the

fraction

of

users

in

the

network

G

that

know

the

value of proﬁle item pj for user ui, given that ri j = h. It depends on

the position of user ui within the network and can be computed by

exploiting any information propagation models (Kempe, Kleinberg,

& Tardos, 2003).

The normalized privacy score φ p(ui, p j ) for any user ui and pro-
ﬁle property pj is computed as follows:

φ p(ui, p j )

=

φp(ui, p j )

max
uk ∈U

φp

(uk

,

p

j

)

(8)

where

φp(ui, p j ) = β jh · Vi jh

(9)

h =0

and

max
uk ∈U

φp(uk,

p

j

)

is

the

maximum

value

of

Eq.

(9)

among

all

users. Normalization is not strictly required, but it uniﬁes the scale

of the privacy scores and make the choice of a suitable threshold

easier.

Finally, the overall privacy score φp(ui) for any user ui is given
by

m

φp(ui) = φ p(ui, p j ).

(10)

j=1

From Eqs. (8)–(10) it is clear that users that have the tendency
to disclose sensitive proﬁle properties to a wide public are more
prone to privacy leakage. Intuitively, φp(ui ) = 0 means that, in each element of the summation, either β jh = 0 (the proﬁle item pj is
not sensitive at all), or Vi jh = 0 (the proﬁle item pj is kept private). On the contrary, the privacy score is maximum when a user dis-
closes to all her or his friends (Vi jh = 1) all sensitive information
(β jh = 1). In this paper, we use φpS when we refer to the score computed
using the original separation-based response matrix RS; we use φCp
when we refer to the privacy score leveraging our circle-based definition of response matrix RC.
Notice that our deﬁnition of privacy score requires the availabil-
ity of visibility preferences for all user friends. It is worth noting
that most social media platforms allow the users to deﬁne friends
groups or circles and set privacy preferences for groups/circles in-
stead of requiring them to set preferences for every individual
friends. However, in the next section, we will see that our frame-
work is designed to minimize the user’s intervention while com-
puting the circle-based privacy policy matrices Mi.

3.4. User preference model

The second key part of our framework is the user preferences
model μp(ui). The classiﬁcation model should be as accurate as
possible in predicting those privacy preferences not explicitly set
by the users. Moreover, the model should be easily updatable
when the user sets more privacy preferences or adds new users.
Our choice is to use a Naive Bayes classiﬁer (Mitchell, 1997), since
it has the desirable properties we enumerated in Section 3.2. In
fact, Naive Bayes classiﬁers are simple and converge quickly even
with few training data. Moreover, they can be easily embedded in
an active learning framework using, for instance, uncertainty sam-
pling (Dagan & Engelson, 1995) thus minimizing the intervention
of the user in the model training phase.
Our privacy preference model for any given user ui ∈ U and any given proﬁle item p j ∈ P is then a classiﬁcation problem in
which we have a set of |N (ui )| instances D = {d1, . . . , d|N (ui)|} cor-
responding to all friends of ui. Each instance dk is characterized by
a set of p attributes {A1, . . . , Ap} with discrete values and m class

variables{C1, . . . , Cm} that take values in the domain {allow, deny}:
Cj = allow (resp. Cj = deny) means that friend uk is allowed (resp. is not allowed) to access the information of proﬁle item pj of user
ui. The values of attributes {A1, . . . , Ap} are partly derived from the
proﬁle vector pk = pk1, . . . , pkm of users uk, partly from the ego network Gi(Vi, Ei) of user ui (see Section 3.1). For instance, they may contain information such as the workplace and home-town
of uk, or the communities in Gi uk belong to. Table 1 is an example of possible small dataset for a generic user consisting of ﬁve
training instances and two test instances with three proﬁle-based
attributes, two network-based attributes and three class variables.
The Naive Bayes classiﬁcation task can be regarded as estimat-
ing the class posterior probabilities given a test example dk, i.e.,
Pr(Cj = allow|dk ) and Pr(Cj = deny|dk ). The class with the high-
est probability is assigned to the example dk. Given a test example dk, the observed attribute values are given by the vector dk =
{ak1, . . . , . . . , akp}, where aks is a possible value of As, s = 1, . . . , p. The prediction is the class c (c ∈ {allow, deny}) such that Pr(Cj = c|A1 = ak1, . . . , Ap = akp) is maximal. By Bayes’ theorem, the above quantity
can be expressed as

Pr(Cj = c|A1 = ak1, . . . , Ap = akp)

=

Pr(A1

=

ak1, . . . , Ap = akp|Cj = c)Pr(Cj Pr(A1 = ak1, . . . , Ap = akp)

=

c)

=

Pr(A1 = ak1, . . . , Ap = akp|Cj = c)Pr(Cj = c) Pr(A1 = ak1, . . . , Ap = akp|Cj = cx )Pr(Cj = cx )

(11)

cx

where, Pr(Cj = c) is the class prior probability of c, which can be
estimated from the training data. If we assume that conditional independence holds, i.e., all attributes are conditionally independent given the class C j = c, then

Pr(A1 = ak1, . . . , Ap = akp|Cj = c)
p
= Pr(As = aks |Cj = c)
s =1

(12)

and, ﬁnally

Pr(Cj = c|A1 = ak1, . . . , Ap = akp)

=

Pr(Cj = c)

p s =1

Pr(As

=

aks |Cj

=

c)

cx Pr(Cj = cx )

p s =1

Pr(As

=

aks |Cj

=

cx )

(13)

Thus, given a test instance dk, its most probable class is given by:

p

c = arg max
cx

Pr(Cj = cx )

Pr(As = aks |Cj = cx )

(14)

s =1

where the prior probabilities Pr(Cj = cx ) and the conditional probabilities Pr(As = aks |Cj = cx ) are estimated from the training
data.
Hence, our preference model is given by μp(ui ) = ( p, c ), where p is the set of all prior probabilities Pr(Cj = cx ) and c is the set of all conditional probabilities Pr(As = aks |Cj = cx ) com-
puted on the set of training instances from D, i.e., on a set of users
from N (ui ) for which ui has given an allow/deny label explicitly.
Now, the key question is: how to predict all Cj’s accurately without requesting too much labeling work to ui?
To solve this problem, we adopt an active learning approach
named uncertainty sampling (Lewis & Gale, 1994) based on the
maximum entropy principle (Dagan & Engelson, 1995). In an active
learning settings the learning algorithm is able to interactively ask
the user for the desired/correct labels of unlabeled data instances.
A way to reduce the amount of labeling queries to the users is
to sample only those data instances whose predicted class is the

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

23

Table 1 Example of input dataset for the classiﬁcation task.

Friend ID Age

Gender Hometown Community

102030 203040 304050 405060 506070 607080 708090

“21-30” Male

Montreal

C10

“31-40” Female New York C5

“15-19” Female Vancouver C7

“41-50” Female Seattle

C5

“51-60” Male

Montreal

C10

“21-30” Female Montreal

C10

“41-50” Male

New York C5

No. of friends
“501-700” “201-300” “101-200” “701-10 0 0” “501-700” “301-500” “301-500”

C work
allow allow allow allow allow ? ?

C photos
allow deny deny deny allow ? ?

C politics
deny deny deny deny deny ? ?

most uncertain. Different measures of uncertainty have been proposed in the literature, e.g., least conﬁdence (Culotta & McCallum, 2005), smallest margin (Scheffer, Decomain, & Wrobel, 2001) and maximum entropy (Dagan & Engelson, 1995), but for binary classiﬁcation tasks they are equivalent. Hence, we decided to adopt the maximum entropy principle. According to this principle, the most uncertain data instance du is given by:

du = arg max − Pr(Cj = cx|dk ) log Pr(Cj = cx|dk )

(15)

dk

cx

Since probabilities Pr(Cj = cx|dk ) are exactly those computed by
the Naive Bayes classiﬁer to take its decision, this principle can be easily adapted to our preference model.
Once all friends’ labels are predicted, each entry of the policy matrix Mi can be updated as follows:

∀uk ∈ N (ui ), mik j =

1, 0,

if Cj = allow for uk if Cj = deny for uk.

(16)

3.5. Privacy check routine

According to the choices that we detailed in the previous sections, we can now provide a more detailed instance of Algorithm 1. The ﬁnal routine for privacy control is described by Algorithm 2 .

Algorithm 2: PrivacyCheckRoutine(P, {μp(ui)}, G, τ ): P is the users’ proﬁle matrix, {μp(ui)} is the set of models of users’ preferences, G is the social graph and τ is a privacy leakage
threshold.
forall the ui ∈ U do
forall the uk ∈ N (ui ) do build dk = {ak1, . . . , . . . , akp} from pk =< pk1, . . . , pkm > and Gi(Vi, Ei );
end
compute the preference model μp(ui ) and the privacy
policy matrix Mi using (14) and (16);
end
forall the ui ∈ U do forall the p j ∈ P do compute ri j using (1); end
compute the privacy score φp(ui ) using (10); if φp(ui ) > τ then
notify user ui; end
end

The ﬁrst step is the construction of the dataset required by the Naive Bayes classiﬁer, followed by the initialization of the privacy policy matrices Mi. This initialization step can be performed in several ways: randomly, following a common criterium, using Naive Bayes on a ﬁrst seed of labeled friends. Then, using matrices Mi, the routine computes the response matrix RC and the initial pri-
vacy scores φp(ui).

The core part of the routine checks whether the privacy score
of any user ui exceeds a given threshold τ . If it is the case, the
routine notiﬁes user ui. Once notiﬁed, user ui has the possibility to enhance her privacy settings by both redeﬁning their friends groups/circles or by trying to update her privacy settings with the active learning procedure described by Algorithm 3. This proce-
Algorithm 3: UpdatePreferences(ui, μp(ui), K): ui is the user, μp(ui) is the model of user ui’s preferences and K is a positive
integer.
ask user ui for the labels of the K most uncertain friends according to (15);
update the preference model μp(ui ) and the privacy policy
matrix Mi using (14) and (16);

dure selects the K most uncertain friends and asks ui for their labels. Afterwards, it launches the Naive Bayes classiﬁer and reassign
the new {allow, deny} labels to all unlabeled friends. Matrix Mi is then updated accordingly.

3.5.1. Relabeling based on privacy score So far, we have only considered users’ relabeling as a result of
the uncertain predictions based on the users’ preference model. However, one may force the framework to be more protective w.r.t. users’ privacy settings by leveraging the privacy score itself. Our assumption is that, if a user has an unsafe behavior w.r.t. his/her own privacy settings, then she/he is more prone to share posts and facts published by his/her friends. For this reason, when predicting the deny/allow labels for the unlabeled friends of a user ui, we add a further control step in which we automatically set to deny all privacy settings related to proﬁle properties pj and friends uk such as

φ p(uk, p j ) > τφ

(17)

where φ p(uk, p j ) is the privacy score of friend uk w.r.t. pj and τ φ is
a user deﬁned threshold. Of course, this control is performed only on predicted labels, i.e., those privacy settings for which the users have not expressed their preferences yet. In the remainder of the paper, we will refer to this particular setting as strict framework.

3.5.2. Theoretical complexity We now analyze the theoretical computational complexity of
our algorithm. Let n be the number of total users in the social network, f the average number of users’ friends, p the number of attributes of the dataset D and m the number of proﬁle items. The initialization step requires O(n · f · p) operations for building the dataset D, and O(n · m) operation for computing the response matrix RC. Obtaining the privacy score for all users requires O(n · m ·
) operations for computing the sensitivity values βjh ( being the
number of privacy levels in RC), the same cost for computing the visibility values Vijh and for computing the ﬁnal value of the score. Under the reasonable assumptions that m < <f and < <p, the overall complexity for computing all the privacy scores is then O(n · f · p).

24

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

In the worst case (when all privacy scores are above the thresh-

old) the core part of the routine needs to train a Naive Bayes clas-

siﬁer for all users and proﬁle items (Algorithm 3). Since training a

Naive Bayes classiﬁer requires O(f · p) operations, the complexity

of this part is O(n · m · f · p).

As a conclusion, the combination of Algorithms 2 and 3 is linear

in all terms. However, in standard applications, we can assume that

f n (the number of users in a social network is much greater

than the average number of users’ friends). Also, it is straightfor-

ward to suppose that

m ∼ p f. Following these reasonable

assumptions, n prevails on all other terms and the overall com-

plexity of a single execution of our routine is O(n). Moreover, most

operations (i.e., training the classiﬁers, computing individual pri-

vacy scores, selecting of the most uncertain friends) can be exe-

cuted concurrently. A single check/update operation on all users is

then highly scalable and the overhead for a system implementing

our framework is reasonably low.

4. Experimental results

In this section we report and discuss the results of an online experiment that we conducted on real Facebook users. The main objectives of our online experiment were:

• to build an original and large enough dataset centered on privacy-related issues in social network data;
• to gather a signiﬁcant number of correct privacy labels for a small set of relevant and differently sensitive items/user actions;
• to make people concern about their privacy in social networks.

As regards this speciﬁc work, the data we gathered should allow us to draw scientiﬁcally justiﬁed conclusions about:

• the relationship between the separation-based privacy policies

and our circle-based policy deﬁnition;

•

relationship

between

the

separation-based

privacy

score

φ

S p

de-

ﬁned by Liu and Terzi (2010) and our circle-based score φCp;

• the relationship between users’ attitude towards privacy self-

protection in Facebook and the value of the privacy score;

• the trend of the privacy score value as a function of the amount

of labeled friends;

• the impact of the threshold on the number of notiﬁed users;

• the reliability of the privacy score;

• the adoption of the additional criterion based on friends’ pri-

vacy scores;

• the scalability of the application w.r.t. the number of users and

CPU’s.

The section is organized as follows: ﬁrst, we describe the data and how we gathered them; then we provide the details of our experimental settings; ﬁnally we report the results and discuss them.

4.1. Dataset

Our online experiments were conducted in two phases. In the ﬁrst phase we promoted the web page of the experiment3 where people could voluntarily grant us access to some data related to their own Facebook proﬁle and friends’ network. We were not able to access any other information rather than what we asked the permission for, i.e.: email (needed to contact the users for the second phase of our experiment), public proﬁle, friend list, gender, age, work, education, hometown, current location and pagelikes. The participants were perfectly aware about the data we asked for and the purpose of our experiment. In this ﬁrst phase, data were

3 http://kdd.di.unito.it/privacyawareness/

gathered through a Facebook application developed in Java JDK 8, using Version 1.0 of Facebook Graph API. From March to April 2015, we collected the data of 185 volunteers, principally from Europe, Asia and Americas. The social network consisting of all participants plus their friends is an undirected graph with 75,193 nodes and 1,377,672 edges. Although the overall social graph has been generated from participants’ ego networks, the largest connected component consists of 73,050 nodes (97.15% of the overall network) and 1,333,276 edges (96.78% of the overall network). This goal was achieved by allowing the Facebook application to publish on the participant’s timeline a special post inviting all her friend to join the experiment. Some statistics (number of nodes and edges, average degree, average clustering coeﬃcient) about the dataset (as computed by Gephi4) are reported in Fig. 1(c), while Fig. 1 present a picture of the network and its degree distribution. All graphs are considered as undirected. During the second phase, all the remaining participants were contacted for the interactive part of our experiment. First, the participants had to indicate to which level (0=no one, 1=close friends, 2=friends except acquaintances, 3=all friends, 4=friends of friends, 5=everyone on Facebook) they were willing to allow the access to ﬁve personal proﬁle topics. The topics were proposed in form of direct questions (see Table 2) with different levels of sensitivity. We used the answers to ﬁll the response matrix RS. Then, to each participant, we proposed a list of 60 randomly chosen friends and 6 randomly chosen friends of friends (when available). The participants had to indicate to which people they were willing to allow the access to the same ﬁve topics. For this phase, we developed a Java JDK 8 mobile-friendly web application leveraging Version 2.0 of Facebook Graph API. We used the answers on friends to ﬁll the response matrix RC. From May 2015 to February 2016, 101 participants out of 185 replied to the ﬁrst part of the survey, 111 to the second part and 74 out of 185 participants answered all questions of two surveys. Hence, we consider the network data provided by all 185 participants and the survey data related to the 74 participants who completed the two parts of the questionnaire. In Fig. 2 we report some statistics describing the 74 participants. All the data have been anonymized to preserve volunteers’ privacy5. The entries in the two resulting 74
× 5 matrices RS and RC take values in {0, . . . , 5}.

4.2. Separation-based vs. circle-based policies

As a preliminary analysis, we measure how the perception of topic sensitivity changes when the two policies (separation-based and circle-based) are presented to the participants. We compare the two response matrix RS and RC in several ways. First, we measure the Pearson’s correlation coeﬃcient between the two matrices. Given two series of n values X = x1 . . . , xn and Y = yi, . . . , yn, the Pearson’s coeﬃcient is computed as:

ρ(X, Y ) =

n i =1

(xi

−

x)(yi

−

y)

(18)

n i =1

(xi

−

x)2

n i =1

(yi

−

y)2

where x =

n i =1

xi

/n

and

y=

n i =1

yi

/n .

It

basically

captures

the

correlation between the two series of values and ranges between

−1 (for inversely correlated sets of values) and +1 (for the max-

imum positive correlation). In our experiment, n = 74 · 5. We ob-
tain a moderate positive correlation (ρ(RS, RC ) = 0.4632), that in-

dicates a substantial difference between the two policies. Then, for

each question Qj, we measure the average difference between each
entry of the two matrices as i (riSj − rCi j )/n. All the average differ-
ences are positive, i.e., the given separation-based policies are less

4 https://gephi.org/ 5 The data collection/storage and processing protocols have been approved by the Law Oﬃce of our institution.

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

25

Fig. 1. Facebook network, its degree distribution and some characteristics.
Table 2 The ﬁve questions of our online survey.
Q1 Which people would you like to tell that you have just changed job? Q2 If your relationship status changed, which friends would you like to tell? Q3 After a nice holiday, which friends would you share your photos with? Q4 With whom would you like to share a comment on current affairs/politics? Q5 With whom would you like to share your mood or something personal that happened to you?

Fig. 2. Personal data statistics of the individuals that participated in our online experiment.

restrictive than circle-based ones. In particular, we measure an av-
erage difference of 0.54 for Q1, 0.43 for Q2, 0.32 for Q3, 0.35 for Q4 and 0.15 for Q5. Moreover, we measure the overall sensitivity
of each topic as β j = h β jh (see Section 3.3) in the two cases. As
can be seen in Fig. 3(a), all sensitivity values increase when the
circle-based policy is adopted. The improved sensitivity perception

is conﬁrmed when we look at the users’ policies more deeply. In particular, for each question Qj, we count:
• the number A of participants that, in the separation-based test, have made Qj at least visible to friends of their friends (riSj ≥ 4), but have denied the access to Qj to some of the friends of their friends in the circle-based test;

26

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

Fig. 3. Comparative results (separation-based approach vs. circle-based approach).

Table 3 Policy differences in visibility.

Measure Q1 Q2 Q3 Q4 Q5

A

2

2

4

9

1

B

0

0

4

9

1

C

20 5

19 21 4

D

0

0

4

9

1

• the number B of users that have granted the access to some of the friends of their friends in the circle-based test while riSj < 4 in the separation-based test;
• the number C of participants that, in the separation-based test, have made Qj visible at least to all friends (riSj ≥ 4), but have denied the access to Qj to some of their friends in the circlebased test rCi j < 5;
• the number D of participants that, in the circle-based test, have made Qj visible to all friends (rCi j = 5), but have denied the access to Qj to some of their friends in the separation-based test r iSj < 3 .
The results in Table 3 indicate that the major differences are on questions Q3 and Q4, that are the less sensitive according to Fig. 3(a). However, then passing from a separation-based policy to a circle-based one, many users have reviewed their choices in a more restrictive way for question Q1 and Q2 as well.
Finally, we also compute the privacy scores φpS (ui, p j ) and φCp (ui, p j ) for each question Qj and each participant ui. The aver-
age score values are given in Fig. 3(b). Interestingly, although the circle-based policy increases the perception of topic sensitivity, the related privacy scores are sensibly smaller than those computed within the separation-based hypothesis, i.e., the participants have a safer behavior w.r.t. the visibility of the topics. For the sake of completeness, we perform a correlation analysis between the val-
ues of φpS (ui ) and φCp(ui ) in Fig. 3(c). The value of the Pearson’s ρ
coeﬃcient (0.4582) shows moderate positive correlation between the two series of scores.
4.2.1. User preferences vs. privacy score To measure the performances of the active learning approach,
we generate 74 × 5 datasets (one for each pair of users and questions) that we use to train and test the Naive Bayes classiﬁer. These datasets contain, for each friend uk of a user ui, the following attributes: gender and age of uk, countryman (true, if uk and ui were born in the same place, fellow_citizen (true, if uk and ui live in the same place), coworker (true, if uk and ui work or have worked in the same place), schoolmate (true, if uk and ui are or have studied in the same school/college/university), and the Jaccard similarity of page likes of ui and uk. All attribute values are derived from the in-

formation extracted by the Facebook proﬁles, when available. Additionally, we also consider the list of communities uk is part of. To this purpose, we execute a community detection algorithm on the so called “ego-minus-ego” networks (the subgraph induced by the
vertex set N (ui ) \ {ui}) of all 74 users. We use DEMON (Coscia, Ros-
setti, Giannotti, & Pedreschi, 2014), a local-ﬁrst approach based on a label propagation algorithm that is able to discover overlapping communities. The algorithm requires two parameters as input: the minimum accepted size for a community (minCommunitySize) and a parameter that determines the minimum overlap two communities should have in order to be merged. In our experiments, we set minCommunitySize = 3 (to discard very small communities) and
= 0.5 (to admit an average overlap degree). Finally, each friend has a class variable that takes values in the set {allow, deny}.
In a ﬁrst experiment, we study the relationship between the accuracy of the predicted user’s privacy settings and the resulting privacy score. By doing so, we are primarily interested in demonstrating empirically the effectiveness of our framework. Secondly, we aim at analyzing to what extent the preferences expressed by the users are in line with a careful and aware behavior w.r.t. their own privacy.
We conduct the experiment as follows. To simulate the active learning framework, for each user and question, i) we start with just ﬁve (randomly chosen) labeled friends with which we train the Naive Bayes classiﬁer described in Section 3.4; ii) we test the classiﬁer on the remaining 55 friends and iii) choose the friend whose prediction is the most uncertain, following the maximum entropy criterion (see Eq. (15) in Section 3.4); iv) we assign to this friend the same label declared by the participant and v) we retrain the classiﬁer on 5 + 1 instances (friends); vi) ﬁnally, we test the new classiﬁer on the remaining 54 instances. We repeat iteratively the last four steps until there are no test instances left.
At the end of each prediction step, we measure the following performance parameters:

• the Accuracy of the predictions, computed as

Accuracy

=

number of correctly predicted number of test friends

labels ;

• the F-Measure of the predictions, computed as

F -Measure

=

2

·

precision · recall precision + recall

where precision and recall are computed by considering the deny class as the positive one; • the privacy score (Eq. (10)) computed by considering both given and predicted {allow, deny} labels for all 74 users and applying Eq. (16) to calculate matrices Mi and Eq. (1) to compute the response matrix RC).

The values of the three parameters are averaged on all 74 users and 30 runs. In each run, the ﬁrst ﬁve labeled friends are chosen

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

27

Fig. 4. Prediction accuracy vs. privacy score: average results.

randomly. The initial value of the privacy score (when no labels are given) is computed by assigning random labels to all 60 friends. All experiments are performed on a server equipped with 8 Intel Xeon E5-2643 dual core CPU’s, 128GB RAM, running Linux (kernel release: 4.0.4).
4.2.2. Average results The results are provided in Fig. 4. The values of the three
parameters are reported for each question separately and for all ﬁve questions together. As a general observation, the accuracy of the prediction increases signiﬁcantly with the number of labeled friends (see Figs. 4(a) and 4(d)). The growth of the F-Measure is less sharp, instead (Fig. 4(b) and (e)). We recall that both measures are computed on the test instances only. The small drop of Accuracy and F-Measure in the last steps can be explained by the fact that misclassiﬁcation errors of few test instances (less than 5 samples) are more likely to happen. Most importantly, the overall privacy score (Fig. 4(f)) starts to decrease when few friends (5 to 15) are labeled, then it starts to grow almost monotonically. This means that, on average, the users don’t have a safe behavior w.r.t. their privacy in deciding whether their friends may access to their information or not. However, our framework may help to provide more effective privacy settings by demanding a very limited labeling effort to the users. Interestingly, predictions are more accurate for the two most sensitive questions (Q2 and Q5). In order to augment the readability of the plots, we do not report the standard deviations (error-bars) of the measures. However, they are reasonably low for all measures when the number of labeled friends is under 45. Then, the number of test friends decreases and the stability of the prediction is slightly affected. As an example, we obtain standard deviation values between 0.07 and 0.19 for the FMeasure and between 6.5 and 20 for the Accuracy. Instead, the variability of the privacy score is more pronounced (since it really depends on each users’ attitude towards privacy).

4.2.3. Threshold assessment According to our privacy check routine (see Algorithm 2 in
Section 3.5) when a user exceeds a given alarm threshold τ , then
she is notiﬁed and may possibly adjust her privacy settings. Hence,
deciding a congruent value for threshold τ is not without con-
sequences for the system. In fact, not only does it implicitly deﬁne the desired safety level of the social network, but it also has an impact on eﬃciency and usability. If the threshold is too low, many users are notiﬁed frequently and system performances may degrade. Furthermore, frequent notiﬁcations may annoy most users and compromise their experience. For this reasons, we also conduct an experiment to verify how many users could be potentially
notiﬁed depending on increasing values of threshold τ (from 0 to
4) and increasing number of labeled instances (5 to 60). From the results shown in Fig. 6, it can be observed that very low threshold
values (τ < 1.0) cause too many alarms and notiﬁcations. However, for intermediate values of the threshold (1 < τ < 2.5), the num-
ber of users exceeding it, in percentage, is below 30%. This experi-
ment also suggests that τ = 2.5 is a reasonable alert threshold for
Algorithm 2 which guarantees a reasonable safety level (we recall that the maximum value for the privacy score is 5) and a tolerable number of notiﬁcations. It is worth noting that, in our experiments, we do not study the impact of the threshold on users’ decisions concerning their privacy settings. This analysis deserves further investigations, but since it requires the deﬁnition of a non trivial use study, we leave it for future work.
4.2.4. Typical users’ results Sice the results presented in Section 4.2.2 are on average, we
also investigate the behaviour of the three performance parameters on three typical users: a wise user (the one with the lowest nonzero privacy score, computed on the correct labels), a careless user (the one with the highest privacy score) and a standard user (the one exhibiting the privacy score closest to the mean). The results reported in Fig. 5(c), show clearly that for a wise user and a careless user, our framework is not useful. However, for a standard user

28

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

Fig. 5. Prediction Accuracy vs. Privacy score: results for three typical users.

(Fig. 5(f)), the active learning algorithm allows its privacy score to decrease and go below 1, conﬁrming the reliability of this threshold for this speciﬁc dataset. Notice that the overall accuracy and F-Measure of the standard user show that the classiﬁer correctly predicts the allow/deny classes (See Fig. 5(e) and (d)). These results also show that with a limited effort (just 20 labeled friends) this user may enhance her privacy protection using settings that follow her preference model. Instead, for the careless user, the F-Measure is 0 since there are no true positives (this user has almost always labeled as allow her friends); consequently, precision and recall are both equal to zero. Notice also that now the values of the privacy score are stable: the standard deviations are between 0.01 and 0.21 for the wise user, between 0.06 and 0.27 for the careless user and between 0.01 and 0.20 for the standard user.
4.3. Reliability of the framework
We also study the reliability of the framework by extending the prediction to all participants’ friends. Since we do not have the correct labels for friends who do not belong to the list proposed to the participants, we can only measure the privacy score computed on the basis of the predicted set of labels. We compare these measures with the privacy score computed by just considering the labeled friends. To do that, we ﬁrst compare the sensitivity values in the two cases (see Fig. 7(a)). All questions are subject to an increase of their sensitivity, but when looking at the average privacy scores (Fig. 7(b)) we note that all scores are higher than those computed when considering only labeled friends. This means that the visibility of the topics is high. Hence, we perform a correlation analysis in order to check whether the behavior of scores is
coherent in the two cases and measure the Pearson’s ρ coeﬃcient
on the two series of privacy score values. Given the privacy scores
φl(ui) for the labeled case and the privacy scores φp(ui) for the

# of labeled instances

60

100

"ps_tau3.txt" using 2:1:($3 * 100)

90

50

80

70

40

60

30

50

40

20

30

20

10

10

0

0 0.5 1 1.5 2 2.5 3 3.5 4

τ threshold

Fig. 6. Percentage of notiﬁed users for increasing values of the alarm threshold and increasing number of labeled friends.

predicted case, the Pearson’s coeﬃcient is computed as:

ρ=

n i =1

φl (ui ) − φl

φp(ui) − φp

n i =1

φl (ui ) − φl

2

n i =1

φp(ui) − φp

2

(19)

where φl =

n i =1

φl

(ui

)/n

and

φp

=

n i =1

φp

(ui

)/n

are

the

average

privacy scores in the two cases (n = 74). We obtained a Pearson’s

coeﬃcient of ρ = 0.8093 (see Fig. 7(c)) denoting high positive cor-

relation. To assess the signiﬁcance of this result, we should ver-

ify whether the null hypothesis that ρ is not signiﬁcantly different

from zero can be rejected. This can be veriﬁed with a two-tailed t-

test by observing that the quantity t = ρ (n − 2)/(1 − ρ2 ) is dis-

tributed approximately as the Student t-distribution. In our test,

t = 11.69, thus the null hypothesis that ρ is not signiﬁcantly differ-

ent from zero is rejected with a p-value p < 0.00001. These results

conﬁrm that: i) the experiments on the limited set of 60 friends

per user are signiﬁcant enough and that, ii) the framework is reli-

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

29

Fig. 7. Privacy scores computed with labeled friends only Vs. privacy scores computed on all friends.

Fig. 8. Results for the strict framework setting.

able even for users with a realistic number of friends. Notice that the overall number of friends of the participants spans between 120 and 1558 (with an average of 435).
4.4. Results on the strict framework
To test the strict framework setting presented in Section 3.5.1, a required property is that a privacy score is associated to all user’s friend. In a realistic scenario, privacy scores are available for all users in the social network. In our experiments, however, since we asked to label only 60 of each participants’ friends, it turns out that the size of the maximal subnetwork of users having the required property is 5. With these numbers it is not possible to compute reliable privacy scores and preference models. Hence, we identify the user ux who has the largest number of friends among the participants to our online survey and asked her to provide privacy settings labels for them (in fact, her initial set of 60 friends not necessarily include some participants to the survey). Then, we execute the same experimental protocol described in Section 4.2.1. The only difference is that, when predicting the privacy settings of
user ux, we take into account the rule given by Eq. (17) for τ φ ∈
[0.5, 1.0] (with 0.1 step), where the threshold of 1.0 corresponds to the standard framework setting. As before, the results are averaged on 30 runs. The results are reported in Fig. 8.
In particular, in Fig. 8(a) we observe that, when we introduce the strict settings rule, the privacy scores are always behind those computed in the standard framework. This is also conﬁrmed by Fig. 8(b) where we plotted the values of the area under the six curves of Fig. 8(a). Furthermore, the ﬁgure shows that the overall

privacy score increase monotonically with the value of τ φ , i.e., as
expected, lower thresholds correspond to safer settings and viceversa. The overall gain, in terms of privacy, is between 5% (for
τφ = 0.9) and 30% (for τφ = 0.5).
4.5. Scalability analysis
In Section 3.5.2 we claimed that the overall complexity of a single execution of our routine is linear in the number of users. Here we provide also the empirical evidence of this statement. We let the number of users vary between 10 and 100 and plot the measured runtime averaged on 30 executions × 56 prediction steps (from 5 to 60 labeled instances). In a realistic scenario, this would correspond to an iteration of the while loop of Algorithm 2, when all users are asked to label new friends and all privacy scores are recomputed. Fig. 9(a) conﬁrms the linearity of the algorithm w.r.t. the number of users. It also shows that an execution on 100 users requires less than 150 milliseconds. On a network of one million users, the same algorithm would require about 20 minutes. However, the computational time can be reduced further, since our algorithm scales well on multiprocessor systems, as shown in Fig. 9(b). To obtain this curve, we have simply implemented the algorithm using the Callable multithreading interface of Java, and executed it on all 111 users who answered the second part of our survey. With just 16 cores, our algorithm would take about two minutes to perform a complete execution on one millions users.

30

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

Fig. 9. Runtime (in milliseconds) for increasing number of users and CPU cores.

5. Conclusions
With the ﬁnal goal of supporting users’ privacy awareness in the Web, we have proposed a framework to keep the privacy risks under control in online social networks. Our framework consists of two main core parts: the computation of a privacy score that can be monitored to alert all users exposed to privacy breaches; ii) an active learning approach to help the exposed users customize their privacy settings by limiting the number of manual operations. We have validated experimentally our framework on an original dataset obtained through a large scale online survey on real Facebook users. The experiments have shown the effectiveness, the reliability and the computational eﬃciency of our approach. We have also shown that state-of-the-art metrics are based on a distorted perception of sensitivity of published items. Based on these results, we believe that our framework can be easily plugged into any domain-speciﬁc or general-purpose social networking platforms without affecting their responsiveness. Furthermore, it may inspire the design of privacy-preserving social networking components for Privacy by Design compliant software (Cavoukian, 2012).
In this paper we have investigated the problem from a simpliﬁed perspective. In fact, we have considered the problem of sharing a well-deﬁned set of attributes (e.g. work status, relationship status, holidays picture album). To be able to infer the sensitivity of the attributes and to be able to model them from the privacy perspective, they need to belong to classes common in a large part of the population so the behavior of the users with respect to them can be modeled. As a further reﬁnement of this work, we will address the inference of such classes (or topics) for posted items by leveraging NLP, sentiment analysis, topic modeling and text categorization techniques. Moreover, since users, supported by social media tools, often provide additional information on their posted items (e.g., tags, geolocation, user IDs from face recognition in images, hashtags), we will investigate a more complex framework to further deﬁne the context of each privacy policy for individual items.
Acknowledgments
This work was supported by Fondazione CRT (grant number 2015-1638). The author wish to thank all the volunteers who participated in the survey.

References
Akcora, C. G., Carminati, B., & Ferrari, E. (2012). Privacy in social networks: How risky is your social graph? In Proceedings of IEEE ICDE 2012 (pp. 9–19). IEEE Computer Society.
Akcora, C. G., Carminati, B., & Ferrari, E. (2012). Risks of friendships on social networks. In Proceedings of IEEE ICDM 2012 (pp. 810–815). IEEE Computer Society.
Backstrom, L., Dwork, C., & Kleinberg, J. M. (2011). Wherefore art thou R3579X?: Anonymized social networks, hidden patterns, and structural steganography. Communications of the ACM, 54(12), 133–141.
Becker, J., & Chen, H. (2009). Measuring privacy risk in online social networks. In Proceedings of web 2.0 security and privacy (W2SP) 2009.
Bioglio, L., & Pensa, R. G. (2017). Modeling the impact of privacy on information diffusion in social networks. In Proceedings of the 8th conference on complex networks CompleNet 2017 (pp. 95–107). Springer.
Campan, A., & Truta, T. M. (2009). Data and structural k-anonymity in social networks. In Proceedings of PinKDD 2008. In LNCS: 5456 (pp. 33–54). Springer.
Cavoukian, A. (2012). Privacy by design [leading edge]. IEEE Technology and Society Magazine., 31(4), 18–19.
Cetto, A., Netter, M., Pernul, G., Richthammer, C., Riesner, M., Roth, C., & Sänger, J. (2014). Friend inspector: A serious game to enhance privacy awareness in social networks. In Proceedings of IDGEI 2014.
Cormode, G., Srivastava, D., Bhagat, S., & Krishnamurthy, B. (2009). Class-based graph anonymization for social network data. PVLDB, 2(1), 766–777.
Coscia, M., Rossetti, G., Giannotti, F., & Pedreschi, D. (2014). Uncovering hierarchical and overlapping communities with a local-ﬁrst approach. TKDD, 9(1), 6:1– 6:27 .
Culotta, A., & McCallum, A. (2005). Reducing labeling effort for structured prediction tasks. In Proceedings of AAAI 2005 (pp. 746–751). AAAI Press / The MIT Press.
Dagan, I., & Engelson, S. P. (1995). Committee-based sampling for training probabilistic classiﬁers. In Proceedings of icml 1995 (pp. 150–157). Morgan Kaufmann.
Dunbar, R. I. M. (2016). Do online social media cut through the constraints that limit the size of oﬄine social networks? Royal Society Open Science, 3(1).
Fang, L., & LeFevre, K. (2010). Privacy wizards for social networking sites. In Proceedings of WWW 2010 (pp. 351–360). ACM.
Hay, M., Li, C., Miklau, G., & Jensen, D. (2009). Accurate estimation of the degree distribution of private networks. In Proceedings of ICDM 2009 (pp. 169–178). IEEE.
Hay, M., Miklau, G., Jensen, D., Towsley, D. F., & Weis, P. (2008). Resisting structural re-identiﬁcation in anonymized social networks. PVLDB, 1(1), 102–114.
Kempe, D., Kleinberg, J. M., & Tardos, É. (2003). Maximizing the spread of inﬂuence through a social network. In Proceedings of ACM SIGKDD 2003 (pp. 137–146). ACM .
Kosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805.
Lewis, D. D., & Gale, W. A. (1994). A sequential algorithm for training text classiﬁers. In Proceedings of ACM-SIGIR 1994 (pp. 3–12). ACM/Springer.
Litt, E. (2013). Understanding social network site users’ privacy tool use. Computers in Human Behavior, 29(4), 1649–1656.
Liu, K., & Terzi, E. (2008). Towards identity anonymization on graphs. In Proceedings of ACM SIGMOD 2008 (pp. 93–106). ACM.
Liu, K., & Terzi, E. (2010). A framework for computing the privacy scores of users in online social networks. TKDD, 5(1), 6:1–6:30.
Liu, Y., Gummadi, P. K., Krishnamurthy, B., & Mislove, A. (2011). Analyzing facebook privacy settings: user expectations vs. reality. In Proceedings of ACM SIGCOMM IMC ’11 (pp. 61–70). ACM.

R.G. Pensa, G. Di Blasi / Expert Systems With Applications 86 (2017) 18–31

31

Mislove, A., Viswanath, B., Gummadi, P. K., & Druschel, P. (2010). You are who you know: inferring user proﬁles in online social networks. In Proceedings of WSDM 2010 (pp. 251–260). ACM.
Misra, G., & Such, J. M. (2016). How socially aware are social media privacy controls? IEEE Computer, 49(3), 96–99.
Mitchell, T. M. (1997). Machine learning. McGraw-Hill. Motahari, S., Ziavras, S. G., & Jones, Q. (2010). Online anonymity protection in com-
puter-mediated communication. IEEE Transaction on Information Forensics and Security, 5(3), 570–580. Pensa, R. G., & di Blasi, G. (2016). A semi-supervised approach to measuring user privacy in online social networks. In Proceedings of DS 2016. In LNCS: 9956 (pp. 392–407). Springer. Roberts, S. G. B., Dunbar, R. I. M., Pollet, T. V., & Kuppens, T. (2009). Exploring variation in active network size: Constraints and ego characteristics. Social Networks, 31(2), 138–146. Scheffer, T., Decomain, C., & Wrobel, S. (2001). Active hidden Markov models for information extraction. In Proceedings of IDA 2001. In LNCS: 2189 (pp. 309–318). Springer . Squicciarini, A. C., Lin, D., Sundareswaran, S., & Wede, J. (2015). Privacy policy inference of user-uploaded images on content sharing sites. IEEE Transactions on Knowledge and Data Engineering, 27(1), 193–206. Squicciarini, A. C., Paci, F., & Sundareswaran, S. (2014). Prima: A comprehensive approach to privacy protection in social network sites. Annales des Télécommunications, 69(1–2), 21–36. Such, J. M., & Criado, N. (2016). Resolving multi-party privacy conﬂicts in social media. IEEE Transactions on Knowledge and Data Engineering, 28(7), 1851–1863. Such, J. M., & Rovatsos, M. (2016). Privacy policy negotiation in social media. TAAS, 11(1), 4:1–4:29.

Talukder, N., Ouzzani, M., Elmagarmid, A. K., Elmeleegy, H., & Yakout, M. (2010). Privometer: Privacy protection in social networks. In Workshops proceedings of ICDE 2010 (pp. 266–269). IEEE.
Task, C., & Clifton, C. (2012). A guide to differential privacy theory in social network analysis. In Proceedings of ASONAM 2012 (pp. 411–417). IEEE.
Vuokko, N., & Terzi, E. (2010). Reconstructing randomized social networks. In Proceedings of SIAM SDM 2010 (pp. 49–59). SIAM.
Wang, Y., Gou, L., Xu, A., Zhou, M. X., Yang, H., & Badenes, H. (2015). Veilme: An interactive visualization tool for privacy conﬁguration of using personality traits. In Proceedings of the ACM conference on human factors in computing systems, CHI 2015 (pp. 817–826). ACM.
Wang, Y., Nepali, R. K., & Nikolai, J. (2014). Social network privacy measurement and simulation. In Proceedings of ICNC 2014 (pp. 802–806). IEEE.
Xue, M., Karras, P., Raïssi, C., Kalnis, P., & Pung, H. K. (2012). Delineating social network data anonymization via random edge perturbation. In Proceedings of cikm 2012 (pp. 475–484).
Ying, X., & Wu, X. (2011). On link privacy in randomizing social networks. Knowledge and Information Systems, 28(3), 645–663.
Zheleva, E., & Getoor, L. (2008). Preserving the privacy of sensitive relationships in graph data. In Proceedings of PinKDD 2007. In LNCS: 4890 (pp. 153–171). Springer .
Zheleva, E., & Getoor, L. (2011). Privacy in social networks: A survey. In C. C. Aggarwal (Ed.), Social network data analytics (pp. 277–306). Springer US.
Zhou, B., & Pei, J. (2011). The k-anonymity and l-diversity approaches for privacy preservation in social networks against neighborhood attacks. Knowledge and Information Systems, 28(1), 47–77.
Zou, L., Chen, L., & Özsu, M. T. (2009). K-Automorphism: A general framework for privacy preserving network publication. PVLDB, 2(1), 946–957.

