See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/304911235
A Semi-supervised Approach to Measuring User Privacy in Online Social Networks
Conference Paper in Lecture Notes in Computer Science · October 2016
DOI: 10.1007/978-3-319-46307-0_25

CITATION
1
2 authors, including:
Ruggero G. Pensa Università degli Studi di Torino 63 PUBLICATIONS 522 CITATIONS
SEE PROFILE

READS
41

Some of the authors of this publication are also working on these related projects: cInQ: consortium on discovering knowledge with Inductive Queries View project Inductive Queries for Mining Patterns and Models View project

All content following this page was uploaded by Ruggero G. Pensa on 12 October 2016.
The user has requested enhancement of the downloaded file.

A Semi-supervised Approach to Measuring User Privacy in Online Social Networks
Ruggero G. Pensa and Gianpiero Di Blasi
Department of Computer Science University of Torino, Italy ruggero.pensa@unito.it
Abstract. During our digital social life, we share terabytes of information that can potentially reveal private facts and personality traits to unexpected strangers. Despite the research eﬀorts aiming at providing eﬃcient solutions for the anonymization of huge databases (including networked data), in online social networks the most powerful privacy protection is in the hands of the users. However, most users are not aware of the risks derived by the indiscriminate disclosure of their personal data. With the aim of fostering their awareness on private data leakage risk, some measures have been proposed that quantify the privacy risk of each user. However, these measures do not capture the objective risk of users since they assume that all user’s direct social connections are close (thus trustworthy) friends. Since this assumption is too strong, in this paper we propose an alternative approach: each user decides which friends are allowed to see each proﬁle item/post and our privacy score is deﬁned accordingly. We show that it can be easily computed with minimal user intervention by leveraging an active learning approach. Finally, we validate our measure on a set of real Facebook users.
Keywords: privacy metrics, active learning, online social networks
1 Introduction
Online social networks are among the main traﬃc sources in the Internet. At the end of 2014, they attracted more than 31% of the worldwide internet trafﬁc towards the Web. Facebook, the most famous social networking platform, drives alone 25% of the whole traﬃc. As a comparison, Google search engine represents just over 37% of the global traﬃc1. More than two billions people are estimated to be registered in at least one of the most popular social media platforms (Facebook hits the goal of one billion users in 2012). Overall, the number of active “social” accounts are more than two billions. The famous “six degrees of separation” theory has been far exceed in Facebook, where an average degree of 3.57 has been recently observed2. Consequently, social network users are constantly exposed to privacy leakage risks. Although most users do not disclose
1 Source: http://www.alexa.com/ 2 https://research.facebook.com/blog/three-and-a-half-degrees-of-
separation/

very sensitive facts (private life events, diseases, political ideas, sexual preferences, and so on), they are simply not aware of the risks due to the disclosure of less sensitive information, such as GPS tags, photos taken during a vacation period, page likes, or comments on news. As an example, the research project myPersonality [14] carried out at the University of Cambridge has shown that, by leveraging Facebook user’s activity (such as ”Likes” to posts or fan pages) it is possible to “guess” some very private traits of the user’s personality. According to another study, it is even possible to infer some user characteristics from the attributes of users who are part of the same communities [18]. As a consequence, privacy has become a primary concern among social network analysts and Web/data scientists. Also, in recent years, many companies are realizing the necessity to consider privacy at every stage of their business. In practice, they have been turning to the principle of Privacy by Design [5] by integrating privacy requirements into their business model.
Despite the huge research eﬀorts aiming at providing eﬃcient solutions to the anonymization of huge databases (including networked data) [3, 25], in online social networks the most powerful privacy protection is in the hands of the users: they, and only they, decide what to publish and to whom. Even though social networking sites (such as Facebook), notify their users about the risks of disclosing private information, most people are not aware of the dangers due to the indiscriminate disclosure of their personal data when they surf the net. Some social media provide advanced tools for controlling the privacy settings of the user’s proﬁle [24]. However, yet a large part of Facebook content is shared with the default privacy settings and exposed to more users than expected [17]. According to Facebook CTO Bret Taylor, even though most people have modiﬁed their privacy settings3, in 2012, still “13 million users [in the United States] said they had never set, or didn’t know about, Facebook’s privacy tools4”.
Some studies try to foster risk perception and awareness by “measuring” users’ proﬁle privacy according to their privacy settings [16, 23]. These metrics usually require a separation-based policy conﬁguration: in other terms, the users decide “how distant” a published item may spread in the network. Typical separation-based privacy policies for proﬁle item/post visibility include: visible to no one, visible to friends, visible to friends of friends, public. However, this policy fails when the number of user friends becomes large. According to a wellknown anthropological theory, in fact, the maximum number of people with whom one can maintain stable social (and cybersocial) relationships (known as Dunbar’s number) is around 150 [10, 20], but the average number of user friends in Facebook is more than double 5. This means that many social links are weak (oﬄine and online interactions with them are sporadic), and a user who sets the
3 http://www.zdnet.com/article/facebook-cto-most-people-have-modifiedtheir-privacy-settings/
4 http://www.consumerreports.org/cro/magazine/2012/06/facebook-yourprivacy/index.htm
5 http://www.pewresearch.org/fact-tank/2014/02/03/6-new-facts-aboutfacebook/

privacy level of an item to “visible to friends” probably is not willing to make that item visible to all her friends.
To address this limitation, in this paper we propose a circle-based formulation of the privacy score proposed by Liu and Terzi [16]. We assume that a user may set the visibility of each action and proﬁle item separately for each other user in her friend list. For instance, a user u may decide to allow the access to all photo albums to friends f1 and f2, but not to friend f3. In our score, the sensitivity and visibility of proﬁle item i published by user u are computed according to the set of u’s friends that are allowed to access the information provided by i. Since the expression of explicit allow/deny policy for each friend and each item may require huge labeling eﬀorts, we also propose an active learning labeling approach to limit the number of manual operations. We show experimentally that i) our circle-based deﬁnition of privacy score better capture the real privacy leakage risk and ii) the active learning approach provides accurate results in terms of both predicted privacy settings and ﬁnal privacy score.
The remainder of the paper is organized as follows: we brieﬂy review the related literature in Section 2; the overview and the theoretical details of our score are presented in Section 3; the active learning approach is presented in Section 4; Section 5 provides the report of our experimental validation; ﬁnally, we draw some conclusions in Section 6.
2 Related work
Most research eﬀorts in social network privacy are devoted to the identiﬁcation and formalization of privacy breaches and to the anonymization of networked data [25]. All these works focus on how to share social networks owned by companies or organizations masking the identities or the sensitive connections of the individuals involved. However, increasing attention is being paid to the privacy risk of users caused by their information-sharing activities (e.g., posts, likes, shares). In fact, since disclosing information on the web is a voluntary activity, a common opinion is that users should care about their privacy during their interaction with other social network users. Thus, another branch of research has focused on investigating strategies and tools to enhance the users’ privacy awareness and help them act more safely during their day-to-day social network activity. In [6] the authors present an online game, called Friend Inspector, that allows Facebook users to check their knowledge of the visibility of their shared personal items and provides recommendations on how to improve privacy settings. Instead, Fang and LeFevre [11] propose a social networking privacy wizard based on active learning. The wizard iteratively asks the user to allow or deny the visibility of proﬁle items to selected friends and assign privileges to the rest of the user’s friends using a classiﬁer. [4] presents a tool to detect unintended information loss in online social networks by quantifying the privacy risk attributed to friend relationships in Facebook. The authors show that a majority of users’ personal attributes can be inferred from social circles. In [22] the authors present a privacy protection tool that measures the inference probability of sensitive at-

tributes from friendship links. In addition, they suggest self-sanitization actions to regulate the amount of leakage. [12], instead, introduces a machine learning technique to monitor users’ privacy settings and recommend reasonable privacy options. Other approaches to privacy control in social networks investigate the problem of the risk perception. In [1, 2], for instance, the authors propose to provide users with a measure of how much it might be risky to have interactions with them, in terms of disclosure of private information. They use an active learning approach to estimate user risk from few required user interactions.
The privacy measure we propose in this paper is closely related to the work of Liu and Terzi [16]. They propose a framework to compute a privacy score measuring the users’ potential risk caused by their participation in the network. This score takes into account the sensitivity and the visibility of the disclosed information and leverages the item response theory as theoretical basis for the mathematical formulation of the score. Another privacy measure has been proposed in [23] where the authors introduce a privacy index to measure the user privacy exposure in a social network. This index, however, strongly relies on predeﬁned sensitivity values for users’ items. Furthermore, in both proposals, the privacy measures are computed by leveraging separation-based privacy policies. Diﬀerently from the above mentioned papers, our proposal considers circle-based policy settings that better suits the real user visibility preferences.
3 A circle-based deﬁnition of privacy score
In this section we introduce our circle-based privacy score aiming at supporting the users participating in a social network in assessing their own privacy leakage risk. Most social networking platforms (such as Facebook or Google+), provide an adequate ﬂexibility in conﬁguring privacy of proﬁle items and user’s actions. Moreover, they oﬀer some advanced facilities, such as the possibility of grouping friends into special lists or social circles. But privacy is not just a matter of users’ preferences; it also relies on the context in which an individual is immersed: the position within the network (very central users are more exposed than marginal users), her or his own attitude on disclosing very private facts, and so on. Hence, we propose a privacy score that takes all these aspects into account and ﬁts the real user expectations about the visibility of proﬁle items.
Before entering the technical details of our approach, we brieﬂy introduce some basic mathematical notation required to formalize the problem.
3.1 Preliminaries and notation
Here we introduce the mathematical notation we will adopt in the rest of our paper. We consider a set of n users U = {u1, . . . , un} corresponding to the individuals participating in a social network. Each user is characterized by a set of m properties or proﬁle items P = {p1, . . . , pm}, corresponding, for instance, to personal information such as gender, age, political views, religion,

workplace, birthplace and so on. Hence, each user ui is described by a vector pi =< pi1, . . . , pim >.
Users are part of a social network. Without loss of generality, we assume
that the link between two users is always reciprocal (if there is a link from uj to uj then there is also a link from uj to ui). Hence, the social network here is represented as an undirected graph G(V, E), where V is a set of n vertices
{v1, . . . , vn} such that each vertex vi ∈ V is the counterpart of user ui ∈ U and E is a set of edges E = {(vi, vk)}. Given a pair of users (ui, uk) ∈ U , (vi, vk) ∈ E iif users ui and uk are connected (e.g., by a friendship link).
For any given vertex vi ∈ V we deﬁne the neighborhood N (vi) as the set of vertices vk directly connected to the vertex vi, i.e., N (vi) = {vk ∈ V | (vi, vk) ∈ E}. Conversationally speaking, N (vi) is the set of friends (also known as friendlist) of user ui, hence we use N (vi) or N (ui) interchangeably. Given a user ui and her friend-list N (ui), we also deﬁne the ego network centered on user ui as the graph Gi(Vi, Ei), where Vi = N (vi) ∪ {vi} and Ei = {(vk, vl) ∈ E | vk, vl ∈ Vi}.
Finally, for any user ui we introduce a privacy policy matrix M i ∈ {0, 1}ni×m (with ni = |N (ui)|) deﬁned as follows: for any element mikj of M i, mikj = 1 iif proﬁle item pj ∈ P is visible to user uk ∈ N (ui) (0 otherwise, i.e., iif user uk is not allowed to access proﬁle item pj).
It is worth noting that our framework can be easily extended to the case of
directed social networks (such as Twitter): in this case, the privacy policies are
deﬁned only on inbound links.

3.2 Privacy score

Our measure is inspired by the privacy score deﬁned by Liu and Terzi [16]. It measures the user’s potential risk caused by his or her participation in the network. A n× m response matrix R is associated to the set of n users U and the set of m proﬁle properties P. In [16], each element rij of R contains a privacy level that determines the willingness of user ui to disclose information associated with property pj. In the binomial case rij ∈ {0, 1}: rij = 1 (resp. rij = 0) means that user ui has made the information associated with proﬁle item pj publicly available (resp. private). In the multinomial case, entries in R take any nonnegative integer values in {0, 1, . . . , ℓ}, where rij = h (with h ∈ {0, 1, . . . , ℓ}) means that user ui discloses information related to item pj to users that are at most h links away in the social network G (e.g., if rij = 0 user ui wants to keep pj private, if rij = 1 user ui is willing to make pj available to all friends, if rij = 2 user ui is willing to make pj available to the friends of her or his friends, and so on). For this reason, we call this policy separation-based. However, in this work, we adopt a diﬀerent meaning for the entries rij of R: in our framework rij is directly proportional to the number of friends to whom ui is willing to disclose the information of proﬁle property pj. Hence, we can compute R according to the circle-based privacy policies deﬁned by matrices M i’s using this formula:

rij

=

  ℓ ·

1 |N (ui)|

|N (ui)|

 

mikj

 

k=1

(1)

where N (ui) is the set of friends of user ui, mikj denotes the visibility of user ui’s proﬁle item pj for friend uk, and ⌊ · ⌋ is the ﬂoor function. As a consequence, rij = ℓ iif ∀uk ∈ N (ui), mikj = 1. Our deﬁnition is conceptually diﬀerent from the original one, since the latter does not take into account the possibility of disclosing personal items to just a part of friends.
In the following, we use RS when we refer to the response matrix computed with the original separation-based policy approach deﬁned in [16]. We use RC when we refer to our circle-based deﬁnition of response matrix.
Using the response matrix it is possible to compute the two main components of the privacy function: the sensitivity βjh of a proﬁle item pj for a given privacy level h, and the visibility Vijh of a proﬁle item pj due to ui for a given level h. The sensitivity of a proﬁle item pj depends on the item itself (attribute “sexual preferences” is usually considered more sensitive than “age”). The visibility, instead, captures to what extent information about proﬁle item pj of user ui spreads in the network. For the computation details of βjh and Vijh we invite the reader to refer to [16], where a mathematical model based on item response theory (a well known theory in psychometrics) is used to compute sensitivity and visibility. Intuitively, sensitivity βj is such that the more users adopt at least privacy level h for privacy item pj, the less sensitive pj is w.r.t. level h. Instead, visibility Vijh is higher when the sensitivity of proﬁle items is low and when users have the tendency to disclose lots of their proﬁle items. Moreover, it depends on the position of user ui within the network and can be computed by exploiting any information propagation models [13].
The privacy score φp(ui, pj) for any user ui and proﬁle property pj is computed as follows:

ℓ

φp(ui, pj ) = βjh · Vijh.

(2)

h=0

and the overall privacy score φp(ui) for any user ui is given by

m

φp(ui) = φp(ui, pj).

(3)

j=1

From Equation 2 and 3 it is clear that users that have the tendency to disclose sensitive proﬁle properties to a wide public are more prone to privacy leakage. Intuitively, φp(ui) = 0 means that, in each element of the summation, either βjh = 0 (the proﬁle item pj is not sensitive at all), or Vijh = 0 (the proﬁle item pj is kept private). On the contrary, the privacy score is maximum when a user discloses to all her or his friends (Vijh = 1) all sensitive information (βjh = 1).
In this paper, we use φSp when we refer to the score computed using the original separation-based response matrix RS; we use φCp when we refer to the privacy score leveraging our circle-based deﬁnition of response matrix RC .

Table 1. Example of Input Dataset for the Classiﬁcation Task

Friend ID Age Gender Hometown Community No. of friends Cwork Cphotos Cpolitics

102030 ”21-30” Male

Rome

C10

”501-700” allow allow deny

203040 ”31-40” Female Madrid

C5

”201-300” allow deny deny

304050 ”15-19” Female Paris

C7

”101-200” allow deny deny

405060 ”41-50” Female Berlin

C5

”701-1000” allow deny deny

506070 ”51-60” Male

Rome

C10

”501-700” allow allow deny

607080 ”21-30” Female Rome

C10

”301-500”

?

?

?

708090 ”41-50” Male Madrid

C5

”301-500”

?

?

?

4 Semi-supervised privacy policy deﬁnition
Our deﬁnition of privacy score requires the availability of visibility preferences for all user friends. However, setting them correctly is often an annoying and frustrating task and many users may prefer to adopt simple but extreme strategies such as “visible-to-all” (exposing themselves to the highest risk), or “hiddento-all” (wasting the positive social and economic potential of social networking websites). In this section we present a semi-supervised approach to minimize the user’s intervention while computing the circle-based privacy policy matrices M i. The classiﬁcation model should be as accurate as possible in predicting those privacy preferences not explicitly set by the users. Moreover, the model should be easily updatable when the user sets more privacy preferences or adds new users. Our choice is to use a Naive Bayes classiﬁer [19], which is simple and converge quickly even with few training data. Moreover, it can be easily embedded in an active learning framework using, for instance, uncertainty sampling [9] thus minimizing the intervention of the user in the model training phase.
For any given user ui ∈ U and any given proﬁle item pj ∈ P we deﬁne a classiﬁcation problem in which we have a set of |N (ui)| instances D = {d1, . . . , d|N (ui)|} corresponding to all friends of ui. Each instance dk is characterized by a set of p attributes {A1, . . . , Ap} with discrete values and m class variables {C1, . . . , Cm} that take values in the domain {allow, deny}: Cj = allow (resp. Cj = deny) means that friend uk is allowed (resp. is not allowed) to access the information of proﬁle item pj of user ui. The values of attributes {A1, . . . , Ap} are partly derived from the proﬁle vector pk =< pk1, . . . , pkm > of users uk, partly from the ego network Gi(Vi, Ei) of user ui (see Section 3.1). For instance, they may contain information such as the workplace and home-town of uk, or the communities in Gi uk belong to. Table 1 is an example of possible small dataset for a generic user consisting of ﬁve training instances and two test instances with three proﬁle-based attributes, two network-based attributes and three class variables.
The Naive Bayes classiﬁcation task can be regarded as estimating the class posterior probabilities given a test example dk, i.e., P r(Cj = allow|dk) and P r(Cj = deny|dk). The class with the highest probability is assigned to the example dk. Given a test example dk, the observed attribute values are given by the vector dk = {ak1, . . . , . . . , akp}, where aks is a possible value of As, s = 1, . . . , p. The prediction is the class c (c ∈ {allow, deny}) such that P r(Cj = c|A1 = ak1, . . . , Ap = akp) is maximal. By Bayes’ theorem, the above quantity can be

expressed as

P r(Cj = c|A1 = ak1 , . . . , Ap = akp) =

=

P r(A1

= ak1, . . . , Ap = akp|Cj = c)P r(Cj P r(A1 = ak1, . . . , Ap = akp)

=

c)

=

=

P r(A1 = ak1 , . . . , Ap = akp|Cj = c)P r(Cj = c) P r(A1 = ak1 , . . . , Ap = akp|Cj = cx)P r(Cj = cx)

(4)

cx

where, P r(Cj = c) is the class prior probability of c, which can be estimated from the training data. If we assume that conditional independence holds, i.e., all attributes are conditionally independent given the class Cj = c, then

p

P r(A1 = ak1, . . . , Ap = akp|Cj = c) = P r(As = aks |Cj = c)

(5)

s=1

and, ﬁnally

P r(Cj = c|A1 = ak1 , . . . , Ap = akp) =

=

P r(Cj = c)

p s=1

P

r(As

=

aks |Cj

=

c)

P r(Cj = cx)

p s=1

P

r(As

=

aks |Cj

=

cx)

(6)

cx

Thus, given a test instance dk, its most probable class is given by:

p

c = arg max P r(Cj = cx) P r(As = aks |Cj = cx)

(7)

cx

s=1

where the prior probabilities P r(Cj = cx) and the conditional probabilities P r(As = aks |Cj = cx) are estimated from the training data.
To predict all Cj ’s accurately without requesting too much labeling work to ui, we adopt an active learning approach named uncertainty sampling [15] based on the maximum entropy principle [9]. In an active learning settings the learning algorithm is able to interactively ask the user for the desired/correct labels of unlabeled data instances. A way to reduce the amount of labeling queries to the users is to sample only those data instances whose predicted class is most uncertain. Diﬀerent measures of uncertainty have been proposed in the literature, e.g., least conﬁdence [8], smallest margin [21] and maximum entropy [9], but for binary classiﬁcation tasks they are equivalent. Hence, we decide to adopt the maximum entropy principle. According to this principle, the most uncertain data instance du is given by:

du = arg max − P r(Cj = cx|dk) log P r(Cj = cx|dk)

(8)

dk

cx

Since probabilities P r(Cj = cx|dk) are exactly those computed by the Naive Bayes classiﬁer to take its decision, this principle can be easily adapted to our classiﬁcation task.

Once all friends’ labels are predicted, each entry of the policy matrix M i can be updated as follows:

∀uk ∈ N (ui), mikj =

1, 0,

if Cj = allow for uk if Cj = deny for uk.

(9)

The entries of M i are then used to compute the response matrix RC as described in Section 3. Note that the original separation-based deﬁnition of privacy score can not take advantage of this active learning strategy.

5 Experimental results
In this section we report and discuss the results of an online experiment that we conducted on real Facebook users. The main objectives of our experiment are: i) to study the relationship between the separation-based privacy policies and our circle-based policy deﬁnition; ii) to analyze the relationship between the separation-based privacy score φSp deﬁned in [16] and our circle-based score φCp ; iii) to assess the performances of our active learning approach in terms of classiﬁcation accuracy and privacy score robustness.
The section is organized as follows: ﬁrst, we describe the data and how we gathered them; then we provide the details of our experimental settings; ﬁnally we report the results and discuss them.

5.1 Dataset
Our online experiments were conducted in two phases. In the ﬁrst phase we promoted the web page of the experiment6 where people could voluntarily grant us access to some data related to their own Facebook proﬁle and friends’ network. We were not able to access any other information rather than what we asked the permission for, i.e.: email (needed to contact the users for the second phase of our experiment), public proﬁle, friend list, gender, age, work, education, hometown, current location and pagelikes. The participants were perfectly aware about the data we asked for and the purpose of our experiment. In this ﬁrst phase, data were gathered through a Facebook application developed in Java JDK 8, using Version 1.0 of Facebook Graph API. From March to April 2015, we collected the data of 185 volunteers, principally from Europe, Asia and Americas. The social network consisting of all participants plus their friends is an undirected graph with 75,193 nodes and 1,377,672 edges.
During the second phase, all the remaining participants were contacted for the interactive part of our experiment. First, the participants had to indicate to which level (0=no one, 1=close friends, 2=friends except acquaintances, 3=all friends, 4=friends of friends, 5=everyone on Facebook) they were willing to allow the access to ﬁve personal proﬁle topics. The topics were proposed in form of
6 http://kdd.di.unito.it/privacyawareness/

Q1 Which people would you like to tell that

you have just changed job?

Q2 If your relationship status changed,

which friends would you like to tell?

Q3 After a nice holiday, which friends would

you share your photos with?

Q4 With whom would you like to share a

comment on current aﬀairs/politics?

Q5 With whom would you like to share your

mood or something personal that hap-

pened to you?

(a)

(b)

Fig. 1. The ﬁve questions (a) and the graphical interface (b) of our online survey

direct questions (see Figure 1(a)) with diﬀerent levels of sensitivity. We used the answers to ﬁll the response matrix RS. Then, to each participant, we proposed a
list of 60 randomly chosen friends and 6 randomly chosen friends of friends (when
available). The participants had to indicate to which people they were willing
to allow the access to the same ﬁve topics. For this phase, we developed a Java
JDK 8 mobile-friendly web application leveraging Version 2.0 of Facebook Graph
API. Figure 1(b) provides a screenshot of our online survey. We used the answers on friends to ﬁll the response matrix RC . From May 2015 to February 2016, 74
out of 185 participants answered all questions of two surveys. Hence, in our
experiments, we consider the network data provided by all 185 participants and
the survey data related to the 74 participants who completed the questionnaire.
All the data have been anonymized to preserve volunteers’ privacy. The entries in the two resulting 74 × 5 matrices RS and RC take values in {0, . . . , 5}.

5.2 Separation-based vs. circle-based policies

As a preliminary analysis, we measure how the perception of topic sensitivity
changes when the two policies (separation-based and circle-based) are presented to the participants. To this purpose we compare the two response matrix RS and RC in several ways. First, we measure the Pearson’s correlation coeﬃcient
between the two matrices. Given two series of n values X = x1 . . . , xn and Y = yi, . . . , yn, the Pearson’s coeﬃcient is computed as:

ρ(X, Y ) =

n i=1

(xi

−

x)

(yi

−

y)

n i=1

(xi

−

x)2

n i=1

(yi

−

y)2

(10)

where x =

n i=1

xi/n

and

y

=

n i=1

yi/n.

It

basically

captures

the

correlation

between the two series of values and ranges between −1 (for inversely correlated

sets of values) and +1 (for the maximum positive correlation). In our experiment, n = 74 · 5. We obtain a moderate positive correlation (ρ(RS , RC ) = 0.4632),

that indicates a substantial diﬀerence between the two policies. Then, for each

question Qj, we measure the average diﬀerence between each entry of the two matrices as i (ridj − ribj )/n. All the average diﬀerences are positive, i.e., the

Table 2. Policy diﬀerences in visibility

Measure Q1 Q2 Q3 Q4 Q5

A

22491

B

00491

C

20 5 19 21 4

D

00491

given separation-based policies are less restrictive than circle-based ones. In par-

ticular, we measure an average diﬀerence of 0.54 for Q1, 0.43 for Q2, 0.32 for

Q3, 0.35 for Q4 and 0.15 for Q5. Moreover, we measure the overall sensitivity

of each topic as βj =

ℓ h

βjh

(see

Section

3.2)

in

the

two

cases.

As

can

be

seen in Figure 2(a), all sensitivity values increase when the circle-based policy is

adopted. The improved sensitivity perception is conﬁrmed when we look at the

users’ policies more deeply. In particular, for each question Qj, we count:

– the number A of participants that, in the separation-based test, have made Qj at least visible to friends of their friends (riSj ≥ 4), but have denied the access to Qj to some of the friends of their friends in the circle-based test;
– the number B of users that have granted the access to some of the friends of their friends in the circle-based test while riSj < 4 in the separation-based test;
– the number C of participants that, in the separation-based test, have made Qj visible at least to all friends (riSj ≥ 4), but have denied the access to Qj to some of their friends in the circle-based test riCj < 5;
– the number D of participants that, in the circle-based test, have made Qj visible to all friends (riCj = 5), but have denied the access to Qj to some of their friends in the separation-based test riSj < 3.

The results in Table 2 indicate that the major diﬀerences are on questions Q3 and Q4, that are the less sensitive according to Figure 2(a). However, then passing from a separation-based policy to a circle-based one, many users have reviewed
their choices in a more restrictive way for question Q1 and Q2 as well. Finally, we also compute the privacy scores φSp (ui, pj) and φCp (ui, pj) for each
question Qj and each participant ui. The average score values are given in Figure 2(b). Interestingly, although the circle-based policy increases the perception
of topic sensitivity, the related privacy scores are sensibly smaller than those
computed within the separation-based hypothesis, i.e., the participants have a
safer behavior w.r.t. the visibility of the topics. For the sake of completeness, we perform a correlation analysis between the values of φSp (ui) and φCp (ui) in Figure 2(c). The value of the Pearson’s ρ coeﬃcient (0.4582) shows moderate
positive correlation between the two series of scores.

5.3 Assessment of the active learning approach
To measure the performances of the active learning approach, we generate 74 × 5 datasets (one for each pair of users and questions) that we use to train and

Sensitivity Privacy scores φC (circle-based)

6 separation-based

5

circle-based

4

3

2

1

0 Q1 Q2 Q3 Q4 Q5
Topic

(a) Sensitivity

0.3

φS

0.25

φC

0.2

0.15

0.1

0.05

0 Q1 Q2 Q3 Q4 Q5
Topic

(b) Privacy score

1.2 1 ρ=0.4582 (p-value<0.000001)
0.8 0.6 0.4 0.2
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 φS (separation-based)
(c) Correlation analysis

Fig. 2. Comparative results (separation-based approach vs. circle-based approach)

test the Naive Bayes classiﬁer. These datasets contain, for each friend uk of a user ui, the following attributes: gender and age of uk, countryman (true, if uk and ui were born in the same place, fellow citizen (true, if uk and ui live in the same place), coworker (true, if uk and ui work or have worked in the same place), schoolmate (true, if uk and ui are or have studied in the same school/college/university), and the Jaccard similarity of page likes of ui and uk. All attribute values are derived from the information extracted by the Facebook proﬁles, when available. Additionally, we also consider the list of communities uk is part of. To this purpose, we execute a community detection algorithm on the so called “ego-minus-ego” networks (the subgraph induced by the vertex set N (ui) \ {ui}) of all 74 users. We use DEMON [7], a local-ﬁrst approach based on a label propagation algorithm that is able to discover overlapping communities. The algorithm requires two parameters as input: the minimum accepted size for a community (minCommunitySize) and a parameter ǫ that determines the minimum overlap two communities should have in order to be merged. In our experiments, we set minCommunitySize = 3 (to discard very small communities) and ǫ = 0.5 (to admit an average overlap degree). Finally, each friend has a class variable that takes values in the set {allow, deny}.
We conduct the experiment as follows. To simulate the active learning framework, for each user and question, i) we start with just ﬁve (randomly chosen) labeled friends with which we train the Naive Bayes classiﬁer described in Section 4; ii) we test the classiﬁer on the remaining 55 friends and iii) choose the friend whose prediction is the most uncertain, following the maximum entropy criterion (see Equation 8 in Section 4); iv) we assign to this friend the same label declared by the participant and v) we re-train the classiﬁer on 5 + 1 instances (friends); vi) ﬁnally, we test the new classiﬁer on the remaining 54 instances. We repeat iteratively the last four steps until there are no test instances left.
At the end of each prediction step, we measure the following performance parameters: i) the Accuracy of the predictions; ii) the F-Measure of the predictions, computed as F -M easure = 2 · (precision · recall)/(precision + recall) where precision and recall are computed by considering the deny class as the positive one; iii) the privacy score (Equation 3) computed by considering both given and predicted {allow, deny} labels for all 74 users and applying Equation 9 to calculate matrices M i and Equation 1 to compute the response matrix RC ).

Accuracy % F-Measure Privacy score

95 90 85 80 75 70 65 60
0

Q1 Q2 Q3 Q4 Q5
10 20 30 40 50 60
# of labeled instances

(a) Average Accuracy

1

0.8

0.6 0.4 0.2
0

Q1 Q2 Q3 Q4 Q5
10 20 30 40 50 60
# of labeled instances

(b) Average F-Measure

0.35

Q1 Q2

Q3

0.3

Q4

Q5

0.25

0.2

0.15 0 10 20 30 40 50 60 # of labeled instances
(c) Average φCp

Fig. 3. Prediction Accuracy vs. Privacy function: average results

The values of the three parameters are averaged on all 74 users and 30 runs. In each run, the ﬁrst ﬁve labeled friends are chosen randomly. The initial value of the privacy function (when no labels are given) is computed by assigning random labels to all 60 friends.
The results are provided in Figure 3. The values of the three parameters are reported for each question separately. As a general observation, the accuracy of the prediction increases signiﬁcantly with the number of labeled friends (see Figure 3(a)). The growth of the F-Measure is less sharp, instead (Figure 3(b)). We recall that both measures are computed on the test instances only. The small drop of Accuracy and F-Measure in the last steps can be explained by the fact that misclassiﬁcation errors of few test instances (less than 5 samples) are more likely to happen. Interestingly, predictions are more accurate for the two most sensitive questions (Q2 and Q5). As for the privacy scores (Figure 3(c)), they start to decrease when few friends (5 to 15) are labeled, then they start to grow almost monotonically and the diﬀerences among them are more emphasized. This behavior can be partially explained by noting that, as the amount of labeled friends increases, the sensitivity perceived by the users gets closer to the realistic sensitivity of the ﬁve topics.
5.4 Reliability of the predictions
We also study the robustness of the approach by extending the prediction to all participants’ friends. Since we do not have the correct labels for friends who do not belong to the list proposed to the participants, we can only measure the privacy scores computed on the basis of the predicted set of labels. We compare these measures with those computed by just considering the labeled friends.
To do that, we ﬁrst compare the sensitivity values in the two cases (see Figure 4(a)). All questions are subject to an increase of their sensitivity, but when looking at the average privacy scores (Figure 4(b)) we note that all scores are higher than those computed when considering only labeled friends. This means that the visibility of the topics is high. Hence, we perform a correlation analysis in order to check whether the behavior of scores is coherent in the two cases and measure the Pearson’s ρ coeﬃcient on the two series of privacy score values. We obtain a Pearson’s coeﬃcient of ρ = 0.8093 with a p-value

Sensitivity Privacy scores Privacy score (all friends)

7 labeled friends

6

all friends

5

4

3

2

1

0 Q1 Q2 Q3 Q4 Q5

Topic

(a) Sensitivity

0.3 labeled friends

0.25

all friends

0.2

0.15

0.1

0.05

0 Q1 Q2 Q3 Q4 Q5
Topic

(b) Privacy scores

5 4 ρ=0.8093 (p-value<0.000001) 3 2 1
0 0.2 0.4 0.6 0.8 1 1.2 Privacy score (labeled friends)
(c) Correlation analysis

Fig. 4. Privacy scores computed with labeled friends only Vs. privacy scores computed on all friends

p < 0.000001 (see Figure 4(c)) denoting high positive correlation. This result conﬁrm that: i) the experiments on the limited set of 60 friends per user are signiﬁcant enough and that, ii) the approach is reliable even for users with a realistic number of friends and few given labels. Note that the overall number of friends of the participants spans between 120 and 1558 (with an average of 435).

6 Conclusions
With the ﬁnal goal of fostering users’ privacy awareness in the Web, we have proposed a privacy score based on an active learning approach to provide the users of online social networks with a measure of their privacy leakage. We have validated experimentally our metrics on an original dataset obtained through an online survey on real Facebook users. The experiments have shown the effectiveness and the reliability of our approach. In particular, we have shown that state-of-the-art metrics are based on a distorted perception of sensitivity of published items. Based on these results, we believe that our framework can be easily plugged into any domain-speciﬁc or general-purpose social networking platforms. Furthermore, it may inspire the design of privacy-preserving social networking components for Privacy by Design compliant software [5].
Acknowledgments The work presented in this paper has been co-funded by Fondazione CRT (grant number 2015-1638). The authors wish to thank all the volunteers who participated in the survey.

References
1. Akcora, C.G., Carminati, B., Ferrari, E.: Privacy in social networks: How risky is your social graph? In: Proc. of ICDE 2012. pp. 9–19 (2012)
2. Akcora, C.G., Carminati, B., Ferrari, E.: Risks of friendships on social networks. In: Proc. of ICDM 2012. pp. 810–815 (2012)
3. Backstrom, L., Dwork, C., Kleinberg, J.M.: Wherefore art thou R3579X?: anonymized social networks, hidden patterns, and structural steganography. Commun. ACM 54(12), 133–141 (2011)

4. Becker, J., Chen, H.: Measuring privacy risk in online social networks. In: Proc. of Web 2.0 Security and Privacy (W2SP) 2009 (2009)
5. Cavoukian, A.: Privacy by design [leading edge]. IEEE Technol. Soc. Mag. 31(4), 18–19 (2012)
6. Cetto, A., Netter, M., Pernul, G., Richthammer, C., Riesner, M., Roth, C., S¨anger, J.: Friend inspector: A serious game to enhance privacy awareness in social networks. In: Proc. of IDGEI 2014 (2014)
7. Coscia, M., Rossetti, G., Giannotti, F., Pedreschi, D.: Uncovering hierarchical and overlapping communities with a local-ﬁrst approach. TKDD 9(1), 6:1–6:27 (2014)
8. Culotta, A., McCallum, A.: Reducing labeling eﬀort for structured prediction tasks. In: Proc. of AAAI 2005. pp. 746–751 (2005)
9. Dagan, I., Engelson, S.P.: Committee-based sampling for training probabilistic classiﬁers. In: Proc. of ICML 1995. pp. 150–157 (1995)
10. Dunbar, R.I.M.: Do online social media cut through the constraints that limit the size of oﬄine social networks? Royal Society Open Science 3(1) (2016)
11. Fang, L., LeFevre, K.: Privacy wizards for social networking sites. In: Proc. of WWW 2010 (2010)
12. Ghazinour, K., Matwin, S., Sokolova, M.: Monitoring and recommending privacy settings in social networks. In: Proc. of 2013 EDBT/ICDT Workshop. pp. 164–168 (2013)
13. Kempe, D., Kleinberg, J.M., Tardos, E´.: Maximizing the spread of inﬂuence through a social network. In: Proc. of SIGKDD 2003. pp. 137–146 (2003)
14. Kosinski, M., Stillwell, D., Graepel, T.: Private traits and attributes are predictable from digital records of human behavior. PNAS 110(15), 5802–5805 (2013)
15. Lewis, D.D., Gale, W.A.: A sequential algorithm for training text classiﬁers. In: Proc. of SIGIR 1994. pp. 3–12 (1994)
16. Liu, K., Terzi, E.: A framework for computing the privacy scores of users in online social networks. TKDD 5(1), 6 (2010)
17. Liu, Y., Gummadi, P.K., Krishnamurthy, B., Mislove, A.: Analyzing facebook privacy settings: user expectations vs. reality. In: Proc. of SIGCOMM IMC ’11. pp. 61–70 (2011)
18. Mislove, A., Viswanath, B., Gummadi, P.K., Druschel, P.: You are who you know: inferring user proﬁles in online social networks. In: Proc. of WSDM 2010. pp. 251– 260 (2010)
19. Mitchell, T.M.: Machine learning. McGraw-Hill (1997) 20. Roberts, S.G.B., Dunbar, R.I.M., Pollet, T.V., Kuppens, T.: Exploring variation
in active network size: Constraints and ego characteristics. Social Networks 31(2), 138–146 (2009) 21. Scheﬀer, T., Decomain, C., Wrobel, S.: Active hidden markov models for information extraction. In: Proc. of IDA 2001. pp. 309–318 (2001) 22. Talukder, N., Ouzzani, M., Elmagarmid, A.K., Elmeleegy, H., Yakout, M.: Privometer: Privacy protection in social networks. In: Proc. of M3SN’10. pp. 266– 269 (2010) 23. Wang, Y., Nepali, R.K., Nikolai, J.: Social network privacy measurement and simulation. In: Proce. of ICNC 2014. pp. 802–806 (2014) 24. Wu, L., Majedi, M., Ghazinour, K., Barker, K.: Analysis of social networking privacy policies. In: Proc. of 2010 EDBT/ICDT Workshops (2010) 25. Zheleva, E., Getoor, L.: Privacy in social networks: A survey. In: Social Network Data Analytics, pp. 277–306 (2011)
View publication stats

