Asian Journal of Convergence in Technology ISSN NO: 2350-1146 I.F-5.11

Volume V Issue I

Low Packet Loss and High PDR based Self Adaptive Sleep Wake Scheduling Technique
for WSN

First Aarushee, M.tech Scholar, CSE, BIT, Meerut, U.P., India, Second Mr. Kshitiz Saxena, Associate Professor, CSE, BIT, Meerut, U.P., India

Abstract— Wireless Sensor Networks consisting of nodes with limited power are deployed to gather useful information from the field. In WSNs it is critical to collect the information in an efficient manner. It is applied in routing and difficult power supply area that cannot be reached and some temporary situations, which do not need fixed network supporting and it can fast deploy with strong anti-damage. In order to avoid the problem, we proposed a new technique called Bio-Inspired mechanism for routing. Proposed algorithm shows better performance in terms of Packet Loss and Delay.

Keywords— optimization

Scheduling;

routing;

delay;

I. INTRODUCTION

Sleep/wake-up planning is one of the basic issues in wireless sensor networks, since the vitality of sensor hubs or nodes is constrained and they are generally un-battery-powered. The motivation behind sleep/wake-up planning is to spare the vitality of every hub by keeping hubs or nodes in sleep mode as far as might be feasible (without sacriﬁcing packet delivery effectiveness) and in this manner expanding their lifetime. [3] Protocols commonly used are shown below: 1) TR-MAC: In TR-MAC, two radios are utilized, where one is for awakening neighbors and the other is for sending packets. In contrast to conventional on-request approaches, in TR-MAC, when a hub has a packet to transmit, it doesn't wake up its whole neighborhood however specifically wakes up a few neighbors which have already occupied with correspondence through rate estimation.

2) DW-MAC: DW-MAC is a synchronized duty cycle MAC convention, where each cycle is isolated into three periods: a) match up; b) information; and c) sleep. DW-MAC needs to synchronize the checks in sensor hubs or nodes intermittently amid the match up period.[4] DWMAC at that point sets up a coordinated relative mapping between an information period and the accompanying sleep time frame. In an information period, the sender will send a booking edge to the recipient. In light of the time interim after the start of the information time frame and the span of the booking outline transmission, both sender and

collector will set up their wake-up time interim amid the accompanying Sleep time frame to transmit/get the packet. 3) EM-MAC: In EM-MAC, every hub utilizes a pseudorandom number generator: Xn+1= (aXn+c) mod m to figure its wake-up times, where m > 0 is the modulus, an is the multiplier, c is the augmentation, X is the current seed and the created X turns into the following seed. In this reenactment, m = 65536, every hub's a, c and X n n+1 are freely picked following the standards recommended by [5]. By asking for the parameters, m, a, c, and X, from a collector, a sender can anticipate the beneficiary’s future wake-up times and get ready to send information at those occasions. EM-MAC does not require synchronization but rather it expects hubs or nodes to trade data before hubs or nodes can make forecasts. 4) AS-MAC: In AS-MAC, hubs or nodes wake up occasionally (yet no concurrently) to get packets. Hubs or nodes proposing to transmit packets wake up at the planned wake-up time of the expected target hubs or nodes. Neighboring hubs or nodes need to impart occasionally to trade data about wake-up timetables to dodge long introductions toward the start of transmission. Likewise, we additionally think about SA-Mech. with its synchronized rendition, SA Mech.- Syn. In SAMech.- Syn, it is accepted that a sink hub intermittently communicates an exceptional packet to the whole network to synchronize the hubs or nodes' tickers. The point of presenting SA Mech.Syn for examination is to test how the presentation of synchronization will influence the execution of SA-Mech.
II. LITRATURE REVIEW:
The sleep/wake-up scheduling schemes are divided into following categories or schemes: Synchronous Schemes, in synchronous plans, for example, S-MAC [2], T-MAC [3], sleeping hubs wake up in the meantime occasionally to speak with each other, which implies the networks need to keep a worldwide synchronization. This sort of techniques is instinctive and direct yet requires synchronization system which requests all the more additional control traffic. [6]

www.asianssr.org

1

Asian Journal of Convergence in Technology ISSN NO: 2350-1146 I.F-5.11
Semi-Synchronous Schemes, Semi-synchronous plans, otherwise called bunch synchronization, for example, [8– 10], is a nearby synchronous technique. In this strategy, sensor hubs are grouped into synchronized bunches. In a similar group, sensor hubs wake up or rest in the meantime. Be that as it may, bunches act together with others non-concurrently. Contrasted and synchronous plans, group-synchronization is simpler to accomplish. Asynchronous Schemes, in non-concurrent wakeup components, for example, [11– 13], every hub has its very own wake-up and sleep plan, which requests the wake-up space should cover among neighbors, or the message can't be transmitted. To fulfill this necessity, the sensor hubs may need to wake up more much of the time than in synchronous wake-up methodologies.
III. PROPOSED ALGORITM FOR ACO:
In view of the proposed model, we present a fortification learning calculation, which is utilized by a player to take in its ideal activities through experimentation associations inside a dynamic situation. The calculation is called Q-learning (Algorithm 1). Q-learning is one of the least complex support learning calculations. In Qlearning technique, ACO technique is implemented to improve the performance of the system. This methodology underlines on distribuends, adaptability, power and immediate or aberrant correspondence among generally straightforward operators. The specialists are self-governing substances, both proactive and responsive and have capacity to adjust co-work and move brilliantly

Volume V Issue I

from one area to the next in the correspondence

network. The results are simulated in NS2 software.

The proposed ant based steering calculation has a

few properties which makes it perfect for the above

determined prerequisites.

•

The calculation has the ability to

progressively reconfigure itself with changing

network topology. This is finished by making

utilization of certain number of information packets

as ants which require the goal hub to send an

affirmation back to the source hub.

•

The Ant based directing calculations does

not trade any steering table data over the network

and the steering depends totally on the

neighborhood data put away in the hub.

•

The Ant based calculation can support

multi way directing as every hub has certain

number of neighbors with determined pheromone

focus levels and the following jump is picked

dependent on the centralization of pheromone.

Consequently, it enables the hub to pick distinctive

courses each time.

IV. SIMULATION RESULTS:
A grid network with 25 nodes is made with same energy levels initially, in earlier scheduling network lifetime is low. By the use of AC Optimization technique packet loss and delay are improved. The proposed is compared with TR-MAC, EM-MAC and other conventional protocols. In figure 1, Packet Loss is represented graphically, which is least when used with ACO on self-adaptive system.

Figure 1: Packet Drop Calculation of the network (proposed)

www.asianssr.org

2

Asian Journal of Convergence in Technology ISSN NO: 2350-1146 I.F-5.11

Volume V Issue I

Figure 2: Packet Delivery Ratio Calculation of the network (proposed) Figure 2 represent the PDR of the proposed system and Figure 3 and 4 are for throughput and routing overhead respectively.

Figure 3: Throughput Calculation of the network (proposed)

Figure 4: Routing Overhead Calculation of the network (proposed)

www.asianssr.org

3

V. CONCLUSION:
In this paper, we have proposed a self-adaptive Q learning scheduling with AC Optimization. This methodology does not utilize the procedure of duty cycling. Instead, it isolates the time pivot into various schedule vacancies and lets every hub/node self-ruling choose to sleep, tune in or transmit in an availability. Every hub settles on a choice dependent on its present circumstance and an estimate of its neighbors' circumstances, where such guess does not require correspondence with neighbors using ACO algorithm. Results prove better in terms of PDR, Packet Loss, Throughput and Routing Overhead.
VI. FUTURE SCOPE:
Sleep scheduling aims at maximizing the network lifetime, actually there exists several techniques that can do the same work. 1. Mobile relays and sinks, low duty-cycle can prolong the lifetime of WSNs, but can also bring about message missing or delivery delay. And the nodes around the sink node ran out of their energy quickly, which can lead to energy hole. Mobile relays and sinks can solve this kind of problem. The mobile sink, just like a mobile robot, can travel around to gather information, which offers a good trade-off between energy consumption, latency and delivery delay. 2. Clustering, clustering is the first step of most semisynchronous theory. In this kind of methods, network is divided into several clusters, and cluster heads are responsible for communicating with other clusters. Actually, the cluster heads are always common nodes and have high duty cycle compared with other nodes. How to balance the energy consumption among them is very important in the future research. 3. Energy Harvesting, as a common train of thought to prolong the network lifetime, sleeping scheduling can only save energy but cannot generate energy. But the harvested energy is not stable and abundant. Thus, energy harvesting becomes another promising area to interact with sleep scheduling, which can solve the problem of energy shortage to some degree.
REFERENCES:
[1] Dayong Ye and Minjie Zhang “A Self-Adaptive Sleep/Wake-Up Scheduling Approach for Wireless Sensor Networks”, IEEE, 2017 [2] B. Jang, J. B. Lim, and M. L. Sichitiu, “An asynchronous scheduled MAC protocol for wireless sensor networks,” Comput. Netw., vol. 57, no. 1, pp. 85–98, 2013. [3] J. Kim, X. Lin, N. B. Shroff, and P. Sinha, “Minimizing delay and maximizing lifetime for wireless sensor networks with anycast,” IEEE/ACM Trans. Netw., vol. 18, no. 2, pp. 515–528, Apr. 2010. [4] T. V. Dam and K. Langendoen, “An adaptive energyefﬁcient MAC protocol for wireless sensor networks,” in Proc. ACM SenSys, Los Angeles, CA, USA, 2003, pp. 171–180.

[5] L. Tang, Y. Sun, O. Gurewitz, and D. B. Johnson, “PWMAC: An energy-efﬁcient predictive-wakeup MAC protocol for wireless sensor networks,” in Proc. IEEE INFOCOM, Shanghai, China, 2011, pp. 1305–1313. [6] L. Tang, Y. Sun, O. Gurewitz, and D. B. Johnson, “EMMAC: A dynamic multichannel energy-efﬁcient MAC protocol for wireless sensor networks,” in Proc. ACM MobiHoc, Paris, France, 2011, pp. 1–11. [7] D. Niyato, E. Hossain, M. M. Rashid, and V. K. Bhargava, “Wireless sensor networks with energy harvesting technologies: A game-theoretic approach to optimal energy management,” IEEE Wireless Commun., vol. 14, no. 4, pp. 90–96, Aug. 2007. [8] Y. Gong et al., “Distributed evolutionary algorithms and their models: A survey of the state-of-the-art,” Appl. Soft Comput., vol. 34, pp. 286–300, Sep. 2015. [9] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Mach. Learn., vol. 8, no. 3, pp. 279–292, 1992. [10] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 1998. [11] S. Abdallah and V. Lesser, “Learning the task allocation game,” in Proc. AAMAS, Hakodate, Japan, 2006, pp. 850– 857. [12] S. Abdallah and V. Lesser, “Multiagent reinforcement learning and selforganization in a network of agents,” in Proc. AAMAS, Honolulu, HI, USA, May 2007, pp. 172–179. [13] C. Zhang, V. Lesser, and P. Shenoy, “A multi-agent learning approach to online distributed resource allocation,” in Proc. IJCAI, Pasadena, CA, USA, Jul. 2009, pp. 361–366. [14] M. Bowling and M. Veloso, “Multiagent learning using a variable learning rate,” Artif. Intell., vol. 136, no. 2, pp. 215– 250, 2002. [15] E. R. Gomes and R. Kowalczyk, “Dynamic analysis of multiagent Q-learning with e-greedy exploration,” in Proc. ICML, Montreal, QC, Canada, 2009, pp. 369–376.

